<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dcat%3Acs.CL%20AND%20%28ti%3Atransformer%20OR%20ti%3Abert%29%26id_list%3D%26start%3D0%26max_results%3D1000" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=cat:cs.CL AND (ti:transformer OR ti:bert)&amp;id_list=&amp;start=0&amp;max_results=1000</title>
  <id>http://arxiv.org/api/A/m0y7CoGOaBgMrhPV4DZmhz5hE</id>
  <updated>2020-03-11T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">432</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1000</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2003.04195v1</id>
    <updated>2020-03-09T15:20:21Z</updated>
    <published>2020-03-09T15:20:21Z</published>
    <title>An Empirical Investigation of Pre-Trained Transformer Language Models
  for Open-Domain Dialogue Generation</title>
    <summary>  We present an empirical investigation of pre-trained Transformer-based
auto-regressive language models for the task of open-domain dialogue
generation. Training paradigm of pre-training and fine-tuning is employed to
conduct the parameter learning. Corpora of News and Wikipedia in Chinese and
English are collected for the pre-training stage respectively. Dialogue context
and response are concatenated into a single sequence utilized as the input of
the models during the fine-tuning stage. A weighted joint prediction paradigm
for both context and response is designed to evaluate the performance of models
with or without the loss term for context prediction. Various of decoding
strategies such as greedy search, beam search, top-k sampling, etc. are
employed to conduct the response text generation. Extensive experiments are
conducted on the typical single-turn and multi-turn dialogue corpora such as
Weibo, Douban, Reddit, DailyDialog, and Persona-Chat. Detailed numbers of
automatic evaluation metrics on relevance and diversity of the generated
results for the languages models as well as the baseline approaches are
reported.
</summary>
    <author>
      <name>Piji Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.04195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.04195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.03106v1</id>
    <updated>2020-03-06T09:46:51Z</updated>
    <published>2020-03-06T09:46:51Z</published>
    <title>Sensitive Data Detection and Classification in Spanish Clinical Text:
  Experiments with BERT</title>
    <summary>  Massive digital data processing provides a wide range of opportunities and
benefits, but at the cost of endangering personal data privacy. Anonymisation
consists in removing or replacing sensitive information from data, enabling its
exploitation for different purposes while preserving the privacy of
individuals. Over the years, a lot of automatic anonymisation systems have been
proposed; however, depending on the type of data, the target language or the
availability of training documents, the task remains challenging still. The
emergence of novel deep-learning models during the last two years has brought
large improvements to the state of the art in the field of Natural Language
Processing. These advancements have been most noticeably led by BERT, a model
proposed by Google in 2018, and the shared language models pre-trained on
millions of documents. In this paper, we use a BERT-based sequence labelling
model to conduct a series of anonymisation experiments on several clinical
datasets in Spanish. We also compare BERT to other algorithms. The experiments
show that a simple BERT-based model with general-domain pre-training obtains
highly competitive results without any domain specific feature engineering.
</summary>
    <author>
      <name>Aitor Garc√≠a-Pablos</name>
    </author>
    <author>
      <name>Naiara Perez</name>
    </author>
    <author>
      <name>Montse Cuadros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at 12th Edition of Language Resources and Evaluation
  Conference (LREC2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.03106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.03106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.02958v1</id>
    <updated>2020-03-05T23:09:24Z</updated>
    <published>2020-03-05T23:09:24Z</published>
    <title>EmpTransfo: A Multi-head Transformer Architecture for Creating
  Empathetic Dialog Systems</title>
    <summary>  Understanding emotions and responding accordingly is one of the biggest
challenges of dialog systems. This paper presents EmpTransfo, a multi-head
Transformer architecture for creating an empathetic dialog system. EmpTransfo
utilizes state-of-the-art pre-trained models (e.g., OpenAI-GPT) for language
generation, though models with different sizes can be used. We show that
utilizing the history of emotions and other metadata can improve the quality of
generated conversations by the dialog system. Our experimental results using a
challenging language corpus show that the proposed approach outperforms other
models in terms of Hit@1 and PPL (Perplexity).
</summary>
    <author>
      <name>Rohola Zandie</name>
    </author>
    <author>
      <name>Mohammad H. Mahoor</name>
    </author>
    <link href="http://arxiv.org/abs/2003.02958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.02912v1</id>
    <updated>2020-03-05T20:42:51Z</updated>
    <published>2020-03-05T20:42:51Z</published>
    <title>What the [MASK]? Making Sense of Language-Specific BERT Models</title>
    <summary>  Recently, Natural Language Processing (NLP) has witnessed an impressive
progress in many areas, due to the advent of novel, pretrained contextual
representation models. In particular, Devlin et al. (2019) proposed a model,
called BERT (Bidirectional Encoder Representations from Transformers), which
enables researchers to obtain state-of-the art performance on numerous NLP
tasks by fine-tuning the representations on their data set and task, without
the need for developing and training highly-specific architectures. The authors
also released multilingual BERT (mBERT), a model trained on a corpus of 104
languages, which can serve as a universal language model. This model obtained
impressive results on a zero-shot cross-lingual natural inference task. Driven
by the potential of BERT models, the NLP community has started to investigate
and generate an abundant number of BERT models that are trained on a particular
language, and tested on a specific data domain and task. This allows us to
evaluate the true potential of mBERT as a universal language model, by
comparing it to the performance of these more specific models. This paper
presents the current state of the art in language-specific BERT models,
providing an overall picture with respect to different dimensions (i.e.
architectures, data domains, and tasks). Our aim is to provide an immediate and
straightforward overview of the commonalities and differences between
Language-Specific (language-specific) BERT models and mBERT. We also provide an
interactive and constantly updated website that can be used to explore the
information we have collected, at https://bertlang.unibocconi.it.
</summary>
    <author>
      <name>Debora Nozza</name>
    </author>
    <author>
      <name>Federico Bianchi</name>
    </author>
    <author>
      <name>Dirk Hovy</name>
    </author>
    <link href="http://arxiv.org/abs/2003.02912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.02738v1</id>
    <updated>2020-03-05T16:06:37Z</updated>
    <published>2020-03-05T16:06:37Z</published>
    <title>BERT as a Teacher: Contextual Embeddings for Sequence-Level Reward</title>
    <summary>  Measuring the quality of a generated sequence against a set of references is
a central problem in many learning frameworks, be it to compute a score, to
assign a reward, or to perform discrimination. Despite great advances in model
architectures, metrics that scale independently of the number of references are
still based on n-gram estimates. We show that the underlying operations,
counting words and comparing counts, can be lifted to embedding words and
comparing embeddings. An in-depth analysis of BERT embeddings shows empirically
that contextual embeddings can be employed to capture the required dependencies
while maintaining the necessary scalability through appropriate pruning and
smoothing techniques. We cast unconditional generation as a reinforcement
learning problem and show that our reward function indeed provides a more
effective learning signal than n-gram reward in this challenging setting.
</summary>
    <author>
      <name>Florian Schmidt</name>
    </author>
    <author>
      <name>Thomas Hofmann</name>
    </author>
    <link href="http://arxiv.org/abs/2003.02738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.02245v1</id>
    <updated>2020-03-04T18:35:19Z</updated>
    <published>2020-03-04T18:35:19Z</published>
    <title>Data Augmentation using Pre-trained Transformer Models</title>
    <summary>  Language model based pre-trained models such as BERT have provided
significant gains across different NLP tasks. In this paper, we study different
types of pre-trained transformer based models such as auto-regressive models
(GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional
data augmentation. We show that prepending the class labels to text sequences
provides a simple yet effective way to condition the pre-trained models for
data augmentation. On three classification benchmarks, pre-trained Seq2Seq
model outperforms other models. Further, we explore how different pre-trained
model based data augmentation differs in-terms of data diversity, and how well
such methods preserve the class-label information.
</summary>
    <author>
      <name>Varun Kumar</name>
    </author>
    <author>
      <name>Ashutosh Choudhary</name>
    </author>
    <author>
      <name>Eunah Cho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.02245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.01680v2</id>
    <updated>2020-03-06T16:01:31Z</updated>
    <published>2020-03-03T18:07:42Z</published>
    <title>Hybrid Generative-Retrieval Transformers for Dialogue Domain Adaptation</title>
    <summary>  Domain adaptation has recently become a key problem in dialogue systems
research. Deep learning, while being the preferred technique for modeling such
systems, works best given massive training data. However, in the real-world
scenario, such resources aren't available for every new domain, so the ability
to train with a few dialogue examples can be considered essential. Pre-training
on large data sources and adapting to the target data has become the standard
method for few-shot problems within the deep learning framework. In this paper,
we present the winning entry at the fast domain adaptation task of DSTC8, a
hybrid generative-retrieval model based on GPT-2 fine-tuned to the multi-domain
MetaLWOz dataset. Robust and diverse in response generation, our model uses
retrieval logic as a fallback, being SoTA on MetaLWOz in human evaluation (&gt;4%
improvement over the 2nd place system) and attaining competitive generalization
performance in adaptation to the unseen MultiWOZ dataset.
</summary>
    <author>
      <name>Igor Shalyminov</name>
    </author>
    <author>
      <name>Alessandro Sordoni</name>
    </author>
    <author>
      <name>Adam Atkinson</name>
    </author>
    <author>
      <name>Hannes Schulz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at DSTC8@AAAI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.01680v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01680v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.01309v1</id>
    <updated>2020-03-03T03:17:29Z</updated>
    <published>2020-03-03T03:17:29Z</published>
    <title>Controllable Time-Delay Transformer for Real-Time Punctuation Prediction
  and Disfluency Detection</title>
    <summary>  With the increased applications of automatic speech recognition (ASR) in
recent years, it is essential to automatically insert punctuation marks and
remove disfluencies in transcripts, to improve the readability of the
transcripts as well as the performance of subsequent applications, such as
machine translation, dialogue systems, and so forth. In this paper, we propose
a Controllable Time-delay Transformer (CT-Transformer) model that jointly
completes the punctuation prediction and disfluency detection tasks in real
time. The CT-Transformer model facilitates freezing partial outputs with
controllable time delay to fulfill the real-time constraints in partial
decoding required by subsequent applications. We further propose a fast
decoding strategy to minimize latency while maintaining competitive
performance. Experimental results on the IWSLT2011 benchmark dataset and an
in-house Chinese annotated dataset demonstrate that the proposed approach
outperforms the previous state-of-the-art models on F-scores and achieves a
competitive inference speed.
</summary>
    <author>
      <name>Qian Chen</name>
    </author>
    <author>
      <name>Mengzhe Chen</name>
    </author>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Wen Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures, accepted by ICASSP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.01309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.00674v1</id>
    <updated>2020-03-02T05:40:57Z</updated>
    <published>2020-03-02T05:40:57Z</published>
    <title>Style Example-Guided Text Generation using Generative Adversarial
  Transformers</title>
    <summary>  We introduce a language generative model framework for generating a styled
paragraph based on a context sentence and a style reference example. The
framework consists of a style encoder and a texts decoder. The style encoder
extracts a style code from the reference example, and the text decoder
generates texts based on the style code and the context. We propose a novel
objective function to train our framework. We also investigate different
network design choices. We conduct extensive experimental validation with
comparison to strong baselines to validate the effectiveness of the proposed
framework using a newly collected dataset with diverse text styles. Both code
and dataset will be released upon publication.
</summary>
    <author>
      <name>Kuo-Hao Zeng</name>
    </author>
    <author>
      <name>Mohammad Shoeybi</name>
    </author>
    <author>
      <name>Ming-Yu Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2003.00674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.00674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.00104v1</id>
    <updated>2020-02-28T22:59:24Z</updated>
    <published>2020-02-28T22:59:24Z</published>
    <title>AraBERT: Transformer-based Model for Arabic Language Understanding</title>
    <summary>  The Arabic language is a morphologically rich and complex language with
relatively little resources and a less explored syntax compared to English.
Given these limitations, tasks like Sentiment Analysis (SA), Named Entity
Recognition (NER), and Question Answering (QA), have proven to be very
challenging to tackle. Recently, with the surge of transformers based models,
language-specific BERT based models proved to have a very efficient
understanding of languages, provided they are pre-trained on a very large
corpus. Such models were able to set new standards and achieve state-of-the-art
results for most NLP tasks. In this paper, we pre-trained BERT specifically for
the Arabic language in the pursuit of achieving the same success that BERT did
for the English language. We then compare the performance of AraBERT with
multilingual BERT provided by Google and other state-of-the-art approaches. The
results of the conducted experiments show that the newly developed AraBERT
achieved state-of-the-art results on most tested tasks. The pretrained araBERT
models are publicly available on hoping to encourage research and applications
for Arabic NLP.
</summary>
    <author>
      <name>Wissam Antoun</name>
    </author>
    <author>
      <name>Fady Baly</name>
    </author>
    <author>
      <name>Hazem Hajj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 tables, this paper will be submitted to The 4th Workshop
  on Open-Source Arabic Corpora and Processing Tools co-located with LREC 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.00104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.00104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.12327v1</id>
    <updated>2020-02-27T18:46:42Z</updated>
    <published>2020-02-27T18:46:42Z</published>
    <title>A Primer in BERTology: What we know about how BERT works</title>
    <summary>  Transformer-based models are now widely used in NLP, but we still do not
understand a lot about their inner workings. This paper describes what is known
to date about the famous BERT model (Devlin et al. 2019), synthesizing over 40
analysis studies. We also provide an overview of the proposed modifications to
the model and its training regime. We then outline the directions for further
research.
</summary>
    <author>
      <name>Anna Rogers</name>
    </author>
    <author>
      <name>Olga Kovaleva</name>
    </author>
    <author>
      <name>Anna Rumshisky</name>
    </author>
    <link href="http://arxiv.org/abs/2002.12327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11794v1</id>
    <updated>2020-02-26T21:17:13Z</updated>
    <published>2020-02-26T21:17:13Z</published>
    <title>Train Large, Then Compress: Rethinking Model Size for Efficient Training
  and Inference of Transformers</title>
    <summary>  Since hardware resources are limited, the objective of training deep learning
models is typically to maximize accuracy subject to the time and memory
constraints of training and inference. We study the impact of model size in
this setting, focusing on Transformer models for NLP tasks that are limited by
compute: self-supervised pretraining and high-resource machine translation. We
first show that even though smaller Transformer models execute faster per
iteration, wider and deeper models converge in significantly fewer steps.
Moreover, this acceleration in convergence typically outpaces the additional
computational overhead of using larger models. Therefore, the most
compute-efficient training strategy is to counterintuitively train extremely
large models but stop after a small number of iterations.
  This leads to an apparent trade-off between the training efficiency of large
Transformer models and the inference efficiency of small Transformer models.
However, we show that large models are more robust to compression techniques
such as quantization and pruning than small models. Consequently, one can get
the best of both worlds: heavily compressed, large models achieve higher
accuracy than lightly compressed, small models.
</summary>
    <author>
      <name>Zhuohan Li</name>
    </author>
    <author>
      <name>Eric Wallace</name>
    </author>
    <author>
      <name>Sheng Shen</name>
    </author>
    <author>
      <name>Kevin Lin</name>
    </author>
    <author>
      <name>Kurt Keutzer</name>
    </author>
    <author>
      <name>Dan Klein</name>
    </author>
    <author>
      <name>Joseph E. Gonzalez</name>
    </author>
    <link href="http://arxiv.org/abs/2002.11794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11643v1</id>
    <updated>2020-02-26T17:18:49Z</updated>
    <published>2020-02-26T17:18:49Z</published>
    <title>Marathi To English Neural Machine Translation With Near Perfect Corpus
  And Transformers</title>
    <summary>  There have been very few attempts to benchmark performances of
state-of-the-art algorithms for Neural Machine Translation task on Indian
Languages. Google, Bing, Facebook and Yandex are some of the very few companies
which have built translation systems for few of the Indian Languages. Among
them, translation results from Google are supposed to be better, based on
general inspection. Bing-Translator do not even support Marathi language which
has around 95 million speakers and ranks 15th in the world in terms of combined
primary and secondary speakers. In this exercise, we trained and compared
variety of Neural Machine Marathi to English Translators trained with
BERT-tokenizer by huggingface and various Transformer based architectures using
Facebook's Fairseq platform with limited but almost correct parallel corpus to
achieve better BLEU scores than Google on Tatoeba and Wikimedia open datasets.
</summary>
    <author>
      <name>Swapnil Ashok Jadhav</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 tables. This report is based on applied research work done
  at Dailyhunt</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.11643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11402v2</id>
    <updated>2020-02-28T18:44:07Z</updated>
    <published>2020-02-26T10:48:53Z</published>
    <title>Detecting Potential Topics In News Using BERT, CRF and Wikipedia</title>
    <summary>  For a news content distribution platform like Dailyhunt, Named Entity
Recognition is a pivotal task for building better user recommendation and
notification algorithms. Apart from identifying names, locations, organisations
from the news for 13+ Indian languages and use them in algorithms, we also need
to identify n-grams which do not necessarily fit in the definition of
Named-Entity, yet they are important. For example, "me too movement", "beef
ban", "alwar mob lynching". In this exercise, given an English language text,
we are trying to detect case-less n-grams which convey important information
and can be used as topics and/or hashtags for a news. Model is built using
Wikipedia titles data, private English news corpus and BERT-Multilingual
pre-trained model, Bi-GRU and CRF architecture. It shows promising results when
compared with industry best Flair, Spacy and Stanford-caseless-NER in terms of
F1 and especially Recall.
</summary>
    <author>
      <name>Swapnil Ashok Jadhav</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 tables, 1 figure, 2 examples. This is a report based on
  applied research work conducted at Dailyhunt</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.11402v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11402v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10957v1</id>
    <updated>2020-02-25T15:21:10Z</updated>
    <published>2020-02-25T15:21:10Z</published>
    <title>MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
  of Pre-Trained Transformers</title>
    <summary>  Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its
variants) have achieved remarkable success in varieties of NLP tasks. However,
these models usually consist of hundreds of millions of parameters which brings
challenges for fine-tuning and online serving in real-life applications due to
latency and capacity constraints. In this work, we present a simple and
effective approach to compress large Transformer (Vaswani et al., 2017) based
pre-trained models, termed as deep self-attention distillation. The small model
(student) is trained by deeply mimicking the self-attention module, which plays
a vital role in Transformer networks, of the large model (teacher).
Specifically, we propose distilling the self-attention module of the last
Transformer layer of the teacher, which is effective and flexible for the
student. Furthermore, we introduce the scaled dot-product between values in the
self-attention module as the new deep self-attention knowledge, in addition to
the attention distributions (i.e., the scaled dot-product of queries and keys)
that have been used in existing works. Moreover, we show that introducing a
teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large
pre-trained Transformer models. Experimental results demonstrate that our model
outperforms state-of-the-art baselines in different parameter size of student
models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and
several GLUE benchmark tasks using 50% of the Transformer parameters and
computations of the teacher model. The code and models are publicly available
at https://github.com/microsoft/unilm/tree/master/minilm
</summary>
    <author>
      <name>Wenhui Wang</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <author>
      <name>Li Dong</name>
    </author>
    <author>
      <name>Hangbo Bao</name>
    </author>
    <author>
      <name>Nan Yang</name>
    </author>
    <author>
      <name>Ming Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code and models:
  https://github.com/microsoft/unilm/tree/master/minilm</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.10957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10832v1</id>
    <updated>2020-02-25T12:44:36Z</updated>
    <published>2020-02-25T12:44:36Z</published>
    <title>BERT Can See Out of the Box: On the Cross-modal Transferability of Text
  Representations</title>
    <summary>  Pre-trained language models such as BERT have recently contributed to
significant advances in Natural Language Processing tasks. Interestingly, while
multilingual BERT models have demonstrated impressive results, recent works
have shown how monolingual BERT can also be competitive in zero-shot
cross-lingual settings. This suggests that the abstractions learned by these
models can transfer across languages, even when trained on monolingual data. In
this paper, we investigate whether such generalization potential applies to
other modalities, such as vision: does BERT contain abstractions that
generalize beyond text? We introduce BERT-gen, an architecture for text
generation based on BERT, able to leverage on either mono- or multi- modal
representations. The results reported under different configurations indicate a
positive answer to our research question, and the proposed model obtains
substantial improvements over the state-of-the-art on two established Visual
Question Generation datasets.
</summary>
    <author>
      <name>Thomas Scialom</name>
    </author>
    <author>
      <name>Patrick Bordes</name>
    </author>
    <author>
      <name>Paul-Alexis Dray</name>
    </author>
    <author>
      <name>Jacopo Staiano</name>
    </author>
    <author>
      <name>Patrick Gallinari</name>
    </author>
    <link href="http://arxiv.org/abs/2002.10832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10695v1</id>
    <updated>2020-02-25T06:41:07Z</updated>
    <published>2020-02-25T06:41:07Z</published>
    <title>Multimodal Transformer with Pointer Network for the DSTC8 AVSD Challenge</title>
    <summary>  Audio-Visual Scene-Aware Dialog (AVSD) is an extension from Video Question
Answering (QA) whereby the dialogue agent is required to generate natural
language responses to address user queries and carry on conversations. This is
a challenging task as it consists of video features of multiple modalities,
including text, visual, and audio features. The agent also needs to learn
semantic dependencies among user utterances and system responses to make
coherent conversations with humans. In this work, we describe our submission to
the AVSD track of the 8th Dialogue System Technology Challenge. We adopt
dot-product attention to combine text and non-text features of input video. We
further enhance the generation capability of the dialogue agent by adopting
pointer networks to point to tokens from multiple source sequences in each
generation step. Our systems achieve high performance in automatic metrics and
obtain 5th and 6th place in human evaluation among all submissions.
</summary>
    <author>
      <name>Hung Le</name>
    </author>
    <author>
      <name>Nancy F. Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at DSTC Workshop at AAAI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.10695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10670v2</id>
    <updated>2020-03-03T05:16:37Z</updated>
    <published>2020-02-25T05:09:48Z</published>
    <title>Exploring BERT Parameter Efficiency on the Stanford Question Answering
  Dataset v2.0</title>
    <summary>  In this paper we explore the parameter efficiency of BERT arXiv:1810.04805 on
version 2.0 of the Stanford Question Answering dataset (SQuAD2.0). We evaluate
the parameter efficiency of BERT while freezing a varying number of final
transformer layers as well as including the adapter layers proposed in
arXiv:1902.00751. Additionally, we experiment with the use of context-aware
convolutional (CACNN) filters, as described in arXiv:1709.08294v3, as a final
augmentation layer for the SQuAD2.0 tasks.
  This exploration is motivated in part by arXiv:1907.10597, which made a
compelling case for broadening the evaluation criteria of artificial
intelligence models to include various measures of resource efficiency. While
we do not evaluate these models based on their floating point operation
efficiency as proposed in arXiv:1907.10597, we examine efficiency with respect
to training time, inference time, and total number of model parameters. Our
results largely corroborate those of arXiv:1902.00751 for adapter modules,
while also demonstrating that gains in F1 score from adding context-aware
convolutional filters are not practical due to the increase in training and
inference time.
</summary>
    <author>
      <name>Eric Hulburd</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.10670v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10670v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10345v1</id>
    <updated>2020-02-24T16:17:12Z</updated>
    <published>2020-02-24T16:17:12Z</published>
    <title>Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation</title>
    <summary>  Fine-tuning pre-trained language models like BERT has become an effective way
in NLP and yields state-of-the-art results on many downstream tasks. Recent
studies on adapting BERT to new tasks mainly focus on modifying the model
structure, re-designing the pre-train tasks, and leveraging external data and
knowledge. The fine-tuning strategy itself has yet to be fully explored. In
this paper, we improve the fine-tuning of BERT with two effective mechanisms:
self-ensemble and self-distillation. The experiments on text classification and
natural language inference tasks show our proposed methods can significantly
improve the adaption of BERT without any external data or knowledge.
</summary>
    <author>
      <name>Yige Xu</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Ligao Zhou</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.10345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10260v1</id>
    <updated>2020-02-24T13:53:06Z</updated>
    <published>2020-02-24T13:53:06Z</published>
    <title>Fixed Encoder Self-Attention Patterns in Transformer-Based Machine
  Translation</title>
    <summary>  Transformer-based models have brought a radical change to neural machine
translation. A key feature of the Transformer architecture is the so-called
multi-head attention mechanism, which allows the model to focus simultaneously
on different parts of the input. However, recent works have shown that
attention heads learn simple positional patterns which are often redundant. In
this paper, we propose to replace all but one attention head of each encoder
layer with fixed -- non-learnable -- attentive patterns that are solely based
on position and do not require any external knowledge. Our experiments show
that fixing the attention heads on the encoder side of the Transformer at
training time does not impact the translation quality and even increases BLEU
scores by up to 3 points in low-resource scenarios.
</summary>
    <author>
      <name>Alessandro Raganato</name>
    </author>
    <author>
      <name>Yves Scherrer</name>
    </author>
    <author>
      <name>J√∂rg Tiedemann</name>
    </author>
    <link href="http://arxiv.org/abs/2002.10260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10107v1</id>
    <updated>2020-02-24T07:56:02Z</updated>
    <published>2020-02-24T07:56:02Z</published>
    <title>Predicting Subjective Features from Questions on QA Websites using BERT</title>
    <summary>  Modern Question-Answering websites, such as StackOverflow and Quora, have
specific user rules to maintain their content quality. These systems rely on
user reports for accessing new contents, which has serious problems including
the slow handling of violations, the loss of normal and experienced users'
time, the low quality of some reports, and discouraging feedback to new users.
Therefore, with the overall goal of providing solutions for automating
moderation actions in Q&amp;A websites, we aim to provide a model to predict 20
quality or subjective aspects of questions in QA websites. To this end, we used
data gathered by the CrowdSource team at Google Research in 2019 and fine-tuned
pre-trained BERT model on our problem. Model achieves 95.4% accuracy after 2
epochs of training and did not improve substantially in the next ones. Results
confirm that by simple fine-tuning, we can achieve accurate models, in little
time, and on less amount of data.
</summary>
    <author>
      <name>Issa Annamoradnejad</name>
    </author>
    <author>
      <name>Mohammadamin Fazli</name>
    </author>
    <author>
      <name>Jafar Habibi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.10107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.7; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10101v1</id>
    <updated>2020-02-24T07:37:17Z</updated>
    <published>2020-02-24T07:37:17Z</published>
    <title>GRET: Global Representation Enhanced Transformer</title>
    <summary>  Transformer, based on the encoder-decoder framework, has achieved
state-of-the-art performance on several natural language generation tasks. The
encoder maps the words in the input sentence into a sequence of hidden states,
which are then fed into the decoder to generate the output sentence. These
hidden states usually correspond to the input words and focus on capturing
local information. However, the global (sentence level) information is seldom
explored, leaving room for the improvement of generation quality. In this
paper, we propose a novel global representation enhanced Transformer (GRET) to
explicitly model global representation in the Transformer network.
Specifically, in the proposed model, an external state is generated for the
global representation from the encoder. The global representation is then fused
into the decoder during the decoding process to improve generation quality. We
conduct experiments in two text generation tasks: machine translation and text
summarization. Experimental results on four WMT machine translation tasks and
LCSTS text summarization task demonstrate the effectiveness of the proposed
approach on natural language generation.
</summary>
    <author>
      <name>Rongxiang Weng</name>
    </author>
    <author>
      <name>Haoran Wei</name>
    </author>
    <author>
      <name>Shujian Huang</name>
    </author>
    <author>
      <name>Heng Yu</name>
    </author>
    <author>
      <name>Lidong Bing</name>
    </author>
    <author>
      <name>Weihua Luo</name>
    </author>
    <author>
      <name>Jiajun Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by AAAI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.10101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.09812v1</id>
    <updated>2020-02-23T03:07:31Z</updated>
    <published>2020-02-23T03:07:31Z</published>
    <title>Sketching Transformed Matrices with Applications to Natural Language
  Processing</title>
    <summary>  Suppose we are given a large matrix $A=(a_{i,j})$ that cannot be stored in
memory but is in a disk or is presented in a data stream. However, we need to
compute a matrix decomposition of the entry-wisely transformed matrix,
$f(A):=(f(a_{i,j}))$ for some function $f$. Is it possible to do it in a space
efficient way? Many machine learning applications indeed need to deal with such
large transformed matrices, for example word embedding method in NLP needs to
work with the pointwise mutual information (PMI) matrix, while the entrywise
transformation makes it difficult to apply known linear algebraic tools.
Existing approaches for this problem either need to store the whole matrix and
perform the entry-wise transformation afterwards, which is space consuming or
infeasible, or need to redesign the learning method, which is application
specific and requires substantial remodeling.
  In this paper, we first propose a space-efficient sketching algorithm for
computing the product of a given small matrix with the transformed matrix. It
works for a general family of transformations with provable small error bounds
and thus can be used as a primitive in downstream learning tasks. We then apply
this primitive to a concrete application: low-rank approximation. We show that
our approach obtains small error and is efficient in both space and time. We
complement our theoretical results with experiments on synthetic and real data.
</summary>
    <author>
      <name>Yingyu Liang</name>
    </author>
    <author>
      <name>Zhao Song</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <author>
      <name>Lin F. Yang</name>
    </author>
    <author>
      <name>Xin Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AISTATS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.09812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.09402v2</id>
    <updated>2020-03-09T09:21:14Z</updated>
    <published>2020-02-21T16:37:57Z</published>
    <title>Accessing Higher-level Representations in Sequential Transformers with
  Feedback Memory</title>
    <summary>  Transformers are feedforward networks that can process input tokens in
parallel. While this parallelization makes them computationally efficient, it
restricts the model from fully exploiting the sequential nature of the input -
the representation at a given layer can only access representations from lower
layers, rather than the higher level representations already built in previous
time steps. In this work, we propose the Feedback Transformer architecture that
exposes all previous representations to all future representations, meaning the
lowest representation of the current timestep is formed from the highest-level
abstract representation of the past. We demonstrate on a variety of benchmarks
in language modeling, neural machine translation, summarization, and
reinforcement learning that the increased representation capacity can improve
over Transformer baselines.
</summary>
    <author>
      <name>Angela Fan</name>
    </author>
    <author>
      <name>Thibaut Lavril</name>
    </author>
    <author>
      <name>Edouard Grave</name>
    </author>
    <author>
      <name>Armand Joulin</name>
    </author>
    <author>
      <name>Sainbayar Sukhbaatar</name>
    </author>
    <link href="http://arxiv.org/abs/2002.09402v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09402v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.08614v1</id>
    <updated>2020-02-20T08:20:52Z</updated>
    <published>2020-02-20T08:20:52Z</published>
    <title>Balancing Cost and Benefit with Tied-Multi Transformers</title>
    <summary>  We propose and evaluate a novel procedure for training multiple Transformers
with tied parameters which compresses multiple models into one enabling the
dynamic choice of the number of encoder and decoder layers during decoding. In
sequence-to-sequence modeling, typically, the output of the last layer of the
N-layer encoder is fed to the M-layer decoder, and the output of the last
decoder layer is used to compute loss. Instead, our method computes a single
loss consisting of NxM losses, where each loss is computed from the output of
one of the M decoder layers connected to one of the N encoder layers. Such a
model subsumes NxM models with different number of encoder and decoder layers,
and can be used for decoding with fewer than the maximum number of encoder and
decoder layers. We then propose a mechanism to choose a priori the number of
encoder and decoder layers for faster decoding, and also explore recurrent
stacking of layers and knowledge distillation for model compression. We present
a cost-benefit analysis of applying the proposed approaches for neural machine
translation and show that they reduce decoding costs while preserving
translation quality.
</summary>
    <author>
      <name>Raj Dabre</name>
    </author>
    <author>
      <name>Raphael Rubino</name>
    </author>
    <author>
      <name>Atsushi Fujita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of our previous manuscript available at
  arXiv:1908.10118</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.08614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.08562v1</id>
    <updated>2020-02-20T04:14:35Z</updated>
    <published>2020-02-20T04:14:35Z</published>
    <title>Federated pretraining and fine tuning of BERT using clinical notes from
  multiple silos</title>
    <summary>  Large scale contextual representation models, such as BERT, have
significantly advanced natural language processing (NLP) in recently years.
However, in certain area like healthcare, accessing diverse large scale text
data from multiple institutions is extremely challenging due to privacy and
regulatory reasons. In this article, we show that it is possible to both
pretrain and fine tune BERT models in a federated manner using clinical texts
from different silos without moving the data.
</summary>
    <author>
      <name>Dianbo Liu</name>
    </author>
    <author>
      <name>Tim Miller</name>
    </author>
    <link href="http://arxiv.org/abs/2002.08562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.08307v1</id>
    <updated>2020-02-19T17:40:57Z</updated>
    <published>2020-02-19T17:40:57Z</published>
    <title>Compressing BERT: Studying the Effects of Weight Pruning on Transfer
  Learning</title>
    <summary>  Universal feature extractors, such as BERT for natural language processing
and VGG for computer vision, have become effective methods for improving deep
learning models without requiring more labeled data. A common paradigm is to
pre-train a feature extractor on large amounts of data then fine-tune it as
part of a deep learning model on some downstream task (i.e. transfer learning).
While effective, feature extractors like BERT may be prohibitively large for
some deployment scenarios. We explore weight pruning for BERT and ask: how does
compression during pre-training affect transfer learning? We find that pruning
affects transfer learning in three broad regimes. Low levels of pruning
(30-40\%) do not affect pre-training loss or transfer to downstream tasks at
all. Medium levels of pruning increase the pre-training loss and prevent useful
pre-training information from being transferred to downstream tasks. High
levels of pruning additionally prevent models from fitting downstream datasets,
leading to further degradation. Finally, we observe that fine-tuning BERT on a
specific task does not improve its prunability. We conclude that BERT can be
pruned once during pre-training rather than separately for each task without
affecting performance.
</summary>
    <author>
      <name>Mitchell A. Gordon</name>
    </author>
    <author>
      <name>Kevin Duh</name>
    </author>
    <author>
      <name>Nicholas Andrews</name>
    </author>
    <link href="http://arxiv.org/abs/2002.08307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.08087v2</id>
    <updated>2020-03-06T15:29:34Z</updated>
    <published>2020-02-19T09:48:39Z</published>
    <title>LAMBERT: Layout-Aware language Modeling using BERT for information
  extraction</title>
    <summary>  In this paper we introduce a novel approach to the problem of understanding
documents where the local semantics is influenced by non-trivial layout.
Namely, we modify the Transformer architecture in a way that allows it to use
the graphical features defined by the layout, without the need to re-learn the
language semantics from scratch, thanks to starting the training process from a
model pretrained on classical language modeling tasks.
</summary>
    <author>
      <name>≈Åukasz Garncarek</name>
    </author>
    <author>
      <name>Rafa≈Ç Powalski</name>
    </author>
    <author>
      <name>Tomasz Stanis≈Çawek</name>
    </author>
    <author>
      <name>Bartosz Topolski</name>
    </author>
    <author>
      <name>Piotr Halama</name>
    </author>
    <author>
      <name>Filip Grali≈Ñski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v1: 9 pages; work in progress; this version of the paper was
  submitted to review on Dec 10, 2019, and subsequently withdrawn on Feb 17,
  2020 v2: 17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.08087v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08087v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.07725v1</id>
    <updated>2020-02-18T16:51:05Z</updated>
    <published>2020-02-18T16:51:05Z</published>
    <title>Gradient-Based Adversarial Training on Transformer Networks for
  Detecting Check-Worthy Factual Claims</title>
    <summary>  We present a study on the efficacy of adversarial training on transformer
neural network models, with respect to the task of detecting check-worthy
claims. In this work, we introduce the first adversarially-regularized,
transformer-based claim spotter model that achieves state-of-the-art results on
multiple challenging benchmarks. We obtain a 4.31 point F1-score improvement
and a 1.09 point mAP score improvement over current state-of-the-art models on
the ClaimBuster Dataset and CLEF2019 Dataset, respectively. In the process, we
propose a method to apply adversarial training to transformer models, which has
the potential to be generalized to many similar text classification tasks.
Along with our results, we are releasing our codebase and manually labeled
datasets. We also showcase our models' real world usage via a live public API.
</summary>
    <author>
      <name>Kevin Meng</name>
    </author>
    <author>
      <name>Damian Jimenez</name>
    </author>
    <author>
      <name>Fatma Arslan</name>
    </author>
    <author>
      <name>Jacob Daniel Devasier</name>
    </author>
    <author>
      <name>Daniel Obembe</name>
    </author>
    <author>
      <name>Chengkai Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.07725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.07725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.07551v1</id>
    <updated>2020-02-18T13:44:49Z</updated>
    <published>2020-02-18T13:44:49Z</published>
    <title>Hierarchical Transformer Network for Utterance-level Emotion Recognition</title>
    <summary>  While there have been significant advances in de-tecting emotions in text, in
the field of utter-ance-level emotion recognition (ULER), there are still many
problems to be solved. In this paper, we address some challenges in ULER in
dialog sys-tems. (1) The same utterance can deliver different emotions when it
is in different contexts or from different speakers. (2) Long-range contextual
in-formation is hard to effectively capture. (3) Unlike the traditional text
classification problem, this task is supported by a limited number of datasets,
among which most contain inadequate conversa-tions or speech. To address these
problems, we propose a hierarchical transformer framework (apart from the
description of other studies, the "transformer" in this paper usually refers to
the encoder part of the transformer) with a lower-level transformer to model
the word-level input and an upper-level transformer to capture the context of
utterance-level embeddings. We use a pretrained language model bidirectional
encoder representa-tions from transformers (BERT) as the lower-level
transformer, which is equivalent to introducing external data into the model
and solve the problem of data shortage to some extent. In addition, we add
speaker embeddings to the model for the first time, which enables our model to
capture the in-teraction between speakers. Experiments on three dialog emotion
datasets, Friends, EmotionPush, and EmoryNLP, demonstrate that our proposed
hierarchical transformer network models achieve 1.98%, 2.83%, and 3.94%
improvement, respec-tively, over the state-of-the-art methods on each dataset
in terms of macro-F1.
</summary>
    <author>
      <name>QingBiao Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Beijing University of Posts and Telecommunications</arxiv:affiliation>
    </author>
    <author>
      <name>ChunHua Wu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Beijing University of Posts and Telecommunications</arxiv:affiliation>
    </author>
    <author>
      <name>KangFeng Zheng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Beijing University of Posts and Telecommunications</arxiv:affiliation>
    </author>
    <author>
      <name>Zhe Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Beijing University of Posts and Telecommunications</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.07551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.07551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.06823v1</id>
    <updated>2020-02-17T08:13:36Z</updated>
    <published>2020-02-17T08:13:36Z</published>
    <title>Incorporating BERT into Neural Machine Translation</title>
    <summary>  The recently proposed BERT has shown great power on a variety of natural
language understanding tasks, such as text classification, reading
comprehension, etc. However, how to effectively apply BERT to neural machine
translation (NMT) lacks enough exploration. While BERT is more commonly used as
fine-tuning instead of contextual embedding for downstream language
understanding tasks, in NMT, our preliminary exploration of using BERT as
contextual embedding is better than using for fine-tuning. This motivates us to
think how to better leverage BERT for NMT along this direction. We propose a
new algorithm named BERT-fused model, in which we first use BERT to extract
representations for an input sequence, and then the representations are fused
with each layer of the encoder and decoder of the NMT model through attention
mechanisms. We conduct experiments on supervised (including sentence-level and
document-level translations), semi-supervised and unsupervised machine
translation, and achieve state-of-the-art results on seven benchmark datasets.
Our code is available at \url{https://github.com/bert-nmt/bert-nmt}.
</summary>
    <author>
      <name>Jinhua Zhu</name>
    </author>
    <author>
      <name>Yingce Xia</name>
    </author>
    <author>
      <name>Lijun Wu</name>
    </author>
    <author>
      <name>Di He</name>
    </author>
    <author>
      <name>Tao Qin</name>
    </author>
    <author>
      <name>Wengang Zhou</name>
    </author>
    <author>
      <name>Houqiang Li</name>
    </author>
    <author>
      <name>Tie-Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICLR-2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.06823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.06652v1</id>
    <updated>2020-02-16T19:02:52Z</updated>
    <published>2020-02-16T19:02:52Z</published>
    <title>SBERT-WK: A Sentence Embedding Method by Dissecting BERT-based Word
  Models</title>
    <summary>  Sentence embedding is an important research topic in natural language
processing (NLP) since it can transfer knowledge to downstream tasks.
Meanwhile, a contextualized word representation, called BERT, achieves the
state-of-the-art performance in quite a few NLP tasks. Yet, it is an open
problem to generate a high quality sentence representation from BERT-based word
models. It was shown in previous study that different layers of BERT capture
different linguistic properties. This allows us to fusion information across
layers to find better sentence representation. In this work, we study the
layer-wise pattern of the word representation of deep contextualized models.
Then, we propose a new sentence embedding method by dissecting BERT-based word
models through geometric analysis of the space spanned by the word
representation. It is called the SBERT-WK method. No further training is
required in SBERT-WK. We evaluate SBERT-WK on semantic textual similarity and
downstream supervised tasks. Furthermore, ten sentence-level probing tasks are
presented for detailed linguistic analysis. Experiments show that SBERT-WK
achieves the state-of-the-art performance. Our codes are publicly available.
</summary>
    <author>
      <name>Bin Wang</name>
    </author>
    <author>
      <name>C. -C. Jay Kuo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figure, 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.06652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.06170v1</id>
    <updated>2020-02-14T18:41:58Z</updated>
    <published>2020-02-14T18:41:58Z</published>
    <title>Transformer on a Diet</title>
    <summary>  Transformer has been widely used thanks to its ability to capture sequence
information in an efficient way. However, recent developments, such as BERT and
GPT-2, deliver only heavy architectures with a focus on effectiveness. In this
paper, we explore three carefully-designed light Transformer architectures to
figure out whether the Transformer with less computations could produce
competitive results. Experimental results on language model benchmark datasets
hint that such trade-off is promising, and the light Transformer reduces 70%
parameters at best, while obtains competitive perplexity compared to standard
Transformer. The source code is publicly available.
</summary>
    <author>
      <name>Chenguang Wang</name>
    </author>
    <author>
      <name>Zihao Ye</name>
    </author>
    <author>
      <name>Aston Zhang</name>
    </author>
    <author>
      <name>Zheng Zhang</name>
    </author>
    <author>
      <name>Alexander J. Smola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 tables, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.06170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05902v1</id>
    <updated>2020-02-14T07:45:33Z</updated>
    <published>2020-02-14T07:45:33Z</published>
    <title>Understanding patient complaint characteristics using contextual
  clinical BERT embeddings</title>
    <summary>  In clinical conversational applications, extracted entities tend to capture
the main subject of a patient's complaint, namely symptoms or diseases.
However, they mostly fail to recognize the characterizations of a complaint
such as the time, the onset, and the severity. For example, if the input is "I
have a headache and it is extreme", state-of-the-art models only recognize the
main symptom entity - headache, but ignore the severity factor of "extreme",
that characterizes headache. In this paper, we design a two-stage approach to
detect the characterizations of entities like symptoms presented by general
users in contexts where they would describe their symptoms to a clinician. We
use Word2Vec and BERT to encode clinical text given by the patients. We
transform the output and re-frame the task as multi-label classification
problem. Finally, we combine the processed encodings with the Linear
Discriminant Analysis (LDA) algorithm to classify the characterizations of the
main entity. Experimental results demonstrate that our method achieves 40-50%
improvement on the accuracy over the state-of-the-art models.
</summary>
    <author>
      <name>Budhaditya Saha</name>
    </author>
    <author>
      <name>Sanal Lisboa</name>
    </author>
    <author>
      <name>Shameek Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been communicated to 42nd IEEE Annual Conference of
  Engineering in Medicine and Biology Society, Montreal, Canada. It is 5 pages
  long. It has 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.05902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05867v1</id>
    <updated>2020-02-14T04:23:28Z</updated>
    <published>2020-02-14T04:23:28Z</published>
    <title>Transformers as Soft Reasoners over Language</title>
    <summary>  AI has long pursued the goal of having systems reason over *explicitly
provided* knowledge, but building suitable representations has proved
challenging. Here we explore whether transformers can similarly learn to reason
(or emulate reasoning), but using rules expressed in language, thus bypassing a
formal representation. We provide the first demonstration that this is
possible, and characterize the extent of this capability. To do this, we use a
collection of synthetic datasets that test increasing levels of reasoning
complexity (number of rules, presence of negation, and depth of chaining). We
find transformers appear to learn rule-based reasoning with high (99%) accuracy
on these datasets, and in a way that generalizes to test data requiring
substantially deeper chaining than in the training data (95%+ scores). We also
demonstrate that the models transfer well to two hand-authored rulebases, and
to rulebases paraphrased into more natural language. These findings are
significant as it suggests a new role for transformers, namely as a limited
"soft theorem prover" operating over explicit theories in language. This in
turn suggests new possibilities for explainability, correctability, and
counterfactual reasoning in question-answering. All datasets and a live demo
are available at http://rule-reasoning.apps.allenai.org/
</summary>
    <author>
      <name>Peter Clark</name>
    </author>
    <author>
      <name>Oyvind Tafjord</name>
    </author>
    <author>
      <name>Kyle Richardson</name>
    </author>
    <link href="http://arxiv.org/abs/2002.05867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.04815v1</id>
    <updated>2020-02-12T06:11:48Z</updated>
    <published>2020-02-12T06:11:48Z</published>
    <title>Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis
  and Natural Language Inference</title>
    <summary>  Aspect based sentiment analysis aims to identify the sentimental tendency
towards a given aspect in text. Fine-tuning of pretrained BERT performs
excellent on this task and achieves state-of-the-art performances. Existing
BERT-based works only utilize the last output layer of BERT and ignore the
semantic knowledge in the intermediate layers. This paper explores the
potential of utilizing BERT intermediate layers to enhance the performance of
fine-tuning of BERT. To the best of our knowledge, no existing work has been
done on this research. To show the generality, we also apply this approach to a
natural language inference task. Experimental results demonstrate the
effectiveness and generality of the proposed approach.
</summary>
    <author>
      <name>Youwei Song</name>
    </author>
    <author>
      <name>Jiahai Wang</name>
    </author>
    <author>
      <name>Zhiwei Liang</name>
    </author>
    <author>
      <name>Zhiyue Liu</name>
    </author>
    <author>
      <name>Tao Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.04815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.04815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.04745v1</id>
    <updated>2020-02-12T00:33:03Z</updated>
    <published>2020-02-12T00:33:03Z</published>
    <title>On Layer Normalization in the Transformer Architecture</title>
    <summary>  The Transformer is widely used in natural language processing tasks. To train
a Transformer however, one usually needs a carefully designed learning rate
warm-up stage, which is shown to be crucial to the final performance but will
slow down the optimization and bring more hyper-parameter tunings. In this
paper, we first study theoretically why the learning rate warm-up stage is
essential and show that the location of layer normalization matters.
Specifically, we prove with mean field theory that at initialization, for the
original-designed Post-LN Transformer, which places the layer normalization
between the residual blocks, the expected gradients of the parameters near the
output layer are large. Therefore, using a large learning rate on those
gradients makes the training unstable. The warm-up stage is practically helpful
for avoiding this problem. On the other hand, our theory also shows that if the
layer normalization is put inside the residual blocks (recently proposed as
Pre-LN Transformer), the gradients are well-behaved at initialization. This
motivates us to remove the warm-up stage for the training of Pre-LN
Transformers. We show in our experiments that Pre-LN Transformers without the
warm-up stage can reach comparable results with baselines while requiring
significantly less training time and hyper-parameter tuning on a wide range of
applications.
</summary>
    <author>
      <name>Ruibin Xiong</name>
    </author>
    <author>
      <name>Yunchang Yang</name>
    </author>
    <author>
      <name>Di He</name>
    </author>
    <author>
      <name>Kai Zheng</name>
    </author>
    <author>
      <name>Shuxin Zheng</name>
    </author>
    <author>
      <name>Chen Xing</name>
    </author>
    <author>
      <name>Huishuai Zhang</name>
    </author>
    <author>
      <name>Yanyan Lan</name>
    </author>
    <author>
      <name>Liwei Wang</name>
    </author>
    <author>
      <name>Tie-Yan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2002.04745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.04745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.04723v1</id>
    <updated>2020-02-11T22:52:40Z</updated>
    <published>2020-02-11T22:52:40Z</published>
    <title>Superbloom: Bloom filter meets Transformer</title>
    <summary>  We extend the idea of word pieces in natural language models to machine
learning tasks on opaque ids. This is achieved by applying hash functions to
map each id to multiple hash tokens in a much smaller space, similarly to a
Bloom filter. We show that by applying a multi-layer Transformer to these Bloom
filter digests, we are able to obtain models with high accuracy. They
outperform models of a similar size without hashing and, to a large degree,
models of a much larger size trained using sampled softmax with the same
computational budget. Our key observation is that it is important to use a
multi-layer Transformer for Bloom filter digests to remove ambiguity in the
hashed input. We believe this provides an alternative method to solving
problems with large vocabulary size.
</summary>
    <author>
      <name>John Anderson</name>
    </author>
    <author>
      <name>Qingqing Huang</name>
    </author>
    <author>
      <name>Walid Krichene</name>
    </author>
    <author>
      <name>Steffen Rendle</name>
    </author>
    <author>
      <name>Li Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2002.04723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.04723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.03921v2</id>
    <updated>2020-02-13T00:50:39Z</updated>
    <published>2020-02-10T16:29:26Z</published>
    <title>End-to-End Multi-speaker Speech Recognition with Transformer</title>
    <summary>  Recently, fully recurrent neural network (RNN) based end-to-end models have
been proven to be effective for multi-speaker speech recognition in both the
single-channel and multi-channel scenarios. In this work, we explore the use of
Transformer models for these tasks by focusing on two aspects. First, we
replace the RNN-based encoder-decoder in the speech recognition model with a
Transformer architecture. Second, in order to use the Transformer in the
masking network of the neural beamformer in the multi-channel case, we modify
the self-attention component to be restricted to a segment rather than the
whole sequence in order to reduce computation. Besides the model architecture
improvements, we also incorporate an external dereverberation preprocessing,
the weighted prediction error (WPE), enabling our model to handle reverberated
signals. Experiments on the spatialized wsj1-2mix corpus show that the
Transformer-based models achieve 40.9% and 25.6% relative WER reduction, down
to 12.1% and 6.4% WER, under the anechoic condition in single-channel and
multi-channel tasks, respectively, while in the reverberant case, our methods
achieve 41.5% and 13.8% relative WER reduction, down to 16.5% and 15.2% WER.
</summary>
    <author>
      <name>Xuankai Chang</name>
    </author>
    <author>
      <name>Wangyou Zhang</name>
    </author>
    <author>
      <name>Yanmin Qian</name>
    </author>
    <author>
      <name>Jonathan Le Roux</name>
    </author>
    <author>
      <name>Shinji Watanabe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICASSP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.03921v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03921v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.02925v2</id>
    <updated>2020-02-10T18:45:41Z</updated>
    <published>2020-02-07T17:52:16Z</published>
    <title>BERT-of-Theseus: Compressing BERT by Progressive Module Replacing</title>
    <summary>  In this paper, we propose a novel model compression approach to effectively
compress BERT by progressive module replacing. Our approach first divides the
original BERT into several modules and builds their compact substitutes. Then,
we randomly replace the original modules with their substitutes to train the
compact modules to mimic the behavior of the original modules. We progressively
increase the probability of replacement through the training. In this way, our
approach brings a deeper level of interaction between the original and compact
models, and smooths the training process. Compared to the previous knowledge
distillation approaches for BERT compression, our approach leverages only one
loss function and one hyper-parameter, liberating human effort from
hyper-parameter tuning. Our approach outperforms existing knowledge
distillation approaches on GLUE benchmark, showing a new perspective of model
compression.
</summary>
    <author>
      <name>Canwen Xu</name>
    </author>
    <author>
      <name>Wangchunshu Zhou</name>
    </author>
    <author>
      <name>Tao Ge</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <author>
      <name>Ming Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages; typo fixed</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.02925v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.02925v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.02649v1</id>
    <updated>2020-02-07T07:19:15Z</updated>
    <published>2020-02-07T07:19:15Z</published>
    <title>Multimodal Matching Transformer for Live Commenting</title>
    <summary>  Automatic live commenting aims to provide real-time comments on videos for
viewers. It encourages users engagement on online video sites, and is also a
good benchmark for video-to-text generation. Recent work on this task adopts
encoder-decoder models to generate comments. However, these methods do not
model the interaction between videos and comments explicitly, so they tend to
generate popular comments that are often irrelevant to the videos. In this
work, we aim to improve the relevance between live comments and videos by
modeling the cross-modal interactions among different modalities. To this end,
we propose a multimodal matching transformer to capture the relationships among
comments, vision, and audio. The proposed model is based on the transformer
framework and can iteratively learn the attention-aware representations for
each modality. We evaluate the model on a publicly available live commenting
dataset. Experiments show that the multimodal matching transformer model
outperforms the state-of-the-art methods.
</summary>
    <author>
      <name>Chaoqun Duan</name>
    </author>
    <author>
      <name>Lei Cui</name>
    </author>
    <author>
      <name>Shuming Ma</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <author>
      <name>Conghui Zhu</name>
    </author>
    <author>
      <name>Tiejun Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2002.02649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.02649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.02562v2</id>
    <updated>2020-02-14T21:47:10Z</updated>
    <published>2020-02-07T00:04:04Z</published>
    <title>Transformer Transducer: A Streamable Speech Recognition Model with
  Transformer Encoders and RNN-T Loss</title>
    <summary>  In this paper we present an end-to-end speech recognition model with
Transformer encoders that can be used in a streaming speech recognition system.
Transformer computation blocks based on self-attention are used to encode both
audio and label sequences independently. The activations from both audio and
label encoders are combined with a feed-forward layer to compute a probability
distribution over the label space for every combination of acoustic frame
position and label history. This is similar to the Recurrent Neural Network
Transducer (RNN-T) model, which uses RNNs for information encoding instead of
Transformer encoders. The model is trained with the RNN-T loss well-suited to
streaming decoding. We present results on the LibriSpeech dataset showing that
limiting the left context for self-attention in the Transformer layers makes
decoding computationally tractable for streaming, with only a slight
degradation in accuracy. We also show that the full attention version of our
model beats the-state-of-the art accuracy on the LibriSpeech benchmarks. Our
results also show that we can bridge the gap between full attention and limited
attention versions of our model by attending to a limited number of future
frames.
</summary>
    <author>
      <name>Qian Zhang</name>
    </author>
    <author>
      <name>Han Lu</name>
    </author>
    <author>
      <name>Hasim Sak</name>
    </author>
    <author>
      <name>Anshuman Tripathi</name>
    </author>
    <author>
      <name>Erik McDermott</name>
    </author>
    <author>
      <name>Stephen Koo</name>
    </author>
    <author>
      <name>Shankar Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the final version of the paper submitted to the ICASSP 2020
  on Oct 21, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.02562v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.02562v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.02450v1</id>
    <updated>2020-02-05T22:56:12Z</updated>
    <published>2020-02-05T22:56:12Z</published>
    <title>Goal-Oriented Multi-Task BERT-Based Dialogue State Tracker</title>
    <summary>  Dialogue State Tracking (DST) is a core component of virtual assistants such
as Alexa or Siri. To accomplish various tasks, these assistants need to support
an increasing number of services and APIs. The Schema-Guided State Tracking
track of the 8th Dialogue System Technology Challenge highlighted the DST
problem for unseen services. The organizers introduced the Schema-Guided
Dialogue (SGD) dataset with multi-domain conversations and released a zero-shot
dialogue state tracking model. In this work, we propose a GOaL-Oriented
Multi-task BERT-based dialogue state tracker (GOLOMB) inspired by architectures
for reading comprehension question answering systems. The model "queries"
dialogue history with descriptions of slots and services as well as possible
values of slots. This allows to transfer slot values in multi-domain dialogues
and have a capability to scale to unseen slot types. Our model achieves a joint
goal accuracy of 53.97% on the SGD dataset, outperforming the baseline model.
</summary>
    <author>
      <name>Pavel Gulyaev</name>
    </author>
    <author>
      <name>Eugenia Elistratova</name>
    </author>
    <author>
      <name>Vasily Konovalov</name>
    </author>
    <author>
      <name>Yuri Kuratov</name>
    </author>
    <author>
      <name>Leonid Pugachev</name>
    </author>
    <author>
      <name>Mikhail Burtsev</name>
    </author>
    <link href="http://arxiv.org/abs/2002.02450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.02450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.01861v1</id>
    <updated>2020-02-05T16:45:44Z</updated>
    <published>2020-02-05T16:45:44Z</published>
    <title>Rapid Adaptation of BERT for Information Extraction on Domain-Specific
  Business Documents</title>
    <summary>  Techniques for automatically extracting important content elements from
business documents such as contracts, statements, and filings have the
potential to make business operations more efficient. This problem can be
formulated as a sequence labeling task, and we demonstrate the adaption of BERT
to two types of business documents: regulatory filings and property lease
agreements. There are aspects of this problem that make it easier than
"standard" information extraction tasks and other aspects that make it more
difficult, but on balance we find that modest amounts of annotated data (less
than 100 documents) are sufficient to achieve reasonable accuracy. We integrate
our models into an end-to-end cloud platform that provides both an easy-to-use
annotation interface as well as an inference interface that allows users to
upload documents and inspect model outputs.
</summary>
    <author>
      <name>Ruixue Zhang</name>
    </author>
    <author>
      <name>Wei Yang</name>
    </author>
    <author>
      <name>Luyun Lin</name>
    </author>
    <author>
      <name>Zhengkai Tu</name>
    </author>
    <author>
      <name>Yuqing Xie</name>
    </author>
    <author>
      <name>Zihang Fu</name>
    </author>
    <author>
      <name>Yuhao Xie</name>
    </author>
    <author>
      <name>Luchen Tan</name>
    </author>
    <author>
      <name>Kun Xiong</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2002.01861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.01861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.00571v1</id>
    <updated>2020-02-03T05:59:52Z</updated>
    <published>2020-02-03T05:59:52Z</published>
    <title>IART: Intent-aware Response Ranking with Transformers in
  Information-seeking Conversation Systems</title>
    <summary>  Personal assistant systems, such as Apple Siri, Google Assistant, Amazon
Alexa, and Microsoft Cortana, are becoming ever more widely used. Understanding
user intent such as clarification questions, potential answers and user
feedback in information-seeking conversations is critical for retrieving good
responses. In this paper, we analyze user intent patterns in
information-seeking conversations and propose an intent-aware neural response
ranking model "IART", which refers to "Intent-Aware Ranking with Transformers".
IART is built on top of the integration of user intent modeling and language
representation learning with the Transformer architecture, which relies
entirely on a self-attention mechanism instead of recurrent nets. It
incorporates intent-aware utterance attention to derive an importance weighting
scheme of utterances in conversation context with the aim of better
conversation history understanding. We conduct extensive experiments with three
information-seeking conversation data sets including both standard benchmarks
and commercial data. Our proposed model outperforms all baseline methods with
respect to a variety of metrics. We also perform case studies and analysis of
learned user intent and its impact on response ranking in information-seeking
conversations to provide interpretation of results.
</summary>
    <author>
      <name>Liu Yang</name>
    </author>
    <author>
      <name>Minghui Qiu</name>
    </author>
    <author>
      <name>Chen Qu</name>
    </author>
    <author>
      <name>Cen Chen</name>
    </author>
    <author>
      <name>Jiafeng Guo</name>
    </author>
    <author>
      <name>Yongfeng Zhang</name>
    </author>
    <author>
      <name>W. Bruce Croft</name>
    </author>
    <author>
      <name>Haiqing Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by WWW2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.00571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.00571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.00198v1</id>
    <updated>2020-02-01T12:36:55Z</updated>
    <published>2020-02-01T12:36:55Z</published>
    <title>Transforming Spectrum and Prosody for Emotional Voice Conversion with
  Non-Parallel Training Data</title>
    <summary>  Emotional voice conversion is to convert the spectrum and prosody to change
the emotional patterns of speech, while preserving the speaker identity and
linguistic content. Many studies require parallel speech data between different
emotional patterns, which is not practical in real life. Moreover, they often
model the conversion of fundamental frequency (F0) with a simple linear
transform. As F0 is a key aspect of intonation that is hierarchical in nature,
we believe that it is more adequate to model F0 in different temporal scales by
using wavelet transform. We propose a CycleGAN network to find an optimal
pseudo pair from non-parallel training data by learning forward and inverse
mappings simultaneously using adversarial and cycle-consistency losses. We also
study the use of continuous wavelet transform (CWT) to decompose F0 into ten
temporal scales, that describes speech prosody at different time resolution,
for effective F0 conversion. Experimental results show that our proposed
framework outperforms the baselines both in objective and subjective
evaluations.
</summary>
    <author>
      <name>Kun Zhou</name>
    </author>
    <author>
      <name>Berrak Sisman</name>
    </author>
    <author>
      <name>Haizhou Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Speaker Odyssey 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.00198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.00198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.00181v1</id>
    <updated>2020-02-01T10:00:06Z</updated>
    <published>2020-02-01T10:00:06Z</published>
    <title>Fine-Tuning BERT for Schema-Guided Zero-Shot Dialogue State Tracking</title>
    <summary>  We present our work on Track 4 in the Dialogue System Technology Challenges 8
(DSTC8). The DSTC8-Track 4 aims to perform dialogue state tracking (DST) under
the zero-shot settings, in which the model needs to generalize on unseen
service APIs given a schema definition of these target APIs. Serving as the
core for many virtual assistants such as Siri, Alexa, and Google Assistant, the
DST keeps track of the user's goal and what happened in the dialogue history,
mainly including intent prediction, slot filling, and user state tracking,
which tests models' ability of natural language understanding. Recently, the
pretrained language models have achieved state-of-the-art results and shown
impressive generalization ability on various NLP tasks, which provide a
promising way to perform zero-shot learning for language understanding. Based
on this, we propose a schema-guided paradigm for zero-shot dialogue state
tracking (SGP-DST) by fine-tuning BERT, one of the most popular pretrained
language models. The SGP-DST system contains four modules for intent
prediction, slot prediction, slot transfer prediction, and user state
summarizing respectively. According to the official evaluation results, our
SGP-DST (team12) ranked 3rd on the joint goal accuracy (primary evaluation
metric for ranking submissions) and 1st on the requsted slots F1 among 25
participant teams.
</summary>
    <author>
      <name>Yu-Ping Ruan</name>
    </author>
    <author>
      <name>Zhen-Hua Ling</name>
    </author>
    <author>
      <name>Jia-Chen Gu</name>
    </author>
    <author>
      <name>Quan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Present on the DSTC8 Workshop @ AAAI-2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.00181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.00181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.00163v1</id>
    <updated>2020-02-01T07:50:43Z</updated>
    <published>2020-02-01T07:50:43Z</published>
    <title>Bridging Text and Video: A Universal Multimodal Transformer for
  Video-Audio Scene-Aware Dialog</title>
    <summary>  Audio-Visual Scene-Aware Dialog (AVSD) is a task to generate responses when
chatting about a given video, which is organized as a track of the 8th Dialog
System Technology Challenge (DSTC8). To solve the task, we propose a universal
multimodal transformer and introduce the multi-task learning method to learn
joint representations among different modalities as well as generate
informative and fluent responses. Our method extends the natural language
generation pre-trained model to multimodal dialogue generation task. Our system
achieves the best performance in both objective and subjective evaluations in
the challenge.
</summary>
    <author>
      <name>Zekang Li</name>
    </author>
    <author>
      <name>Zongjia Li</name>
    </author>
    <author>
      <name>Jinchao Zhang</name>
    </author>
    <author>
      <name>Yang Feng</name>
    </author>
    <author>
      <name>Cheng Niu</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by AAAI2020 DSTC8 workshop. Ranked 1st in DSTC8-AVSD track</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.00163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.00163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.11985v1</id>
    <updated>2020-01-31T18:14:17Z</updated>
    <published>2020-01-31T18:14:17Z</published>
    <title>Pretrained Transformers for Simple Question Answering over Knowledge
  Graphs</title>
    <summary>  Answering simple questions over knowledge graphs is a well-studied problem in
question answering. Previous approaches for this task built on recurrent and
convolutional neural network based architectures that use pretrained word
embeddings. It was recently shown that finetuning pretrained transformer
networks (e.g. BERT) can outperform previous approaches on various natural
language processing tasks. In this work, we investigate how well BERT performs
on SimpleQuestions and provide an evaluation of both BERT and BiLSTM-based
models in datasparse scenarios.
</summary>
    <author>
      <name>D. Lukovnikov</name>
    </author>
    <author>
      <name>A. Fischer</name>
    </author>
    <author>
      <name>J. Lehmann</name>
    </author>
    <link href="http://arxiv.org/abs/2001.11985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.11316v2</id>
    <updated>2020-01-31T12:33:57Z</updated>
    <published>2020-01-30T13:53:58Z</published>
    <title>Adversarial Training for Aspect-Based Sentiment Analysis with BERT</title>
    <summary>  Aspect-Based Sentiment Analysis (ABSA) deals with the extraction of
sentiments and their targets. Collecting labeled data for this task in order to
help neural networks generalize better can be laborious and time-consuming. As
an alternative, similar data to the real-world examples can be produced
artificially through an adversarial process which is carried out in the
embedding space. Although these examples are not real sentences, they have been
shown to act as a regularization method which can make neural networks more
robust. In this work, we apply adversarial training, which was put forward by
Goodfellow et al. (2014), to the post-trained BERT (BERT-PT) language model
proposed by Xu et al. (2019) on the two major tasks of Aspect Extraction and
Aspect Sentiment Classification in sentiment analysis. After improving the
results of post-trained BERT by an ablation study, we propose a novel
architecture called BERT Adversarial Training (BAT) to utilize adversarial
training in ABSA. The proposed model outperforms post-trained BERT in both
tasks. To the best of our knowledge, this is the first study on the application
of adversarial training in ABSA.
</summary>
    <author>
      <name>Akbar Karimi</name>
    </author>
    <author>
      <name>Leonardo Rossi</name>
    </author>
    <author>
      <name>Andrea Prati</name>
    </author>
    <author>
      <name>Katharina Full</name>
    </author>
    <link href="http://arxiv.org/abs/2001.11316v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11316v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.11268v1</id>
    <updated>2020-01-30T11:45:59Z</updated>
    <published>2020-01-30T11:45:59Z</published>
    <title>Data Mining in Clinical Trial Text: Transformers for Classification and
  Question Answering Tasks</title>
    <summary>  This research on data extraction methods applies recent advances in natural
language processing to evidence synthesis based on medical texts. Texts of
interest include abstracts of clinical trials in English and in multilingual
contexts. The main focus is on information characterized via the Population,
Intervention, Comparator, and Outcome (PICO) framework, but data extraction is
not limited to these fields. Recent neural network architectures based on
transformers show capacities for transfer learning and increased performance on
downstream natural language processing tasks such as universal reading
comprehension, brought forward by this architecture's use of contextualized
word embeddings and self-attention mechanisms. This paper contributes to
solving problems related to ambiguity in PICO sentence prediction tasks, as
well as highlighting how annotations for training named entity recognition
systems are used to train a high-performing, but nevertheless flexible
architecture for question answering in systematic review automation.
Additionally, it demonstrates how the problem of insufficient amounts of
training annotations for PICO entity extraction is tackled by augmentation. All
models in this paper were created with the aim to support systematic review
(semi)automation. They achieve high F1 scores, and demonstrate the feasibility
of applying transformer-based classification methods to support data mining in
the biomedical literature.
</summary>
    <author>
      <name>Lena Schmidt</name>
    </author>
    <author>
      <name>Julie Weeds</name>
    </author>
    <author>
      <name>Julian P. T. Higgins</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">HEALTHINF 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2001.11268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09309v1</id>
    <updated>2020-01-25T13:35:34Z</updated>
    <published>2020-01-25T13:35:34Z</published>
    <title>Further Boosting BERT-based Models by Duplicating Existing Layers: Some
  Intriguing Phenomena inside BERT</title>
    <summary>  Although Bidirectional Encoder Representations from Transformers (BERT) have
achieved tremendous success in many natural language processing (NLP) tasks, it
remains a black box, so much previous work has tried to lift the veil of BERT
and understand the functionality of each layer. In this paper, we found that
removing or duplicating most layers in BERT would not change their outputs.
This fact remains true across a wide variety of BERT-based models. Based on
this observation, we propose a quite simple method to boost the performance of
BERT. By duplicating some layers in the BERT-based models to make it deeper (no
extra training required in this step), they obtain better performance in the
down-stream tasks after fine-tuning.
</summary>
    <author>
      <name>Wei-Tsung Kao</name>
    </author>
    <author>
      <name>Tsung-Han Wu</name>
    </author>
    <author>
      <name>Po-Han Chi</name>
    </author>
    <author>
      <name>Chun-Cheng Hsieh</name>
    </author>
    <author>
      <name>Hung-Yi Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 8 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.09309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.08950v1</id>
    <updated>2020-01-24T11:36:12Z</updated>
    <published>2020-01-24T11:36:12Z</published>
    <title>PoWER-BERT: Accelerating BERT inference for Classification Tasks</title>
    <summary>  BERT has emerged as a popular model for natural language understanding. Given
its compute intensive nature, even for inference, many recent studies have
considered optimization of two important performance characteristics: model
size and inference time. We consider classification tasks and propose a novel
method, called PoWER-BERT, for improving the inference time for the BERT model
without significant loss in the accuracy. The method works by eliminating
word-vectors (intermediate vector outputs) from the encoder pipeline. We design
a strategy for measuring the significance of the word-vectors based on the
self-attention mechanism of the encoders which helps us identify the
word-vectors to be eliminated. Experimental evaluation on the standard GLUE
benchmark shows that PoWER-BERT achieves up to 4.5x reduction in inference time
over BERT with &lt; 1% loss in accuracy. We show that compared to the prior
inference time reduction methods, PoWER-BERT offers better trade-off between
accuracy and inference time. Lastly, we demonstrate that our scheme can also be
used in conjunction with ALBERT (a highly compressed version of BERT) and can
attain up to 6.8x factor reduction in inference time with &lt; 1% loss in
accuracy.
</summary>
    <author>
      <name>Saurabh Goyal</name>
    </author>
    <author>
      <name>Anamitra Roy Choudhary</name>
    </author>
    <author>
      <name>Venkatesan Chakaravarthy</name>
    </author>
    <author>
      <name>Saurabh ManishRaje</name>
    </author>
    <author>
      <name>Yogish Sabharwal</name>
    </author>
    <author>
      <name>Ashish Verma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.08950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.08950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.08904v1</id>
    <updated>2020-01-24T07:16:32Z</updated>
    <published>2020-01-24T07:16:32Z</published>
    <title>MT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition
  using Deep Bidirectional Transformers</title>
    <summary>  Conversational agents such as Cortana, Alexa and Siri are continuously
working on increasing their capabilities by adding new domains. The support of
a new domain includes the design and development of a number of NLU components
for domain classification, intents classification and slots tagging (including
named entity recognition). Each component only performs well when trained on a
large amount of labeled data. Second, these components are deployed on
limited-memory devices which requires some model compression. Third, for some
domains such as the health domain, it is hard to find a single training data
set that covers all the required slot types. To overcome these mentioned
problems, we present a multi-task transformer-based neural architecture for
slot tagging. We consider the training of a slot tagger using multiple data
sets covering different slot types as a multi-task learning problem. The
experimental results on the biomedical domain have shown that the proposed
approach outperforms the previous state-of-the-art systems for slot tagging on
the different benchmark biomedical datasets in terms of (time and memory)
efficiency and effectiveness. The output slot tagger can be used by the
conversational agent to better identify entities in the input utterances.
</summary>
    <author>
      <name>Muhammad Raza Khan</name>
    </author>
    <author>
      <name>Morteza Ziyadi</name>
    </author>
    <author>
      <name>Mohamed AbdelHady</name>
    </author>
    <link href="http://arxiv.org/abs/2001.08904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.08904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.08764v1</id>
    <updated>2020-01-23T19:06:18Z</updated>
    <published>2020-01-23T19:06:18Z</published>
    <title>Fine-Tuning a Transformer-Based Language Model to Avoid Generating
  Non-Normative Text</title>
    <summary>  Large-scale, transformer-based language models such as GPT-2 are pretrained
on diverse corpora scraped from the internet. Consequently, they are prone to
generating content that one might find inappropriate or non-normative (i.e. in
violation of social norms). In this paper, we describe a technique for
fine-tuning GPT-2 such that the amount of non-normative content generated is
significantly reduced. A model capable of classifying normative behavior is
used to produce an additional reward signal; a policy gradient reinforcement
learning technique uses that reward to fine-tune the language model weights.
Using this fine-tuning technique, with 24,000 sentences from a science fiction
plot summary dataset, halves the percentage of generated text containing
non-normative behavior from 35.1% to 15.7%.
</summary>
    <author>
      <name>Xiangyu Peng</name>
    </author>
    <author>
      <name>Siyan Li</name>
    </author>
    <author>
      <name>Spencer Frazier</name>
    </author>
    <author>
      <name>Mark Riedl</name>
    </author>
    <link href="http://arxiv.org/abs/2001.08764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.08764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.07234v1</id>
    <updated>2020-01-20T20:02:02Z</updated>
    <published>2020-01-20T20:02:02Z</published>
    <title>Multi-level Head-wise Match and Aggregation in Transformer for Textual
  Sequence Matching</title>
    <summary>  Transformer has been successfully applied to many natural language processing
tasks. However, for textual sequence matching, simple matching between the
representation of a pair of sequences might bring in unnecessary noise. In this
paper, we propose a new approach to sequence pair matching with Transformer, by
learning head-wise matching representations on multiple levels. Experiments
show that our proposed approach can achieve new state-of-the-art performance on
multiple tasks that rely only on pre-computed sequence-vector-representation,
such as SNLI, MNLI-match, MNLI-mismatch, QQP, and SQuAD-binary.
</summary>
    <author>
      <name>Shuohang Wang</name>
    </author>
    <author>
      <name>Yunshi Lan</name>
    </author>
    <author>
      <name>Yi Tay</name>
    </author>
    <author>
      <name>Jing Jiang</name>
    </author>
    <author>
      <name>Jingjing Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2020, 8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.07234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.07234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05540v1</id>
    <updated>2020-01-15T20:26:48Z</updated>
    <published>2020-01-15T20:26:48Z</published>
    <title>Insertion-Deletion Transformer</title>
    <summary>  We propose the Insertion-Deletion Transformer, a novel transformer-based
neural architecture and training method for sequence generation. The model
consists of two phases that are executed iteratively, 1) an insertion phase and
2) a deletion phase. The insertion phase parameterizes a distribution of
insertions on the current output hypothesis, while the deletion phase
parameterizes a distribution of deletions over the current output hypothesis.
The training method is a principled and simple algorithm, where the deletion
model obtains its signal directly on-policy from the insertion model output. We
demonstrate the effectiveness of our Insertion-Deletion Transformer on
synthetic translation tasks, obtaining significant BLEU score improvement over
an insertion-only model.
</summary>
    <author>
      <name>Laura Ruis</name>
    </author>
    <author>
      <name>Mitchell Stern</name>
    </author>
    <author>
      <name>Julia Proskurnia</name>
    </author>
    <author>
      <name>William Chan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as an Extended Abstract at the Workshop of Neural Generation
  and Translation (WNGT 2019) at EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.05540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05136v1</id>
    <updated>2020-01-15T05:32:18Z</updated>
    <published>2020-01-15T05:32:18Z</published>
    <title>Parallel Machine Translation with Disentangled Context Transformer</title>
    <summary>  State-of-the-art neural machine translation models generate a translation
from left to right and every step is conditioned on the previously generated
tokens. The sequential nature of this generation process causes fundamental
latency in inference since we cannot generate multiple tokens in each sentence
in parallel. We propose an attention-masking based model, called Disentangled
Context (DisCo) transformer, that simultaneously generates all tokens given
different contexts. The DisCo transformer is trained to predict every output
token given an arbitrary subset of the other reference tokens. We also develop
the parallel easy-first inference algorithm, which iteratively refines every
token in parallel and reduces the number of required iterations. Our extensive
experiments on 7 directions with varying data sizes demonstrate that our model
achieves competitive, if not better, performance compared to the state of the
art in non-autoregressive machine translation while significantly reducing
decoding time on average.
</summary>
    <author>
      <name>Jungo Kasai</name>
    </author>
    <author>
      <name>James Cross</name>
    </author>
    <author>
      <name>Marjan Ghazvininejad</name>
    </author>
    <author>
      <name>Jiatao Gu</name>
    </author>
    <link href="http://arxiv.org/abs/2001.05136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05308v1</id>
    <updated>2020-01-14T17:24:41Z</updated>
    <published>2020-01-14T17:24:41Z</published>
    <title>Auto Completion of User Interface Layout Design Using Transformer-Based
  Tree Decoders</title>
    <summary>  It has been of increasing interest in the field to develop automatic
machineries to facilitate the design process. In this paper, we focus on
assisting graphical user interface (UI) layout design, a crucial task in app
development. Given a partial layout, which a designer has entered, our model
learns to complete the layout by predicting the remaining UI elements with a
correct position and dimension as well as the hierarchical structures. Such
automation will significantly ease the effort of UI designers and developers.
While we focus on interface layout prediction, our model can be generally
applicable for other layout prediction problems that involve tree structures
and 2-dimensional placements. Particularly, we design two versions of
Transformer-based tree decoders: Pointer and Recursive Transformer, and
experiment with these models on a public dataset. We also propose several
metrics for measuring the accuracy of tree prediction and ground these metrics
in the domain of user experience. These contribute a new task and methods to
deep learning research.
</summary>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Julien Amelot</name>
    </author>
    <author>
      <name>Xin Zhou</name>
    </author>
    <author>
      <name>Samy Bengio</name>
    </author>
    <author>
      <name>Si Si</name>
    </author>
    <link href="http://arxiv.org/abs/2001.05308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05326v1</id>
    <updated>2020-01-14T13:50:08Z</updated>
    <published>2020-01-14T13:50:08Z</published>
    <title>A BERT based Sentiment Analysis and Key Entity Detection Approach for
  Online Financial Texts</title>
    <summary>  The emergence and rapid progress of the Internet have brought ever-increasing
impact on financial domain. How to rapidly and accurately mine the key
information from the massive negative financial texts has become one of the key
issues for investors and decision makers. Aiming at the issue, we propose a
sentiment analysis and key entity detection approach based on BERT, which is
applied in online financial text mining and public opinion analysis in social
media. By using pre-train model, we first study sentiment analysis, and then we
consider key entity detection as a sentence matching or Machine Reading
Comprehension (MRC) task in different granularity. Among them, we mainly focus
on negative sentimental information. We detect the specific entity by using our
approach, which is different from traditional Named Entity Recognition (NER).
In addition, we also use ensemble learning to improve the performance of
proposed approach. Experimental results show that the performance of our
approach is generally higher than SVM, LR, NBM, and BERT for two financial
sentiment analysis and key entity detection datasets.
</summary>
    <author>
      <name>Lingyun Zhao</name>
    </author>
    <author>
      <name>Lin Li</name>
    </author>
    <author>
      <name>Xinhao Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/2001.05326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.04589v1</id>
    <updated>2020-01-14T02:14:09Z</updated>
    <published>2020-01-14T02:14:09Z</published>
    <title>Faster Transformer Decoding: N-gram Masked Self-Attention</title>
    <summary>  Motivated by the fact that most of the information relevant to the prediction
of target tokens is drawn from the source sentence $S=s_1, \ldots, s_S$, we
propose truncating the target-side window used for computing self-attention by
making an $N$-gram assumption. Experiments on WMT EnDe and EnFr data sets show
that the $N$-gram masked self-attention model loses very little in BLEU score
for $N$ values in the range $4, \ldots, 8$, depending on the task.
</summary>
    <author>
      <name>Ciprian Chelba</name>
    </author>
    <author>
      <name>Mia Chen</name>
    </author>
    <author>
      <name>Ankur Bapna</name>
    </author>
    <author>
      <name>Noam Shazeer</name>
    </author>
    <link href="http://arxiv.org/abs/2001.04589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.04451v2</id>
    <updated>2020-02-18T16:01:18Z</updated>
    <published>2020-01-13T18:38:28Z</published>
    <title>Reformer: The Efficient Transformer</title>
    <summary>  Large Transformer models routinely achieve state-of-the-art results on a
number of tasks but training these models can be prohibitively costly,
especially on long sequences. We introduce two techniques to improve the
efficiency of Transformers. For one, we replace dot-product attention by one
that uses locality-sensitive hashing, changing its complexity from O($L^2$) to
O($L\log L$), where $L$ is the length of the sequence. Furthermore, we use
reversible residual layers instead of the standard residuals, which allows
storing activations only once in the training process instead of $N$ times,
where $N$ is the number of layers. The resulting model, the Reformer, performs
on par with Transformer models while being much more memory-efficient and much
faster on long sequences.
</summary>
    <author>
      <name>Nikita Kitaev</name>
    </author>
    <author>
      <name>≈Åukasz Kaiser</name>
    </author>
    <author>
      <name>Anselm Levskaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.04451v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04451v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.04246v1</id>
    <updated>2020-01-13T14:03:26Z</updated>
    <published>2020-01-13T14:03:26Z</published>
    <title>AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural
  Architecture Search</title>
    <summary>  Large pre-trained language models such as BERT have shown their effectiveness
in various natural language processing tasks. However, the huge parameter size
makes them difficult to be deployed in real-time applications that require
quick inference with limited resources. Existing methods compress BERT into
small models while such compression is task-independent, i.e., the same
compressed BERT for all different downstream tasks. Motivated by the necessity
and benefits of task-oriented BERT compression, we propose a novel compression
method, AdaBERT, that leverages differentiable Neural Architecture Search to
automatically compress BERT into task-adaptive small models for specific tasks.
We incorporate a task-oriented knowledge distillation loss to provide search
hints and an efficiency-aware loss as search constraints, which enables a good
trade-off between efficiency and effectiveness for task-adaptive BERT
compression. We evaluate AdaBERT on several NLP tasks, and the results
demonstrate that those task-adaptive compressed models are 12.7x to 29.3x
faster than BERT in inference time and 11.5x to 17.0x smaller in terms of
parameter size, while comparable performance is maintained.
</summary>
    <author>
      <name>Daoyuan Chen</name>
    </author>
    <author>
      <name>Yaliang Li</name>
    </author>
    <author>
      <name>Minghui Qiu</name>
    </author>
    <author>
      <name>Zhen Wang</name>
    </author>
    <author>
      <name>Bofang Li</name>
    </author>
    <author>
      <name>Bolin Ding</name>
    </author>
    <author>
      <name>Hongbo Deng</name>
    </author>
    <author>
      <name>Jun Huang</name>
    </author>
    <author>
      <name>Wei Lin</name>
    </author>
    <author>
      <name>Jingren Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2001.04246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.03521v1</id>
    <updated>2020-01-10T15:45:59Z</updated>
    <published>2020-01-10T15:45:59Z</published>
    <title>Towards Minimal Supervision BERT-based Grammar Error Correction</title>
    <summary>  Current grammatical error correction (GEC) models typically consider the task
as sequence generation, which requires large amounts of annotated data and
limit the applications in data-limited settings. We try to incorporate
contextual information from pre-trained language model to leverage annotation
and benefit multilingual scenarios. Results show strong potential of
Bidirectional Encoder Representations from Transformers (BERT) in grammatical
error correction task.
</summary>
    <author>
      <name>Yiyuan Li</name>
    </author>
    <author>
      <name>Antonios Anastasopoulos</name>
    </author>
    <author>
      <name>Alan W Black</name>
    </author>
    <link href="http://arxiv.org/abs/2001.03521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.03521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.03303v1</id>
    <updated>2020-01-10T04:39:44Z</updated>
    <published>2020-01-10T04:39:44Z</published>
    <title>Linking Social Media Posts to News with Siamese Transformers</title>
    <summary>  Many computational social science projects examine online discourse
surrounding a specific trending topic. These works often involve the
acquisition of large-scale corpora relevant to the event in question to analyze
aspects of the response to the event. Keyword searches present a
precision-recall trade-off and crowd-sourced annotations, while effective, are
costly. This work aims to enable automatic and accurate ad-hoc retrieval of
comments discussing a trending topic from a large corpus, using only a handful
of seed news articles.
</summary>
    <author>
      <name>Jacob Danovitch</name>
    </author>
    <link href="http://arxiv.org/abs/2001.03303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.03303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.02885v1</id>
    <updated>2020-01-09T08:43:30Z</updated>
    <published>2020-01-09T08:43:30Z</published>
    <title>Resolving the Scope of Speculation and Negation using Transformer-Based
  Architectures</title>
    <summary>  Speculation is a naturally occurring phenomena in textual data, forming an
integral component of many systems, especially in the biomedical information
retrieval domain. Previous work addressing cue detection and scope resolution
(the two subtasks of speculation detection) have ranged from rule-based systems
to deep learning-based approaches. In this paper, we apply three popular
transformer-based architectures, BERT, XLNet and RoBERTa to this task, on two
publicly available datasets, BioScope Corpus and SFU Review Corpus, reporting
substantial improvements over previously reported results (by at least 0.29 F1
points on cue detection and 4.27 F1 points on scope resolution). We also
experiment with joint training of the model on multiple datasets, which
outperforms the single dataset training approach by a good margin. We observe
that XLNet consistently outperforms BERT and RoBERTa, contrary to results on
other benchmark datasets. To confirm this observation, we apply XLNet and
RoBERTa to negation detection and scope resolution, reporting state-of-the-art
results on negation scope resolution for the BioScope Corpus (increase of 3.16
F1 points on the BioScope Full Papers, 0.06 F1 points on the BioScope
Abstracts) and the SFU Review Corpus (increase of 0.3 F1 points).
</summary>
    <author>
      <name>Benita Kathleen Britto</name>
    </author>
    <author>
      <name>Aditya Khandelwal</name>
    </author>
    <link href="http://arxiv.org/abs/2001.02885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.02885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.02674v3</id>
    <updated>2020-02-27T15:10:13Z</updated>
    <published>2020-01-08T18:58:02Z</published>
    <title>Streaming automatic speech recognition with the transformer model</title>
    <summary>  Encoder-decoder based sequence-to-sequence models have demonstrated
state-of-the-art results in end-to-end automatic speech recognition (ASR).
Recently, the transformer architecture, which uses self-attention to model
temporal context information, has been shown to achieve significantly lower
word error rates (WERs) compared to recurrent neural network (RNN) based system
architectures. Despite its success, the practical usage is limited to offline
ASR tasks, since encoder-decoder architectures typically require an entire
speech utterance as input. In this work, we propose a transformer based
end-to-end ASR system for streaming ASR, where an output must be generated
shortly after each spoken word. To achieve this, we apply time-restricted
self-attention for the encoder and triggered attention for the encoder-decoder
attention mechanism. Our proposed streaming transformer architecture achieves
2.8% and 7.3% WER for the clean and other test data of LibriSpeech, which to
our knowledge is the best published streaming end-to-end ASR result for this
task.
</summary>
    <author>
      <name>Niko Moritz</name>
    </author>
    <author>
      <name>Takaaki Hori</name>
    </author>
    <author>
      <name>Jonathan Le Roux</name>
    </author>
    <link href="http://arxiv.org/abs/2001.02674v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.02674v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.02524v1</id>
    <updated>2020-01-08T13:44:10Z</updated>
    <published>2020-01-08T13:44:10Z</published>
    <title>LTP: A New Active Learning Strategy for Bert-CRF Based Named Entity
  Recognition</title>
    <summary>  In recent years, deep learning has achieved great success in many natural
language processing tasks including named entity recognition. The shortcoming
is that a large amount of manually-annotated data is usually required. Previous
studies have demonstrated that both transfer learning and active learning could
elaborately reduce the cost of data annotation in terms of their corresponding
advantages, but there is still plenty of room for improvement. We assume that
the convergence of the two methods can complement with each other, so that the
model could be trained more accurately with less labelled data, and active
learning method could enhance transfer learning method to accurately select the
minimum data samples for iterative learning. However, in real applications we
found this approach is challenging because the sample selection of traditional
active learning strategy merely depends on the final probability value of its
model output, and this makes it quite difficult to evaluate the quality of the
selected data samples. In this paper, we first examine traditional active
learning strategies in a specific case of BERT-CRF that has been widely used in
named entity recognition. Then we propose an uncertainty-based active learning
strategy called Lowest Token Probability (LTP) which considers not only the
final output but also the intermediate results. We test LTP on multiple
datasets, and the experiments show that LTP performs better than traditional
strategies (incluing LC and NLC) on both token-level $F_1$ and sentence-level
accuracy, especially in complex imbalanced datasets.
</summary>
    <author>
      <name>Mingyi Liu</name>
    </author>
    <author>
      <name>Zhiying Tu</name>
    </author>
    <author>
      <name>Zhongjie Wang</name>
    </author>
    <author>
      <name>Xiaofei Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2001.02524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.02524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01140v1</id>
    <updated>2020-01-04T23:27:59Z</updated>
    <published>2020-01-04T23:27:59Z</published>
    <title>Transformer-based language modeling and decoding for conversational
  speech recognition</title>
    <summary>  We propose a way to use a transformer-based language model in conversational
speech recognition. Specifically, we focus on decoding efficiently in a
weighted finite-state transducer framework. We showcase an approach to lattice
re-scoring that allows for longer range history captured by a transfomer-based
language model and takes advantage of a transformer's ability to avoid
computing sequentially.
</summary>
    <author>
      <name>Kareem Nassar</name>
    </author>
    <link href="http://arxiv.org/abs/2001.01140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00926v1</id>
    <updated>2020-01-03T18:40:35Z</updated>
    <published>2020-01-03T18:40:35Z</published>
    <title>Learning Accurate Integer Transformer Machine-Translation Models</title>
    <summary>  We describe a method for training accurate Transformer machine-translation
models to run inference using 8-bit integer (INT8) hardware matrix multipliers,
as opposed to the more costly single-precision floating-point (FP32) hardware.
Unlike previous work, which converted only 85 Transformer matrix
multiplications to INT8, leaving 48 out of 133 of them in FP32 because of
unacceptable accuracy loss, we convert them all to INT8 without compromising
accuracy. Tested on the newstest2014 English-to-German translation task, our
INT8 Transformer Base and Transformer Big models yield BLEU scores that are
99.3% to 100% relative to those of the corresponding FP32 models. Our approach
converts all matrix-multiplication tensors from an existing FP32 model into
INT8 tensors by automatically making range-precision trade-offs during
training. To demonstrate the robustness of this approach, we also include
results from INT6 Transformer models.
</summary>
    <author>
      <name>Ephrem Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00891v1</id>
    <updated>2020-01-03T17:06:41Z</updated>
    <published>2020-01-03T17:06:41Z</published>
    <title>Two-Level Transformer and Auxiliary Coherence Modeling for Improved Text
  Segmentation</title>
    <summary>  Breaking down the structure of long texts into semantically coherent segments
makes the texts more readable and supports downstream applications like
summarization and retrieval. Starting from an apparent link between text
coherence and segmentation, we introduce a novel supervised model for text
segmentation with simple but explicit coherence modeling. Our model -- a neural
architecture consisting of two hierarchically connected Transformer networks --
is a multi-task learning model that couples the sentence-level segmentation
objective with the coherence objective that differentiates correct sequences of
sentences from corrupt ones. The proposed model, dubbed Coherence-Aware Text
Segmentation (CATS), yields state-of-the-art segmentation performance on a
collection of benchmark datasets. Furthermore, by coupling CATS with
cross-lingual word embeddings, we demonstrate its effectiveness in zero-shot
language transfer: it can successfully segment texts in languages unseen in
training.
</summary>
    <author>
      <name>Goran Glava≈°</name>
    </author>
    <author>
      <name>Swapna Somasundaran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.00891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01557v1</id>
    <updated>2020-01-02T15:04:08Z</updated>
    <published>2020-01-02T15:04:08Z</published>
    <title>Speaker-aware speech-transformer</title>
    <summary>  Recently, end-to-end (E2E) models become a competitive alternative to the
conventional hybrid automatic speech recognition (ASR) systems. However, they
still suffer from speaker mismatch in training and testing condition. In this
paper, we use Speech-Transformer (ST) as the study platform to investigate
speaker aware training of E2E models. We propose a model called Speaker-Aware
Speech-Transformer (SAST), which is a standard ST equipped with a speaker
attention module (SAM). The SAM has a static speaker knowledge block (SKB) that
is made of i-vectors. At each time step, the encoder output attends to the
i-vectors in the block, and generates a weighted combined speaker embedding
vector, which helps the model to normalize the speaker variations. The SAST
model trained in this way becomes independent of specific training speakers and
thus generalizes better to unseen testing speakers. We investigate different
factors of SAM. Experimental results on the AISHELL-1 task show that SAST
achieves a relative 6.5% CER reduction (CERR) over the speaker-independent (SI)
baseline. Moreover, we demonstrate that SAST still works quite well even if the
i-vectors in SKB all come from a different data source other than the acoustic
training set.
</summary>
    <author>
      <name>Zhiyun Fan</name>
    </author>
    <author>
      <name>Jie Li</name>
    </author>
    <author>
      <name>Shiyu Zhou</name>
    </author>
    <author>
      <name>Bo Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2001.01557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.11637v1</id>
    <updated>2019-12-25T10:59:31Z</updated>
    <published>2019-12-25T10:59:31Z</published>
    <title>Explicit Sparse Transformer: Concentrated Attention Through Explicit
  Selection</title>
    <summary>  Self-attention based Transformer has demonstrated the state-of-the-art
performances in a number of natural language processing tasks. Self-attention
is able to model long-term dependencies, but it may suffer from the extraction
of irrelevant information in the context. To tackle the problem, we propose a
novel model called \textbf{Explicit Sparse Transformer}. Explicit Sparse
Transformer is able to improve the concentration of attention on the global
context through an explicit selection of the most relevant segments. Extensive
experimental results on a series of natural language processing and computer
vision tasks, including neural machine translation, image captioning, and
language modeling, all demonstrate the advantages of Explicit Sparse
Transformer in model performance. We also show that our proposed sparse
attention method achieves comparable or better results than the previous sparse
attention method, but significantly reduces training and testing time. For
example, the inference speed is twice that of sparsemax in Transformer model.
Code will be available at
\url{https://github.com/lancopku/Explicit-Sparse-Transformer}
</summary>
    <author>
      <name>Guangxiang Zhao</name>
    </author>
    <author>
      <name>Junyang Lin</name>
    </author>
    <author>
      <name>Zhiyuan Zhang</name>
    </author>
    <author>
      <name>Xuancheng Ren</name>
    </author>
    <author>
      <name>Qi Su</name>
    </author>
    <author>
      <name>Xu Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1912.11637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.09582v1</id>
    <updated>2019-12-19T22:59:26Z</updated>
    <published>2019-12-19T22:59:26Z</published>
    <title>BERTje: A Dutch BERT Model</title>
    <summary>  The transformer-based pre-trained language model BERT has helped to improve
state-of-the-art performance on many natural language processing (NLP) tasks.
Using the same architecture and parameters, we developed and evaluated a
monolingual Dutch BERT model called BERTje. Compared to the multilingual BERT
model, which includes Dutch but is only based on Wikipedia text, BERTje is
based on a large and diverse dataset of 2.4 billion tokens. BERTje consistently
outperforms the equally-sized multilingual BERT model on downstream NLP tasks
(part-of-speech tagging, named-entity recognition, semantic role labeling, and
sentiment analysis). Our pre-trained Dutch BERT model is made available at
https://github.com/wietsedv/bertje.
</summary>
    <author>
      <name>Wietse de Vries</name>
    </author>
    <author>
      <name>Andreas van Cranenburgh</name>
    </author>
    <author>
      <name>Arianna Bisazza</name>
    </author>
    <author>
      <name>Tommaso Caselli</name>
    </author>
    <author>
      <name>Gertjan van Noord</name>
    </author>
    <author>
      <name>Malvina Nissim</name>
    </author>
    <link href="http://arxiv.org/abs/1912.09582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.09582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.08226v1</id>
    <updated>2019-12-17T19:03:23Z</updated>
    <published>2019-12-17T19:03:23Z</published>
    <title>M$^2$: Meshed-Memory Transformer for Image Captioning</title>
    <summary>  Transformer-based architectures represent the state of the art in sequence
modeling tasks like machine translation and language understanding. Their
applicability to multi-modal contexts like image captioning, however, is still
largely under-explored. With the aim of filling this gap, we present M$^2$ - a
Meshed Transformer with Memory for Image Captioning. The architecture improves
both the image encoding and the language generation steps: it learns a
multi-level representation of the relationships between image regions
integrating learned a priori knowledge, and uses a mesh-like connectivity at
decoding stage to exploit low- and high-level features. Experimentally, we
investigate the performance of the M$^2$ Transformer and different
fully-attentive models in comparison with recurrent ones. When tested on COCO,
our proposal achieves a new state of the art in single-model and ensemble
configurations on the "Karpathy" test split and on the online test server. We
also assess its performances when describing objects unseen in the training
set. Trained models and code for reproducing the experiments are publicly
available at: https://github.com/aimagelab/meshed-memory-transformer.
</summary>
    <author>
      <name>Marcella Cornia</name>
    </author>
    <author>
      <name>Matteo Stefanini</name>
    </author>
    <author>
      <name>Lorenzo Baraldi</name>
    </author>
    <author>
      <name>Rita Cucchiara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Source code: https://github.com/aimagelab/meshed-memory-transformer</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.08226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.07840v2</id>
    <updated>2020-02-15T18:48:42Z</updated>
    <published>2019-12-17T06:53:05Z</published>
    <title>Cross-Lingual Ability of Multilingual BERT: An Empirical Study</title>
    <summary>  Recent work has exhibited the surprising cross-lingual abilities of
multilingual BERT (M-BERT) -- surprising since it is trained without any
cross-lingual objective and with no aligned data. In this work, we provide a
comprehensive study of the contribution of different components in M-BERT to
its cross-lingual ability. We study the impact of linguistic properties of the
languages, the architecture of the model, and the learning objectives. The
experimental study is done in the context of three typologically different
languages -- Spanish, Hindi, and Russian -- and using two conceptually
different NLP tasks, textual entailment and named entity recognition. Among our
key conclusions is the fact that the lexical overlap between languages plays a
negligible role in the cross-lingual success, while the depth of the network is
an integral part of it. All our models and implementations can be found on our
project page: http://cogcomp.org/page/publication_view/900 .
</summary>
    <author>
      <name>Karthikeyan K</name>
    </author>
    <author>
      <name>Zihan Wang</name>
    </author>
    <author>
      <name>Stephen Mayhew</name>
    </author>
    <author>
      <name>Dan Roth</name>
    </author>
    <link href="http://arxiv.org/abs/1912.07840v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.07840v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.07076v1</id>
    <updated>2019-12-15T17:50:56Z</updated>
    <published>2019-12-15T17:50:56Z</published>
    <title>Multilingual is not enough: BERT for Finnish</title>
    <summary>  Deep learning-based language models pretrained on large unannotated text
corpora have been demonstrated to allow efficient transfer learning for natural
language processing, with recent approaches such as the transformer-based BERT
model advancing the state of the art across a variety of tasks. While most work
on these models has focused on high-resource languages, in particular English,
a number of recent efforts have introduced multilingual models that can be
fine-tuned to address tasks in a large number of different languages. However,
we still lack a thorough understanding of the capabilities of these models, in
particular for lower-resourced languages. In this paper, we focus on Finnish
and thoroughly evaluate the multilingual BERT model on a range of tasks,
comparing it with a new Finnish BERT model trained from scratch. The new
language-specific model is shown to systematically and clearly outperform the
multilingual. While the multilingual model largely fails to reach the
performance of previously proposed methods, the custom Finnish BERT model
establishes new state-of-the-art results on all corpora for all reference
tasks: part-of-speech tagging, named entity recognition, and dependency
parsing. We release the model and all related resources created for this study
with open licenses at https://turkunlp.org/finbert .
</summary>
    <author>
      <name>Antti Virtanen</name>
    </author>
    <author>
      <name>Jenna Kanerva</name>
    </author>
    <author>
      <name>Rami Ilo</name>
    </author>
    <author>
      <name>Jouni Luoma</name>
    </author>
    <author>
      <name>Juhani Luotolahti</name>
    </author>
    <author>
      <name>Tapio Salakoski</name>
    </author>
    <author>
      <name>Filip Ginter</name>
    </author>
    <author>
      <name>Sampo Pyysalo</name>
    </author>
    <link href="http://arxiv.org/abs/1912.07076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.07076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06813v1</id>
    <updated>2019-12-14T09:30:52Z</updated>
    <published>2019-12-14T09:30:52Z</published>
    <title>Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using
  Transformer with Text-to-Speech Pretraining</title>
    <summary>  We introduce a novel sequence-to-sequence (seq2seq) voice conversion (VC)
model based on the Transformer architecture with text-to-speech (TTS)
pretraining. Seq2seq VC models are attractive owing to their ability to convert
prosody. While seq2seq models based on recurrent neural networks (RNNs) and
convolutional neural networks (CNNs) have been successfully applied to VC, the
use of the Transformer network, which has shown promising results in various
speech processing tasks, has not yet been investigated. Nonetheless, their
data-hungry property and the mispronunciation of converted speech make seq2seq
models far from practical. To this end, we propose a simple yet effective
pretraining technique to transfer knowledge from learned TTS models, which
benefit from large-scale, easily accessible TTS corpora. VC models initialized
with such pretrained model parameters are able to generate effective hidden
representations for high-fidelity, highly intelligible converted speech.
Experimental results show that such a pretraining scheme can facilitate
data-efficient training and outperform an RNN-based seq2seq VC model in terms
of intelligibility, naturalness, and similarity.
</summary>
    <author>
      <name>Wen-Chin Huang</name>
    </author>
    <author>
      <name>Tomoki Hayashi</name>
    </author>
    <author>
      <name>Yi-Chiao Wu</name>
    </author>
    <author>
      <name>Hirokazu Kameoka</name>
    </author>
    <author>
      <name>Tomoki Toda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.06813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.05238v1</id>
    <updated>2019-12-11T11:27:06Z</updated>
    <published>2019-12-11T11:27:06Z</published>
    <title>BERT has a Moral Compass: Improvements of ethical and moral values of
  machines</title>
    <summary>  Allowing machines to choose whether to kill humans would be devastating for
world peace and security. But how do we equip machines with the ability to
learn ethical or even moral choices? Jentzsch et al.(2019) showed that applying
machine learning to human texts can extract deontological ethical reasoning
about "right" and "wrong" conduct by calculating a moral bias score on a
sentence level using sentence embeddings. The machine learned that it is
objectionable to kill living beings, but it is fine to kill time; It is
essential to eat, yet one might not eat dirt; it is important to spread
information, yet one should not spread misinformation. However, the evaluated
moral bias was restricted to simple actions -- one verb -- and a ranking of
actions with surrounding context. Recently BERT ---and variants such as RoBERTa
and SBERT--- has set a new state-of-the-art performance for a wide range of NLP
tasks. But has BERT also a better moral compass? In this paper, we discuss and
show that this is indeed the case. Thus, recent improvements of language
representations also improve the representation of the underlying ethical and
moral values of the machine. We argue that through an advanced semantic
representation of text, BERT allows one to get better insights of moral and
ethical values implicitly represented in text. This enables the Moral Choice
Machine (MCM) to extract more accurate imprints of moral choices and ethical
values.
</summary>
    <author>
      <name>Patrick Schramowski</name>
    </author>
    <author>
      <name>Cigdem Turan</name>
    </author>
    <author>
      <name>Sophie Jentzsch</name>
    </author>
    <author>
      <name>Constantin Rothkopf</name>
    </author>
    <author>
      <name>Kristian Kersting</name>
    </author>
    <link href="http://arxiv.org/abs/1912.05238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.05238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.05308v1</id>
    <updated>2019-12-10T16:08:26Z</updated>
    <published>2019-12-10T16:08:26Z</published>
    <title>Unsupervised Transfer Learning via BERT Neuron Selection</title>
    <summary>  Recent advancements in language representation models such as BERT have led
to a rapid improvement in numerous natural language processing tasks. However,
language models usually consist of a few hundred million trainable parameters
with embedding space distributed across multiple layers, thus making them
challenging to be fine-tuned for a specific task or to be transferred to a new
domain. To determine whether there are task-specific neurons that can be
exploited for unsupervised transfer learning, we introduce a method for
selecting the most important neurons to solve a specific classification task.
This algorithm is further extended to multi-source transfer learning by
computing the importance of neurons for several single-source transfer learning
scenarios between different subsets of data sources. Besides, a task-specific
fingerprint for each data source is obtained based on the percentage of the
selected neurons in each layer. We perform extensive experiments in
unsupervised transfer learning for sentiment analysis, natural language
inference and sentence similarity, and compare our results with the existing
literature and baselines. Significantly, we found that the source and target
data sources with higher degrees of similarity between their task-specific
fingerprints demonstrate a better transferability property. We conclude that
our method can lead to better performance using just a few hundred
task-specific and interpretable neurons.
</summary>
    <author>
      <name>Mehrdad Valipour</name>
    </author>
    <author>
      <name>En-Shiun Annie Lee</name>
    </author>
    <author>
      <name>Jaime R. Jamacaro</name>
    </author>
    <author>
      <name>Carolina Bessega</name>
    </author>
    <link href="http://arxiv.org/abs/1912.05308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.05308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.03010v1</id>
    <updated>2019-12-06T07:55:04Z</updated>
    <published>2019-12-06T07:55:04Z</published>
    <title>Semantic Mask for Transformer based End-to-End Speech Recognition</title>
    <summary>  Attention-based encoder-decoder model has achieved impressive results for
both automatic speech recognition (ASR) and text-to-speech (TTS) tasks. This
approach takes advantage of the memorization capacity of neural networks to
learn the mapping from the input sequence to the output sequence from scratch,
without the assumption of prior knowledge such as the alignments. However, this
model is prone to overfitting, especially when the amount of training data is
limited. Inspired by SpecAugment and BERT, in this paper, we propose a semantic
mask based regularization for training such kind of end-to-end (E2E) model. The
idea is to mask the input features corresponding to a particular output token,
e.g., a word or a word-piece, in order to encourage the model to fill the token
based on the contextual information. While this approach is applicable to the
encoder-decoder framework with any type of neural network architecture, we
study the transformer-based model for ASR in this work. We perform experiments
on Librispeech 960h and TedLium2 data sets, and achieve the state-of-the-art
performance on the test set in the scope of E2E models.
</summary>
    <author>
      <name>Chengyi Wang</name>
    </author>
    <author>
      <name>Yu Wu</name>
    </author>
    <author>
      <name>Yujiao Du</name>
    </author>
    <author>
      <name>Jinyu Li</name>
    </author>
    <author>
      <name>Shujie Liu</name>
    </author>
    <author>
      <name>Liang Lu</name>
    </author>
    <author>
      <name>Shuo Ren</name>
    </author>
    <author>
      <name>Guoli Ye</name>
    </author>
    <author>
      <name>Sheng Zhao</name>
    </author>
    <author>
      <name>Ming Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1912.03010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.03010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.02958v2</id>
    <updated>2020-02-24T03:49:45Z</updated>
    <published>2019-12-06T03:05:12Z</published>
    <title>Synchronous Transformers for End-to-End Speech Recognition</title>
    <summary>  For most of the attention-based sequence-to-sequence models, the decoder
predicts the output sequence conditioned on the entire input sequence processed
by the encoder. The asynchronous problem between the encoding and decoding
makes these models difficult to be applied for online speech recognition. In
this paper, we propose a model named synchronous transformer to address this
problem, which can predict the output sequence chunk by chunk. Once a
fixed-length chunk of the input sequence is processed by the encoder, the
decoder begins to predict symbols immediately. During training, a
forward-backward algorithm is introduced to optimize all the possible alignment
paths. Our model is evaluated on a Mandarin dataset AISHELL-1. The experiments
show that the synchronous transformer is able to perform encoding and decoding
synchronously, and achieves a character error rate of 8.91% on the test set.
</summary>
    <author>
      <name>Zhengkun Tian</name>
    </author>
    <author>
      <name>Jiangyan Yi</name>
    </author>
    <author>
      <name>Ye Bai</name>
    </author>
    <author>
      <name>Jianhua Tao</name>
    </author>
    <author>
      <name>Shuai Zhang</name>
    </author>
    <author>
      <name>Zhengqi Wen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICASSP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.02958v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.02958v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.01673v1</id>
    <updated>2019-12-03T20:20:31Z</updated>
    <published>2019-12-03T20:20:31Z</published>
    <title>COSTRA 1.0: A Dataset of Complex Sentence Transformations</title>
    <summary>  We present COSTRA 1.0, a dataset of complex sentence transformations. The
dataset is intended for the study of sentence-level embeddings beyond simple
word alternations or standard paraphrasing. This first version of the dataset
is limited to sentences in Czech but the construction method is universal and
we plan to use it also for other languages. The dataset consist of 4,262 unique
sentences with average length of 10 words, illustrating 15 types of
modifications such as simplification, generalization, or formal and informal
language variation. The hope is that with this dataset, we should be able to
test semantic properties of sentence embeddings and perhaps even to find some
topologically interesting 'skeleton' in the sentence embedding space. A
preliminary analysis using LASER, multi-purpose multi-lingual sentence
embeddings suggests that the LASER space does not exhibit the desired
properties.
</summary>
    <author>
      <name>Petra Baranƒç√≠kov√°</name>
    </author>
    <author>
      <name>Ond≈ôej Bojar</name>
    </author>
    <link href="http://arxiv.org/abs/1912.01673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.01673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.00871v1</id>
    <updated>2019-12-02T15:42:06Z</updated>
    <published>2019-12-02T15:42:06Z</published>
    <title>Solving Arithmetic Word Problems Automatically Using Transformer and
  Unambiguous Representations</title>
    <summary>  Constructing accurate and automatic solvers of math word problems has proven
to be quite challenging. Prior attempts using machine learning have been
trained on corpora specific to math word problems to produce arithmetic
expressions in infix notation before answer computation. We find that
custom-built neural networks have struggled to generalize well. This paper
outlines the use of Transformer networks trained to translate math word
problems to equivalent arithmetic expressions in infix, prefix, and postfix
notations. In addition to training directly on domain-specific corpora, we use
an approach that pre-trains on a general text corpus to provide foundational
language abilities to explore if it improves performance. We compare results
produced by a large number of neural configurations and find that most
configurations outperform previously reported approaches on three of four
datasets with significant increases in accuracy of over 20 percentage points.
The best neural approaches boost accuracy by almost 10% on average when
compared to the previous state of the art.
</summary>
    <author>
      <name>Kaden Griffith</name>
    </author>
    <author>
      <name>Jugal Kalita</name>
    </author>
    <link href="http://arxiv.org/abs/1912.00871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.00871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.12753v1</id>
    <updated>2019-11-28T15:38:53Z</updated>
    <published>2019-11-28T15:38:53Z</published>
    <title>Inducing Relational Knowledge from BERT</title>
    <summary>  One of the most remarkable properties of word embeddings is the fact that
they capture certain types of semantic and syntactic relationships. Recently,
pre-trained language models such as BERT have achieved groundbreaking results
across a wide range of Natural Language Processing tasks. However, it is
unclear to what extent such models capture relational knowledge beyond what is
already captured by standard word embeddings. To explore this question, we
propose a methodology for distilling relational knowledge from a pre-trained
language model. Starting from a few seed instances of a given relation, we
first use a large text corpus to find sentences that are likely to express this
relation. We then use a subset of these extracted sentences as templates.
Finally, we fine-tune a language model to predict whether a given word pair is
likely to be an instance of some relation, when given an instantiated template
for that relation as input.
</summary>
    <author>
      <name>Zied Bouraoui</name>
    </author>
    <author>
      <name>Jose Camacho-Collados</name>
    </author>
    <author>
      <name>Steven Schockaert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to AAAI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.12753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.12753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.12377v1</id>
    <updated>2019-11-27T19:00:24Z</updated>
    <published>2019-11-27T19:00:24Z</published>
    <title>Perceive, Transform, and Act: Multi-Modal Attention Networks for
  Vision-and-Language Navigation</title>
    <summary>  Vision-and-Language Navigation (VLN) is a challenging task in which an agent
needs to follow a language-specified path to reach a target destination. In
this paper, we strive for the creation of an agent able to tackle three key
issues: multi-modality, long-term dependencies, and adaptability towards
different locomotive settings. To that end, we devise "Perceive, Transform, and
Act" (PTA): a fully-attentive VLN architecture that leaves the recurrent
approach behind and the first Transformer-like architecture incorporating three
different modalities - natural language, images, and discrete actions for the
agent control. In particular, we adopt an early fusion strategy to merge
lingual and visual information efficiently in our encoder. We then propose to
refine the decoding phase with a late fusion extension between the agent's
history of actions and the perception modalities. We experimentally validate
our model on two datasets and two different action settings. PTA surpasses
previous state-of-the-art architectures for low-level VLN on R2R and achieves
the first place for both setups in the recently proposed R4R benchmark. Our
code is publicly available at
https://github.com/aimagelab/perceive-transform-and-act.
</summary>
    <author>
      <name>Federico Landi</name>
    </author>
    <author>
      <name>Lorenzo Baraldi</name>
    </author>
    <author>
      <name>Marcella Cornia</name>
    </author>
    <author>
      <name>Massimiliano Corsini</name>
    </author>
    <author>
      <name>Rita Cucchiara</name>
    </author>
    <link href="http://arxiv.org/abs/1911.12377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.12377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.12246v1</id>
    <updated>2019-11-27T16:09:11Z</updated>
    <published>2019-11-27T16:09:11Z</published>
    <title>Do Attention Heads in BERT Track Syntactic Dependencies?</title>
    <summary>  We investigate the extent to which individual attention heads in pretrained
transformer language models, such as BERT and RoBERTa, implicitly capture
syntactic dependency relations. We employ two methods---taking the maximum
attention weight and computing the maximum spanning tree---to extract implicit
dependency relations from the attention weights of each layer/head, and compare
them to the ground-truth Universal Dependency (UD) trees. We show that, for
some UD relation types, there exist heads that can recover the dependency type
significantly better than baselines on parsed English text, suggesting that
some self-attention heads act as a proxy for syntactic structure. We also
analyze BERT fine-tuned on two datasets---the syntax-oriented CoLA and the
semantics-oriented MNLI---to investigate whether fine-tuning affects the
patterns of their self-attention, but we do not observe substantial differences
in the overall dependency relations extracted using our methods. Our results
suggest that these models have some specialist attention heads that track
individual dependency types, but no generalist head that performs holistic
parsing significantly better than a trivial baseline, and that analyzing
attention weights directly may not reveal much of the syntactic knowledge that
BERT-style models are known to learn.
</summary>
    <author>
      <name>Phu Mon Htut</name>
    </author>
    <author>
      <name>Jason Phang</name>
    </author>
    <author>
      <name>Shikha Bordia</name>
    </author>
    <author>
      <name>Samuel R. Bowman</name>
    </author>
    <link href="http://arxiv.org/abs/1911.12246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.12246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.11951v1</id>
    <updated>2019-11-27T04:52:53Z</updated>
    <published>2019-11-27T04:52:53Z</published>
    <title>Taking a Stance on Fake News: Towards Automatic Disinformation
  Assessment via Deep Bidirectional Transformer Language Models for Stance
  Detection</title>
    <summary>  The exponential rise of social media and digital news in the past decade has
had the unfortunate consequence of escalating what the United Nations has
called a global topic of concern: the growing prevalence of disinformation.
Given the complexity and time-consuming nature of combating disinformation
through human assessment, one is motivated to explore harnessing AI solutions
to automatically assess news articles for the presence of disinformation. A
valuable first step towards automatic identification of disinformation is
stance detection, where given a claim and a news article, the aim is to predict
if the article agrees, disagrees, takes no position, or is unrelated to the
claim. Existing approaches in literature have largely relied on hand-engineered
features or shallow learned representations (e.g., word embeddings) to encode
the claim-article pairs, which can limit the level of representational
expressiveness needed to tackle the high complexity of disinformation
identification. In this work, we explore the notion of harnessing large-scale
deep bidirectional transformer language models for encoding claim-article pairs
in an effort to construct state-of-the-art stance detection geared for
identifying disinformation. Taking advantage of bidirectional cross-attention
between claim-article pairs via pair encoding with self-attention, we construct
a large-scale language model for stance detection by performing transfer
learning on a RoBERTa deep bidirectional transformer language model, and were
able to achieve state-of-the-art performance (weighted accuracy of 90.01%) on
the Fake News Challenge Stage 1 (FNC-I) benchmark. These promising results
serve as motivation for harnessing such large-scale language models as powerful
building blocks for creating effective AI solutions to combat disinformation.
</summary>
    <author>
      <name>Chris Dulhanty</name>
    </author>
    <author>
      <name>Jason L. Deglint</name>
    </author>
    <author>
      <name>Ibrahim Ben Daya</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the AI for Social Good Workshop at NeurIPS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.11951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.10677v1</id>
    <updated>2019-11-25T03:08:42Z</updated>
    <published>2019-11-25T03:08:42Z</published>
    <title>Non-autoregressive Transformer by Position Learning</title>
    <summary>  Non-autoregressive models are promising on various text generation tasks.
Previous work hardly considers to explicitly model the positions of generated
words. However, position modeling is an essential problem in non-autoregressive
text generation. In this study, we propose PNAT, which incorporates positions
as a latent variable into the text generative process. Experimental results
show that PNAT achieves top results on machine translation and paraphrase
generation tasks, outperforming several strong baselines.
</summary>
    <author>
      <name>Yu Bao</name>
    </author>
    <author>
      <name>Hao Zhou</name>
    </author>
    <author>
      <name>Jiangtao Feng</name>
    </author>
    <author>
      <name>Mingxuan Wang</name>
    </author>
    <author>
      <name>Shujian Huang</name>
    </author>
    <author>
      <name>Jiajun Chen</name>
    </author>
    <author>
      <name>Lei LI</name>
    </author>
    <link href="http://arxiv.org/abs/1911.10677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.10677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.10666v1</id>
    <updated>2019-11-25T02:12:45Z</updated>
    <published>2019-11-25T02:12:45Z</published>
    <title>Who did They Respond to? Conversation Structure Modeling using Masked
  Hierarchical Transformer</title>
    <summary>  Conversation structure is useful for both understanding the nature of
conversation dynamics and for providing features for many downstream
applications such as summarization of conversations. In this work, we define
the problem of conversation structure modeling as identifying the parent
utterance(s) to which each utterance in the conversation responds to. Previous
work usually took a pair of utterances to decide whether one utterance is the
parent of the other. We believe the entire ancestral history is a very
important information source to make accurate prediction. Therefore, we design
a novel masking mechanism to guide the ancestor flow, and leverage the
transformer model to aggregate all ancestors to predict parent utterances. Our
experiments are performed on the Reddit dataset (Zhang, Culbertson, and
Paritosh 2017) and the Ubuntu IRC dataset (Kummerfeld et al. 2019). In
addition, we also report experiments on a new larger corpus from the Reddit
platform and release this dataset. We show that the proposed model, that takes
into account the ancestral history of the conversation, significantly
outperforms several strong baselines including the BERT model on all datasets
</summary>
    <author>
      <name>Henghui Zhu</name>
    </author>
    <author>
      <name>Feng Nan</name>
    </author>
    <author>
      <name>Zhiguo Wang</name>
    </author>
    <author>
      <name>Ramesh Nallapati</name>
    </author>
    <author>
      <name>Bing Xiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.10666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.10666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.10401v1</id>
    <updated>2019-11-23T18:37:48Z</updated>
    <published>2019-11-23T18:37:48Z</published>
    <title>A Transformer-based approach to Irony and Sarcasm detection</title>
    <summary>  Figurative Language (FL) seems ubiquitous in all social-media discussion
forums and chats, posing extra challenges to sentiment analysis endeavors.
Identification of FL schemas in short texts remains largely an unresolved issue
in the broader field of Natural Language Processing (NLP), mainly due to their
contradictory and metaphorical meaning content. The main FL expression forms
are sarcasm, irony and metaphor. In the present paper we employ advanced Deep
Learning (DL) methodologies to tackle the problem of identifying the
aforementioned FL forms. Significantly extending our previous work [71], we
propose a neural network methodology that builds on a recently proposed
pre-trained transformer-based network architecture which, is further enhanced
with the employment and devise of a recurrent convolutional neural network
(RCNN). With this set-up, data preprocessing is kept in minimum. The
performance of the devised hybrid neural architecture is tested on four
benchmark datasets, and contrasted with other relevant state of the art
methodologies and systems. Results demonstrate that the proposed methodology
achieves state of the art performance under all benchmark datasets,
outperforming, even by a large margin, all other methodologies and published
studies.
</summary>
    <author>
      <name>Rolandos Alexandros Potamias</name>
    </author>
    <author>
      <name>Georgios Siolas</name>
    </author>
    <author>
      <name>Andreas - Georgios Stafylopatis</name>
    </author>
    <link href="http://arxiv.org/abs/1911.10401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.10401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.10235v1</id>
    <updated>2019-11-22T20:11:40Z</updated>
    <published>2019-11-22T20:11:40Z</published>
    <title>Improving N-gram Language Models with Pre-trained Deep Transformer</title>
    <summary>  Although n-gram language models (LMs) have been outperformed by the
state-of-the-art neural LMs, they are still widely used in speech recognition
due to its high efficiency in inference. In this paper, we demonstrate that
n-gram LM can be improved by neural LMs through a text generation based data
augmentation method. In contrast to previous approaches, we employ a
large-scale general domain pre-training followed by in-domain fine-tuning
strategy to construct deep Transformer based neural LMs. Large amount of
in-domain text data is generated with the well trained deep Transformer to
construct new n-gram LMs, which are then interpolated with baseline n-gram
systems. Empirical studies on different speech recognition tasks show that the
proposed approach can effectively improve recognition accuracy. In particular,
our proposed approach brings significant relative word error rate reduction up
to 6.0% for domains with limited in-domain data.
</summary>
    <author>
      <name>Yiren Wang</name>
    </author>
    <author>
      <name>Hongzhao Huang</name>
    </author>
    <author>
      <name>Zhe Liu</name>
    </author>
    <author>
      <name>Yutong Pang</name>
    </author>
    <author>
      <name>Yongqiang Wang</name>
    </author>
    <author>
      <name>ChengXiang Zhai</name>
    </author>
    <author>
      <name>Fuchun Peng</name>
    </author>
    <link href="http://arxiv.org/abs/1911.10235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.10235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.09912v1</id>
    <updated>2019-11-22T08:15:52Z</updated>
    <published>2019-11-22T08:15:52Z</published>
    <title>Go From the General to the Particular: Multi-Domain Translation with
  Domain Transformation Networks</title>
    <summary>  The key challenge of multi-domain translation lies in simultaneously encoding
both the general knowledge shared across domains and the particular knowledge
distinctive to each domain in a unified model. Previous work shows that the
standard neural machine translation (NMT) model, trained on mixed-domain data,
generally captures the general knowledge, but misses the domain-specific
knowledge. In response to this problem, we augment NMT model with additional
domain transformation networks to transform the general representations to
domain-specific representations, which are subsequently fed to the NMT decoder.
To guarantee the knowledge transformation, we also propose two complementary
supervision signals by leveraging the power of knowledge distillation and
adversarial learning. Experimental results on several language pairs, covering
both balanced and unbalanced multi-domain translation, demonstrate the
effectiveness and universality of the proposed approach. Encouragingly, the
proposed unified model achieves comparable results with the fine-tuning
approach that requires multiple models to preserve the particular knowledge.
Further analyses reveal that the domain transformation networks successfully
capture the domain-specific knowledge as expected.
</summary>
    <author>
      <name>Yong Wang</name>
    </author>
    <author>
      <name>Longyue Wang</name>
    </author>
    <author>
      <name>Shuming Shi</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <author>
      <name>Zhaopeng Tu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.09912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.09826v1</id>
    <updated>2019-11-22T03:14:32Z</updated>
    <published>2019-11-22T03:14:32Z</published>
    <title>Factorized Multimodal Transformer for Multimodal Sequential Learning</title>
    <summary>  The complex world around us is inherently multimodal and sequential
(continuous). Information is scattered across different modalities and requires
multiple continuous sensors to be captured. As machine learning leaps towards
better generalization to real world, multimodal sequential learning becomes a
fundamental research area. Arguably, modeling arbitrarily distributed
spatio-temporal dynamics within and across modalities is the biggest challenge
in this research area. In this paper, we present a new transformer model,
called the Factorized Multimodal Transformer (FMT) for multimodal sequential
learning. FMT inherently models the intramodal and intermodal (involving two or
more modalities) dynamics within its multimodal input in a factorized manner.
The proposed factorization allows for increasing the number of self-attentions
to better model the multimodal phenomena at hand; without encountering
difficulties during training (e.g. overfitting) even on relatively low-resource
setups. All the attention mechanisms within FMT have a full time-domain
receptive field which allows them to asynchronously capture long-range
multimodal dynamics. In our experiments we focus on datasets that contain the
three commonly studied modalities of language, vision and acoustic. We perform
a wide range of experiments, spanning across 3 well-studied datasets and 21
distinct labels. FMT shows superior performance over previously proposed
models, setting new state of the art in the studied datasets.
</summary>
    <author>
      <name>Amir Zadeh</name>
    </author>
    <author>
      <name>Chengfeng Mao</name>
    </author>
    <author>
      <name>Kelly Shi</name>
    </author>
    <author>
      <name>Yiwei Zhang</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>Soujanya Poria</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <link href="http://arxiv.org/abs/1911.09826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.01389v2</id>
    <updated>2019-12-12T18:23:41Z</updated>
    <published>2019-11-19T19:48:02Z</published>
    <title>Towards Lingua Franca Named Entity Recognition with BERT</title>
    <summary>  Information extraction is an important task in NLP, enabling the automatic
extraction of data for relational database filling. Historically, research and
data was produced for English text, followed in subsequent years by datasets in
Arabic, Chinese (ACE/OntoNotes), Dutch, Spanish, German (CoNLL evaluations),
and many others. The natural tendency has been to treat each language as a
different dataset and build optimized models for each. In this paper we
investigate a single Named Entity Recognition model, based on a multilingual
BERT, that is trained jointly on many languages simultaneously, and is able to
decode these languages with better accuracy than models trained only on one
language. To improve the initial model, we study the use of regularization
strategies such as multitask learning and partial gradient updates. In addition
to being a single model that can tackle multiple languages (including code
switch), the model could be used to make zero-shot predictions on a new
language, even ones for which training data is not available, out of the box.
The results show that this model not only performs competitively with
monolingual models, but it also achieves state-of-the-art results on the
CoNLL02 Dutch and Spanish datasets, OntoNotes Arabic and Chinese datasets.
Moreover, it performs reasonably well on unseen languages, achieving
state-of-the-art for zero-shot on three CoNLL languages.
</summary>
    <author>
      <name>Taesun Moon</name>
    </author>
    <author>
      <name>Parul Awasthy</name>
    </author>
    <author>
      <name>Jian Ni</name>
    </author>
    <author>
      <name>Radu Florian</name>
    </author>
    <link href="http://arxiv.org/abs/1912.01389v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.01389v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07523v1</id>
    <updated>2019-11-18T10:16:38Z</updated>
    <published>2019-11-18T10:16:38Z</published>
    <title>Fine-Grained Static Detection of Obfuscation Transforms Using
  Ensemble-Learning and Semantic Reasoning</title>
    <summary>  The ability to efficiently detect the software protections used is at a prime
to facilitate the selection and application of adequate deob-fuscation
techniques. We present a novel approach that combines semantic reasoning
techniques with ensemble learning classification for the purpose of providing a
static detection framework for obfuscation transformations. By contrast to
existing work, we provide a methodology that can detect multiple layers of
obfuscation, without depending on knowledge of the underlying functionality of
the training-set used. We also extend our work to detect constructions of
obfuscation transformations, thus providing a fine-grained methodology. To that
end, we provide several studies for the best practices of the use of machine
learning techniques for a scalable and efficient model. According to our
experimental results and evaluations on obfuscators such as Tigress and OLLVM,
our models have up to 91% accuracy on state-of-the-art obfuscation
transformations. Our overall accuracies for their constructions are up to 100%.
</summary>
    <author>
      <name>Ramtine Tofighi-Shirazi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TL</arxiv:affiliation>
    </author>
    <author>
      <name>Irina Mariuca Asavoae</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TL</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Elbaz-Vincent</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IF</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Software Security, Protection, and Reverse Engineering Workshop
  (SSPREW9), Dec 2019, San Juan, United States</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.07523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07470v2</id>
    <updated>2019-11-30T12:49:24Z</updated>
    <published>2019-11-18T07:45:19Z</published>
    <title>Graph Transformer for Graph-to-Sequence Learning</title>
    <summary>  The dominant graph-to-sequence transduction models employ graph neural
networks for graph representation learning, where the structural information is
reflected by the receptive field of neurons. Unlike graph neural networks that
restrict the information exchange between immediate neighborhood, we propose a
new model, known as Graph Transformer, that uses explicit relation encoding and
allows direct communication between two distant nodes. It provides a more
efficient way for global graph structure modeling. Experiments on the
applications of text generation from Abstract Meaning Representation (AMR) and
syntax-based neural machine translation show the superiority of our proposed
model. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU
on LDC2017T10 for AMR-to-text generation, outperforming the state-of-the-art
results by up to 2.2 points. On the syntax-based translation tasks, our model
establishes new single-model state-of-the-art BLEU scores, 21.3 for
English-to-German and 14.1 for English-to-Czech, improving over the existing
best results, including ensembles, by over 1 BLEU.
</summary>
    <author>
      <name>Deng Cai</name>
    </author>
    <author>
      <name>Wai Lam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by AAAI2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.07470v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07470v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.06258v2</id>
    <updated>2019-12-05T07:41:07Z</updated>
    <published>2019-11-14T17:32:10Z</published>
    <title>Iterative Answer Prediction with Pointer-Augmented Multimodal
  Transformers for TextVQA</title>
    <summary>  Many visual scenes contain text that carries crucial information, and it is
thus essential to understand text in images for downstream reasoning tasks. For
example, a deep water label on a warning sign warns people about the danger in
the scene. Recent work has explored the TextVQA task that requires reading and
understanding text in images to answer a question. However, existing approaches
for TextVQA are mostly based on custom pairwise fusion mechanisms between a
pair of two modalities and are restricted to a single prediction step by
casting TextVQA as a classification task. In this work, we propose a novel
model for the TextVQA task based on a multimodal transformer architecture
accompanied by a rich representation for text in images. Our model naturally
fuses different modalities homogeneously by embedding them into a common
semantic space where self-attention is applied to model inter- and intra-
modality context. Furthermore, it enables iterative answer decoding with a
dynamic pointer network, allowing the model to form an answer through
multi-step prediction instead of one-step classification. Our model outperforms
existing approaches on three benchmark datasets for the TextVQA task by a large
margin.
</summary>
    <author>
      <name>Ronghang Hu</name>
    </author>
    <author>
      <name>Amanpreet Singh</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Marcus Rohrbach</name>
    </author>
    <link href="http://arxiv.org/abs/1911.06258v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06258v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.05758v1</id>
    <updated>2019-11-13T19:04:42Z</updated>
    <published>2019-11-13T19:04:42Z</published>
    <title>What do you mean, BERT? Assessing BERT as a Distributional Semantics
  Model</title>
    <summary>  Contextualized word embeddings, i.e. vector representations for words in
context, are naturally seen as an extension of previous noncontextual
distributional semantic models. In this work, we focus on BERT, a deep neural
network that produces contextualized embeddings and has set the
state-of-the-art in several semantic tasks, and study the semantic coherence of
its embedding space. While showing a tendency towards coherence, BERT does not
fully live up to the natural expectations for a semantic vector space. In
particular, we find that the position of the sentence in which a word occurs,
while having no meaning correlates, leaves a noticeable trace on the word
embeddings and disturbs similarity relationships.
</summary>
    <author>
      <name>Timothee Mickus</name>
    </author>
    <author>
      <name>Denis Paperno</name>
    </author>
    <author>
      <name>Mathieu Constant</name>
    </author>
    <author>
      <name>Kees van Deemeter</name>
    </author>
    <link href="http://arxiv.org/abs/1911.05758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.05758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.04997v1</id>
    <updated>2019-11-12T16:32:38Z</updated>
    <published>2019-11-12T16:32:38Z</published>
    <title>Character-based NMT with Transformer</title>
    <summary>  Character-based translation has several appealing advantages, but its
performance is in general worse than a carefully tuned BPE baseline. In this
paper we study the impact of character-based input and output with the
Transformer architecture. In particular, our experiments on EN-DE show that
character-based Transformer models are more robust than their BPE counterpart,
both when translating noisy text, and when translating text from a different
domain. To obtain comparable BLEU scores in clean, in-domain data and close the
gap with BPE-based models we use known techniques to train deeper Transformer
models.
</summary>
    <author>
      <name>Rohit Gupta</name>
    </author>
    <author>
      <name>Laurent Besacier</name>
    </author>
    <author>
      <name>Marc Dymetman</name>
    </author>
    <author>
      <name>Matthias Gall√©</name>
    </author>
    <link href="http://arxiv.org/abs/1911.04997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.04997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.04525v1</id>
    <updated>2019-11-11T19:16:01Z</updated>
    <published>2019-11-11T19:16:01Z</published>
    <title>Understanding BERT performance in propaganda analysis</title>
    <summary>  In this paper, we describe our system used in the shared task for
fine-grained propaganda analysis at sentence level. Despite the challenging
nature of the task, our pretrained BERT model (team YMJA) fine tuned on the
training dataset provided by the shared task scored 0.62 F1 on the test set and
ranked third among 25 teams who participated in the contest. We present a set
of illustrative experiments to better understand the performance of our BERT
model on this shared task. Further, we explore beyond the given dataset for
false-positive cases that likely to be produced by our system. We show that
despite the high performance on the given testset, our system may have the
tendency of classifying opinion pieces as propaganda and cannot distinguish
quotations of propaganda speech from actual usage of propaganda techniques.
</summary>
    <author>
      <name>Yiqing Hua</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18653/v1/D19-5019</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18653/v1/D19-5019" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Second Workshop on Natural Language Processing
  for Internet Freedom: Censorship, Disinformation, and Propaganda (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.04525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.04525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.04118v2</id>
    <updated>2019-11-20T05:21:22Z</updated>
    <published>2019-11-11T07:40:37Z</published>
    <title>TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer
  Sentence Selection</title>
    <summary>  We propose TANDA, an effective technique for fine-tuning pre-trained
Transformer models for natural language tasks. Specifically, we first transfer
a pre-trained model into a model for a general task by fine-tuning it with a
large and high-quality dataset. We then perform a second fine-tuning step to
adapt the transferred model to the target domain. We demonstrate the benefits
of our approach for answer sentence selection, which is a well-known inference
task in Question Answering. We built a large scale dataset to enable the
transfer step, exploiting the Natural Questions dataset. Our approach
establishes the state of the art on two well-known benchmarks, WikiQA and
TREC-QA, achieving MAP scores of 92% and 94.3%, respectively, which largely
outperform the previous highest scores of 83.4% and 87.5%, obtained in very
recent work. We empirically show that TANDA generates more stable and robust
models reducing the effort required for selecting optimal hyper-parameters.
Additionally, we show that the transfer step of TANDA makes the adaptation step
more robust to noise. This enables a more effective use of noisy datasets for
fine-tuning. Finally, we also confirm the positive impact of TANDA in an
industrial setting, using domain specific datasets subject to different types
of noise.
</summary>
    <author>
      <name>Siddhant Garg</name>
    </author>
    <author>
      <name>Thuy Vu</name>
    </author>
    <author>
      <name>Alessandro Moschitti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI 2020),
  Oral Presentation</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.04118v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.04118v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.04474v3</id>
    <updated>2019-12-10T07:01:25Z</updated>
    <published>2019-11-10T15:05:48Z</published>
    <title>TENER: Adapting Transformer Encoder for Named Entity Recognition</title>
    <summary>  The Bidirectional long short-term memory networks (BiLSTM) have been widely
used as an encoder in models solving the named entity recognition (NER) task.
Recently, the Transformer is broadly adopted in various Natural Language
Processing (NLP) tasks owing to its parallelism and advantageous performance.
Nevertheless, the performance of the Transformer in NER is not as good as it is
in other NLP tasks. In this paper, we propose TENER, a NER architecture
adopting adapted Transformer Encoder to model the character-level features and
word-level features. By incorporating the direction and relative distance aware
attention and the un-scaled attention, we prove the Transformer-like encoder is
just as effective for NER as other NLP tasks.
</summary>
    <author>
      <name>Hang Yan</name>
    </author>
    <author>
      <name>Bocao Deng</name>
    </author>
    <author>
      <name>Xiaonan Li</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrept typos, update performance based on the public available codes</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.04474v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.04474v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03918v2</id>
    <updated>2020-02-25T14:40:15Z</updated>
    <published>2019-11-10T12:17:35Z</published>
    <title>Improving BERT Fine-tuning with Embedding Normalization</title>
    <summary>  Large pre-trained sentence encoders like BERT start a new chapter in natural
language processing. A common practice to apply pre-trained BERT to sequence
classification tasks (e.g., classification of sentences or sentence pairs) is
by feeding the embedding of [CLS] token (in the last layer) to a task-specific
classification layer, and then fine tune the model parameters of BERT and
classifier jointly. In this paper, we conduct systematic analysis over several
sequence classification datasets to examine the embedding values of [CLS] token
before the fine tuning phase, and present the biased embedding distribution
issue---i.e., embedding values of [CLS] concentrate on a few dimensions and are
non-zero centered. Such biased embedding brings challenge to the optimization
process during fine-tuning as gradients of [CLS] embedding may explode and
result in degraded model performance. We further propose several simple yet
effective normalization methods to modify the [CLS] embedding during the
fine-tuning. Compared with the previous practice, neural classification model
with the normalized embedding shows improvements on several text classification
tasks, demonstrates the effectiveness of our method.
</summary>
    <author>
      <name>Wenxuan Zhou</name>
    </author>
    <author>
      <name>Junyi Du</name>
    </author>
    <author>
      <name>Xiang Ren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.03918v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03918v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03895v1</id>
    <updated>2019-11-10T10:48:09Z</updated>
    <published>2019-11-10T10:48:09Z</published>
    <title>A Bilingual Generative Transformer for Semantic Sentence Embedding</title>
    <summary>  Semantic sentence embedding models encode natural language sentences into
vectors, such that closeness in embedding space indicates closeness in the
semantics between the sentences. Bilingual data offers a useful signal for
learning such embeddings: properties shared by both sentences in a translation
pair are likely semantic, while divergent properties are likely stylistic or
language-specific. We propose a deep latent variable model that attempts to
perform source separation on parallel sentences, isolating what they have in
common in a latent semantic vector, and explaining what is left over with
language-specific latent vectors. Our proposed approach differs from past work
on semantic sentence encoding in two ways. First, by using a variational
probabilistic framework, we introduce priors that encourage source separation,
and can use our model's posterior to predict sentence embeddings for
monolingual data at test time. Second, we use high-capacity transformers as
both data generating distributions and inference networks -- contrasting with
most past work on sentence embeddings. In experiments, our approach
substantially outperforms the state-of-the-art on a standard suite of
unsupervised semantic similarity evaluations. Further, we demonstrate that our
approach yields the largest gains on more difficult subsets of these
evaluations where simple word overlap is not a good indicator of similarity.
</summary>
    <author>
      <name>John Wieting</name>
    </author>
    <author>
      <name>Graham Neubig</name>
    </author>
    <author>
      <name>Taylor Berg-Kirkpatrick</name>
    </author>
    <link href="http://arxiv.org/abs/1911.03895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03864v1</id>
    <updated>2019-11-10T06:14:15Z</updated>
    <published>2019-11-10T06:14:15Z</published>
    <title>Improving Transformer Models by Reordering their Sublayers</title>
    <summary>  Multilayer transformer networks consist of interleaved self-attention and
feedforward sublayers. Could ordering the sublayers in a different pattern
achieve better performance? We generate randomly ordered transformers and train
them with the language modeling objective. We observe that some of these models
are able to achieve better performance than the interleaved baseline, and that
those successful variants tend to have more self-attention at the bottom and
more feedforward sublayers at the top. We propose a new transformer design
pattern that adheres to this property, the sandwich transformer, and show that
it improves perplexity on the WikiText-103 language modeling benchmark, at no
cost in parameters, memory, or training time.
</summary>
    <author>
      <name>Ofir Press</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <author>
      <name>Omer Levy</name>
    </author>
    <link href="http://arxiv.org/abs/1911.03864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.06156v1</id>
    <updated>2019-11-10T04:42:13Z</updated>
    <published>2019-11-10T04:42:13Z</published>
    <title>Syntax-Infused Transformer and BERT models for Machine Translation and
  Natural Language Understanding</title>
    <summary>  Attention-based models have shown significant improvement over traditional
algorithms in several NLP tasks. The Transformer, for instance, is an
illustrative example that generates abstract representations of tokens inputted
to an encoder based on their relationships to all tokens in a sequence. Recent
studies have shown that although such models are capable of learning syntactic
features purely by seeing examples, explicitly feeding this information to deep
learning models can significantly enhance their performance. Leveraging
syntactic information like part of speech (POS) may be particularly beneficial
in limited training data settings for complex models such as the Transformer.
We show that the syntax-infused Transformer with multiple features achieves an
improvement of 0.7 BLEU when trained on the full WMT 14 English to German
translation dataset and a maximum improvement of 1.99 BLEU points when trained
on a fraction of the dataset. In addition, we find that the incorporation of
syntax into BERT fine-tuning outperforms baseline on a number of downstream
tasks from the GLUE benchmark.
</summary>
    <author>
      <name>Dhanasekar Sundararaman</name>
    </author>
    <author>
      <name>Vivek Subramanian</name>
    </author>
    <author>
      <name>Guoyin Wang</name>
    </author>
    <author>
      <name>Shijing Si</name>
    </author>
    <author>
      <name>Dinghan Shen</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <link href="http://arxiv.org/abs/1911.06156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03829v1</id>
    <updated>2019-11-10T02:12:38Z</updated>
    <published>2019-11-10T02:12:38Z</published>
    <title>Distilling the Knowledge of BERT for Text Generation</title>
    <summary>  Large-scale pre-trained language model, such as BERT, has recently achieved
great success in a wide range of language understanding tasks. However, it
remains an open question how to utilize BERT for text generation tasks. In this
paper, we present a novel approach to addressing this challenge in a generic
sequence-to-sequence (Seq2Seq) setting. We first propose a new task,
Conditional Masked Language Modeling (C-MLM), to enable fine-tuning of BERT on
target text-generation dataset. The fine-tuned BERT (i.e., teacher) is then
exploited as extra supervision to improve conventional Seq2Seq models (i.e.,
student) for text generation. By leveraging BERT's idiosyncratic bidirectional
nature, distilling the knowledge learned from BERT can encourage
auto-regressive Seq2Seq models to plan ahead, imposing global sequence-level
supervision for coherent text generation. Experiments show that the proposed
approach significantly outperforms strong baselines of Transformer on multiple
text generation tasks, including machine translation (MT) and text
summarization. Our proposed model also achieves new state-of-the-art results on
the IWSLT German-English and English-Vietnamese MT datasets.
</summary>
    <author>
      <name>Yen-Chun Chen</name>
    </author>
    <author>
      <name>Zhe Gan</name>
    </author>
    <author>
      <name>Yu Cheng</name>
    </author>
    <author>
      <name>Jingzhou Liu</name>
    </author>
    <author>
      <name>Jingjing Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress (13 pages)</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.03829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03688v1</id>
    <updated>2019-11-09T13:35:18Z</updated>
    <published>2019-11-09T13:35:18Z</published>
    <title>ConveRT: Efficient and Accurate Conversational Representations from
  Transformers</title>
    <summary>  General-purpose pretrained sentence encoders such as BERT are not ideal for
real-world conversational AI applications; they are computationally heavy,
slow, and expensive to train. We propose ConveRT (Conversational
Representations from Transformers), a faster, more compact dual sentence
encoder specifically optimized for dialog tasks. We pretrain using a
retrieval-based response selection task, effectively leveraging quantization
and subword-level parameterization in the dual encoder to build a lightweight
memory- and energy-efficient model. In our evaluation, we show that ConveRT
achieves state-of-the-art performance across widely established response
selection tasks. We also demonstrate that the use of extended dialog history as
context yields further performance gains. Finally, we show that pretrained
representations from the proposed encoder can be transferred to the intent
classification task, yielding strong results across three diverse data sets.
  ConveRT trains substantially faster than standard sentence encoders or
previous state-of-the-art dual encoders. With its reduced size and superior
performance, we believe this model promises wider portability and scalability
for Conversational AI applications.
</summary>
    <author>
      <name>Matthew Henderson</name>
    </author>
    <author>
      <name>I√±igo Casanueva</name>
    </author>
    <author>
      <name>Nikola Mrk≈°iƒá</name>
    </author>
    <author>
      <name>Pei-Hao Su</name>
    </author>
    <author>
      <name>Tsung-Hsien Wen</name>
    </author>
    <author>
      <name>Ivan Vuliƒá</name>
    </author>
    <link href="http://arxiv.org/abs/1911.03688v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03688v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03681v1</id>
    <updated>2019-11-09T13:08:25Z</updated>
    <published>2019-11-09T13:08:25Z</published>
    <title>BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based
  Reasoning in Unsupervised QA</title>
    <summary>  The BERT language model (LM) (Devlin et al., 2019) is surprisingly good at
answering cloze-style questions about relational facts. Petroni et al. (2019)
take this as evidence that BERT memorizes factual knowledge during
pre-training. We take issue with this interpretation and argue that the
performance of BERT is partly due to reasoning about (the surface form of)
entity names, e.g., guessing that a person with an Italian-sounding name speaks
Italian. More specifically, we show that BERT's precision drops dramatically
when we filter certain easy-to-guess facts. As a remedy, we propose E-BERT, an
extension of BERT that replaces entity mentions with symbolic entity
embeddings. E-BERT outperforms both BERT and ERNIE (Zhang et al., 2019) on
hard-to-guess queries. We take this as evidence that E-BERT is richer in
factual knowledge, and we show two ways of ensembling BERT and E-BERT.
</summary>
    <author>
      <name>Nina Poerner</name>
    </author>
    <author>
      <name>Ulli Waltinger</name>
    </author>
    <author>
      <name>Hinrich Sch√ºtze</name>
    </author>
    <link href="http://arxiv.org/abs/1911.03681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.04910v1</id>
    <updated>2019-11-09T07:02:33Z</updated>
    <published>2019-11-09T07:02:33Z</published>
    <title>Orthogonal Relation Transforms with Graph Context Modeling for Knowledge
  Graph Embedding</title>
    <summary>  Translational distance-based knowledge graph embedding has shown progressive
improvements on the link prediction task, from TransE to the latest
state-of-the-art RotatE. However, N-1, 1-N and N-N predictions still remain
challenging. In this work, we propose a novel translational distance-based
approach for knowledge graph link prediction. The proposed method includes
two-folds, first we extend the RotatE from 2D complex domain to high dimension
space with orthogonal transforms to model relations for better modeling
capacity. Second, the graph context is explicitly modeled via two directed
context representations. These context representations are used as part of the
distance scoring function to measure the plausibility of the triples during
training and inference. The proposed approach effectively improves prediction
accuracy on the difficult N-1, 1-N and N-N cases for knowledge graph link
prediction task. The experimental results show that it achieves better
performance on two benchmark data sets compared to the baseline RotatE,
especially on data set (FB15k-237) with many high in-degree connection nodes.
</summary>
    <author>
      <name>Yun Tang</name>
    </author>
    <author>
      <name>Jing Huang</name>
    </author>
    <author>
      <name>Guangtao Wang</name>
    </author>
    <author>
      <name>Xiaodong He</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1911.04910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.04910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03604v3</id>
    <updated>2020-02-18T19:25:24Z</updated>
    <published>2019-11-09T03:29:06Z</published>
    <title>A Simplified Fully Quantized Transformer for End-to-end Speech
  Recognition</title>
    <summary>  While significant improvements have been made in recent years in terms of
end-to-end automatic speech recognition (ASR) performance, such improvements
were obtained through the use of very large neural networks, unfit for embedded
use on edge devices. That being said, in this paper, we work on simplifying and
compressing Transformer-based encoder-decoder architectures for the end-to-end
ASR task. We empirically introduce a more compact Speech-Transformer by
investigating the impact of discarding particular modules on the performance of
the model. Moreover, we evaluate reducing the numerical precision of our
network's weights and activations while maintaining the performance of the
full-precision model. Our experiments show that we can reduce the number of
parameters of the full-precision model and then further compress the model 4x
by fully quantizing to 8-bit fixed point precision.
</summary>
    <author>
      <name>Alex Bie</name>
    </author>
    <author>
      <name>Bharat Venkitesh</name>
    </author>
    <author>
      <name>Joao Monteiro</name>
    </author>
    <author>
      <name>Md. Akmal Haidar</name>
    </author>
    <author>
      <name>Mehdi Rezagholizadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Changed title Added references</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.03604v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03604v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03561v1</id>
    <updated>2019-11-08T22:14:35Z</updated>
    <published>2019-11-08T22:14:35Z</published>
    <title>Graph-to-Graph Transformer for Transition-based Dependency Parsing</title>
    <summary>  Transition-based dependency parsing is a challenging task for conditioning on
and predicting structures. We demonstrate state-of-the-art results on this
benchmark with the Graph2Graph Transformer architecture. This novel
architecture supports both the input and output of arbitrary graphs via its
attention mechanism. It can also be integrated both with previous neural
network structured prediction techniques and with existing Transformer
pre-trained models. Both with and without BERT pretraining, adding dependency
graph inputs via the attention mechanism results in significant improvements
over previously proposed mechanism for encoding the partial parse tree,
resulting in accuracies which improve the state-of-the-art in transition-based
dependency parsing, achieving 95.64% UAS and 93.81% LAS performance on Stanford
WSJ dependencies. Graph2Graph Transformers are not restricted to tree
structures and can be easily applied to a wide range of NLP tasks.
</summary>
    <author>
      <name>Alireza Mohammadshahi</name>
    </author>
    <author>
      <name>James Henderson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.03561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03324v1</id>
    <updated>2019-11-08T15:28:21Z</updated>
    <published>2019-11-08T15:28:21Z</published>
    <title>Transforming Wikipedia into Augmented Data for Query-Focused
  Summarization</title>
    <summary>  The manual construction of a query-focused summarization corpus is costly and
timeconsuming. The limited size of existing datasets renders training
data-driven summarization models challenging. In this paper, we use Wikipedia
to automatically collect a large query-focused summarization dataset (named as
WIKIREF) of more than 280,000 examples, which can serve as a means of data
augmentation. Moreover, we develop a query-focused summarization model based on
BERT to extract summaries from the documents. Experimental results on three DUC
benchmarks show that the model pre-trained on WIKIREF has already achieved
reasonable performance. After fine-tuning on the specific datasets, the model
with data augmentation outperforms the state of the art on the benchmarks.
</summary>
    <author>
      <name>Haichao Zhu</name>
    </author>
    <author>
      <name>Li Dong</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <author>
      <name>Bing Qin</name>
    </author>
    <author>
      <name>Ting Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1911.03324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03310v1</id>
    <updated>2019-11-08T15:12:36Z</updated>
    <published>2019-11-08T15:12:36Z</published>
    <title>How Language-Neutral is Multilingual BERT?</title>
    <summary>  Multilingual BERT (mBERT) provides sentence representations for 104
languages, which are useful for many multi-lingual tasks. Previous work probed
the cross-linguality of mBERT using zero-shot transfer learning on
morphological and syntactic tasks. We instead focus on the semantic properties
of mBERT. We show that mBERT representations can be split into a
language-specific component and a language-neutral component, and that the
language-neutral component is sufficiently general in terms of modeling
semantics to allow high-accuracy word-alignment and sentence retrieval but is
not yet good enough for the more difficult task of MT quality estimation. Our
work presents interesting challenges which must be solved to build better
language-neutral representations, particularly for tasks requiring linguistic
transfer of semantics.
</summary>
    <author>
      <name>Jind≈ôich Libovick√Ω</name>
    </author>
    <author>
      <name>Rudolf Rosa</name>
    </author>
    <author>
      <name>Alexander Fraser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.03310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03179v1</id>
    <updated>2019-11-08T10:52:43Z</updated>
    <published>2019-11-08T10:52:43Z</published>
    <title>Why Deep Transformers are Difficult to Converge? From Computation Order
  to Lipschitz Restricted Parameter Initialization</title>
    <summary>  The Transformer translation model employs residual connection and layer
normalization to ease the optimization difficulties caused by its multi-layer
encoder/decoder structure. While several previous works show that even with
residual connection and layer normalization, deep Transformers still have
difficulty in training, and particularly a Transformer model with more than 12
encoder/decoder layers fails to converge. In this paper, we first empirically
demonstrate that a simple modification made in the official implementation
which changes the computation order of residual connection and layer
normalization can effectively ease the optimization of deep Transformers. In
addition, we deeply compare the subtle difference in computation order, and
propose a parameter initialization method which simply puts Lipschitz
restriction on the initialization of Transformers but can effectively ensure
their convergence. We empirically show that with proper parameter
initialization, deep Transformers with the original computation order can
converge, which is quite in contrast to all previous works, and obtain
significant improvements with up to 24 layers. Our proposed approach
additionally enables to benefit from deep decoders compared to previous works
which focus on deep encoders.
</summary>
    <author>
      <name>Hongfei Xu</name>
    </author>
    <author>
      <name>Qiuhui Liu</name>
    </author>
    <author>
      <name>Josef van Genabith</name>
    </author>
    <author>
      <name>Jingyi Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A similar work (Improving Deep Transformer with Depth-Scaled
  Initialization and Merged Attention) is accepted by EMNLP 2019, but there are
  differences between in analysis and approaches</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.03179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03090v1</id>
    <updated>2019-11-08T07:05:20Z</updated>
    <published>2019-11-08T07:05:20Z</published>
    <title>What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning</title>
    <summary>  Pretrained transformer-based language models have achieved state of the art
across countless tasks in natural language processing. These models are highly
expressive, comprising at least a hundred million parameters and a dozen
layers. Recent evidence suggests that only a few of the final layers need to be
fine-tuned for high quality on downstream tasks. Naturally, a subsequent
research question is, "how many of the last layers do we need to fine-tune?" In
this paper, we precisely answer this question. We examine two recent pretrained
language models, BERT and RoBERTa, across standard tasks in textual entailment,
semantic similarity, sentiment analysis, and linguistic acceptability. We vary
the number of final layers that are fine-tuned, then study the resulting change
in task-specific effectiveness. We show that only a fourth of the final layers
need to be fine-tuned to achieve 90% of the original quality. Surprisingly, we
also find that fine-tuning all layers does not always help.
</summary>
    <author>
      <name>Jaejun Lee</name>
    </author>
    <author>
      <name>Raphael Tang</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.03090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.02969v1</id>
    <updated>2019-11-07T16:20:40Z</updated>
    <published>2019-11-07T16:20:40Z</published>
    <title>BERTs of a feather do not generalize together: Large variability in
  generalization across models with similar test set performance</title>
    <summary>  If the same neural architecture is trained multiple times on the same
dataset, will it make similar linguistic generalizations across runs? To study
this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural
Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which
measures syntactic generalization in natural language inference. On the MNLI
development set, the behavior of all instances was remarkably consistent, with
accuracy ranging between 83.6% and 84.8%. In stark contrast, the same models
varied widely in their generalization performance. For example, on the simple
case of subject-object swap (e.g., knowing that "the doctor visited the lawyer"
does not entail "the lawyer visited the doctor"), accuracy ranged from 0.00% to
66.2%. Such variation likely arises from the presence of many local minima that
are equally attractive to a low-bias learner such as a neural network;
decreasing the variability may therefore require models with stronger inductive
biases.
</summary>
    <author>
      <name>R. Thomas McCoy</name>
    </author>
    <author>
      <name>Junghyun Min</name>
    </author>
    <author>
      <name>Tal Linzen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.02969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.02969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.02914v1</id>
    <updated>2019-11-07T14:06:36Z</updated>
    <published>2019-11-07T14:06:36Z</published>
    <title>Transformation of Dense and Sparse Text Representations</title>
    <summary>  Sparsity is regarded as a desirable property of representations, especially
in terms of explanation. However, its usage has been limited due to the gap
with dense representations. Most NLP research progresses in recent years are
based on dense representations. Thus the desirable property of sparsity cannot
be leveraged. Inspired by Fourier Transformation, in this paper, we propose a
novel Semantic Transformation method to bridge the dense and sparse spaces,
which can facilitate the NLP research to shift from dense space to sparse space
or to jointly use both spaces. The key idea of the proposed approach is to use
a Forward Transformation to transform dense representations to sparse
representations. Then some useful operations in the sparse space can be
performed over the sparse representations, and the sparse representations can
be used directly to perform downstream tasks such as text classification and
natural language inference. Then, a Backward Transformation can also be carried
out to transform those processed sparse representations to dense
representations. Experiments using classification tasks and natural language
inference task show that the proposed Semantic Transformation is effective.
</summary>
    <author>
      <name>Wenpeng Hu</name>
    </author>
    <author>
      <name>Mengyu Wang</name>
    </author>
    <author>
      <name>Bing Liu</name>
    </author>
    <author>
      <name>Feng Ji</name>
    </author>
    <author>
      <name>Haiqing Chen</name>
    </author>
    <author>
      <name>Dongyan Zhao</name>
    </author>
    <author>
      <name>Jinwen Ma</name>
    </author>
    <author>
      <name>Rui Yan</name>
    </author>
    <link href="http://arxiv.org/abs/1911.02914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.02914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.02847v1</id>
    <updated>2019-11-07T10:59:40Z</updated>
    <published>2019-11-07T10:59:40Z</published>
    <title>Explicit Pairwise Word Interaction Modeling Improves Pretrained
  Transformers for English Semantic Similarity Tasks</title>
    <summary>  In English semantic similarity tasks, classic word embedding-based approaches
explicitly model pairwise "interactions" between the word representations of a
sentence pair. Transformer-based pretrained language models disregard this
notion, instead modeling pairwise word interactions globally and implicitly
through their self-attention mechanism. In this paper, we hypothesize that
introducing an explicit, constrained pairwise word interaction mechanism to
pretrained language models improves their effectiveness on semantic similarity
tasks. We validate our hypothesis using BERT on four tasks in semantic textual
similarity and answer sentence selection. We demonstrate consistent
improvements in quality by adding an explicit pairwise word interaction module
to BERT.
</summary>
    <author>
      <name>Yinan Zhang</name>
    </author>
    <author>
      <name>Raphael Tang</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.02847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.02847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.02733v1</id>
    <updated>2019-11-07T02:58:17Z</updated>
    <published>2019-11-07T02:58:17Z</published>
    <title>Porous Lattice-based Transformer Encoder for Chinese NER</title>
    <summary>  Incorporating lattices into character-level Chinese named entity recognition
is an effective method to exploit explicit word information. Recent works
extend recurrent and convolutional neural networks to model lattice inputs.
However, due to the DAG structure or the variable-sized potential word set for
lattice inputs, these models prevent the convenient use of batched computation,
resulting in serious inefficient. In this paper, we propose a porous
lattice-based transformer encoder for Chinese named entity recognition, which
is capable to better exploit the GPU parallelism and batch the computation
owing to the mask mechanism in transformer. We first investigate the
lattice-aware self-attention coupled with relative position representations to
explore effective word information in the lattice structure. Besides, to
strengthen the local dependencies among neighboring tokens, we propose a novel
porous structure during self-attentional computation processing, in which every
two non-neighboring tokens are connected through a shared pivot node.
Experimental results on four datasets show that our model performs up to 9.47
times faster than state-of-the-art models, while is roughly on a par with its
performance. The source code of this paper can be obtained from
https://github.com/xxx/xxx.
</summary>
    <author>
      <name>Xue Mengge</name>
    </author>
    <author>
      <name>Yu Bowen</name>
    </author>
    <author>
      <name>Liu Tingwen</name>
    </author>
    <author>
      <name>Wang Bin</name>
    </author>
    <author>
      <name>Meng Erli</name>
    </author>
    <author>
      <name>Li Quangang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.02733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.02733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.02365v1</id>
    <updated>2019-11-06T13:23:41Z</updated>
    <published>2019-11-06T13:23:41Z</published>
    <title>Learning to Answer by Learning to Ask: Getting the Best of GPT-2 and
  BERT Worlds</title>
    <summary>  Automatic question generation aims at the generation of questions from a
context, with the corresponding answers being sub-spans of the given passage.
Whereas, most of the methods mostly rely on heuristic rules to generate
questions, more recently also neural network approaches have been proposed. In
this work, we propose a variant of the self-attention Transformer network
architectures model to generate meaningful and diverse questions. To this end,
we propose an easy to use model consisting of the conjunction of the
Transformer decoder GPT-2 model with Transformer encoder BERT for the
downstream task for question answering. The model is trained in an end-to-end
fashion, where the language model is trained to produce a question-answer-aware
input representation that facilitates to generate an answer focused question.
Our result of neural question generation from text on the SQuAD 1.1 dataset
suggests that our method can produce semantically correct and diverse
questions. Additionally, we assessed the performance of our proposed method for
the downstream task of question answering. The analysis shows that our proposed
generation &amp; answering collaboration framework relatively improves both tasks
and is particularly powerful in the semi-supervised setup. The results further
suggest a robust and comparably lean pipeline facilitating question generation
in the small-data regime.
</summary>
    <author>
      <name>Tassilo Klein</name>
    </author>
    <author>
      <name>Moin Nabi</name>
    </author>
    <link href="http://arxiv.org/abs/1911.02365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.02365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.02150v1</id>
    <updated>2019-11-06T00:19:05Z</updated>
    <published>2019-11-06T00:19:05Z</published>
    <title>Fast Transformer Decoding: One Write-Head is All You Need</title>
    <summary>  Multi-head attention layers, as used in the Transformer neural sequence
model, are a powerful alternative to RNNs for moving information across and
between sequences. While training these layers is generally fast and simple,
due to parallelizability across the length of the sequence, incremental
inference (where such paralleization is impossible) is often slow, due to the
memory-bandwidth cost of repeatedly loading the large "keys" and "values"
tensors. We propose a variant called multi-query attention, where the keys and
values are shared across all of the different attention "heads", greatly
reducing the size of these tensors and hence the memory bandwidth requirements
of incremental decoding. We verify experimentally that the resulting models can
indeed be much faster to decode, and incur only minor quality degradation from
the baseline.
</summary>
    <author>
      <name>Noam Shazeer</name>
    </author>
    <link href="http://arxiv.org/abs/1911.02150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.02150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.02147v2</id>
    <updated>2019-11-08T04:55:18Z</updated>
    <published>2019-11-06T00:08:19Z</published>
    <title>Seq2Emo for Multi-label Emotion Classification Based on Latent Variable
  Chains Transformation</title>
    <summary>  Emotion detection in text is an important task in NLP and is essential in
many applications. Most of the existing methods treat this task as a problem of
single-label multi-class text classification. To predict multiple emotions for
one instance, most of the existing works regard it as a general Multi-label
Classification (MLC) problem, where they usually either apply a manually
determined threshold on the last output layer of their neural network models or
train multiple binary classifiers and make predictions in the fashion of
one-vs-all. However, compared to labels in the general MLC datasets, the number
of emotion categories are much fewer (less than 10). Additionally, emotions
tend to have more correlations with each other. For example, the human usually
does not express "joy" and "anger" at the same time, but it is very likely to
have "joy" and "love" expressed together. Given this intuition, in this paper,
we propose a Latent Variable Chain (LVC) transformation and a tailored model --
Seq2Emo model that not only naturally predicts multiple emotion labels but also
takes into consideration their correlations. We perform the experiments on the
existing multi-label emotion datasets as well as on our newly collected
datasets. The results show that our model compares favorably with existing
state-of-the-art methods.
</summary>
    <author>
      <name>Chenyang Huang</name>
    </author>
    <author>
      <name>Amine Trabelsi</name>
    </author>
    <author>
      <name>Xuebin Qin</name>
    </author>
    <author>
      <name>Nawshad Farruque</name>
    </author>
    <author>
      <name>Osmar R. Za√Øane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.02147v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.02147v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.01528v3</id>
    <updated>2020-02-11T14:20:01Z</updated>
    <published>2019-11-04T23:16:47Z</published>
    <title>BAS: An Answer Selection Method Using BERT Language Model</title>
    <summary>  In recent years, Question Answering systems have become more popular and
widely used by users. Despite the increasing popularity of these systems, the
their performance is not even sufficient for textual data and requires further
research. These systems consist of several parts that one of them is the Answer
Selection component. This component detects the most relevant answer from a
list of candidate answers. The methods presented in previous researches have
attempted to provide an independent model to undertake the answer-selection
task. An independent model cannot comprehend the syntactic and semantic
features of questions and answers with a small training dataset. To fill this
gap, language models can be employed in implementing the answer selection part.
This action enables the model to have a better understanding of the language in
order to understand questions and answers better than previous works. In this
research, we will present the "BAS" (BERT Answer Selection) that uses the BERT
language model to comprehend language. The empirical results of applying the
model on the TrecQA Raw, TrecQA Clean, and WikiQA datasets demonstrate that
using a robust language model such as BERT can enhance the performance. Using a
more robust classifier also enhances the effect of the language model on the
answer selection component. The results demonstrate that language comprehension
is an essential requirement in natural language processing tasks such as
answer-selection.
</summary>
    <author>
      <name>Jamshid Mozafari</name>
    </author>
    <author>
      <name>Afsaneh Fatemi</name>
    </author>
    <author>
      <name>Mohammad Ali Nematbakhsh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 19 figures, 8 tables, Natural Language Engineering</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.01528v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01528v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.01188v1</id>
    <updated>2019-11-04T13:25:26Z</updated>
    <published>2019-11-04T13:25:26Z</published>
    <title>Analysing Coreference in Transformer Outputs</title>
    <summary>  We analyse coreference phenomena in three neural machine translation systems
trained with different data settings with or without access to explicit intra-
and cross-sentential anaphoric information. We compare system performance on
two different genres: news and TED talks. To do this, we manually annotate (the
possibly incorrect) coreference chains in the MT outputs and evaluate the
coreference chain translations. We define an error typology that aims to go
further than pronoun translation adequacy and includes types such as incorrect
word selection or missing words. The features of coreference chains in
automatic translations are also compared to those of the source texts and human
translations. The analysis shows stronger potential translationese effects in
machine translated outputs than in human translations.
</summary>
    <author>
      <name>Ekaterina Lapshinova-Koltunski</name>
    </author>
    <author>
      <name>Cristina Espa√±a-Bonet</name>
    </author>
    <author>
      <name>Josef van Genabith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.01188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00637v1</id>
    <updated>2019-11-02T03:39:19Z</updated>
    <published>2019-11-02T03:39:19Z</published>
    <title>Sentence-Level BERT and Multi-Task Learning of Age and Gender in Social
  Media</title>
    <summary>  Social media currently provide a window on our lives, making it possible to
learn how people from different places, with different backgrounds, ages, and
genders use language. In this work we exploit a newly-created Arabic dataset
with ground truth age and gender labels to learn these attributes both
individually and in a multi-task setting at the sentence level. Our models are
based on variations of deep bidirectional neural networks. More specifically,
we build models with gated recurrent units and bidirectional encoder
representations from transformers (BERT). We show the utility of multi-task
learning (MTL) on the two tasks and identify task-specific attention as a
superior choice in this context. We also find that a single-task BERT model
outperform our best MTL models on the two tasks. We report tweet-level accuracy
of 51.43% for the age task (three-way) and 65.30% on the gender task (binary),
both of which outperforms our baselines with a large margin. Our models are
language-agnostic, and so can be applied to other languages.
</summary>
    <author>
      <name>Muhammad Abdul-Mageed</name>
    </author>
    <author>
      <name>Chiyu Zhang</name>
    </author>
    <author>
      <name>Arun Rajendran</name>
    </author>
    <author>
      <name>AbdelRahim Elmadany</name>
    </author>
    <author>
      <name>Michael Przystupa</name>
    </author>
    <author>
      <name>Lyle Ungar</name>
    </author>
    <link href="http://arxiv.org/abs/1911.00637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00473v1</id>
    <updated>2019-11-01T17:30:21Z</updated>
    <published>2019-11-01T17:30:21Z</published>
    <title>BERT Goes to Law School: Quantifying the Competitive Advantage of Access
  to Large Legal Corpora in Contract Understanding</title>
    <summary>  Fine-tuning language models, such as BERT, on domain specific corpora has
proven to be valuable in domains like scientific papers and biomedical text. In
this paper, we show that fine-tuning BERT on legal documents similarly provides
valuable improvements on NLP tasks in the legal domain. Demonstrating this
outcome is significant for analyzing commercial agreements, because obtaining
large legal corpora is challenging due to their confidential nature. As such,
we show that having access to large legal corpora is a competitive advantage
for commercial applications, and academic research on analyzing contracts.
</summary>
    <author>
      <name>Emad Elwany</name>
    </author>
    <author>
      <name>Dave Moore</name>
    </author>
    <author>
      <name>Gaurav Oberoi</name>
    </author>
    <link href="http://arxiv.org/abs/1911.00473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00203v1</id>
    <updated>2019-11-01T05:16:12Z</updated>
    <published>2019-11-01T05:16:12Z</published>
    <title>Improving Generalization of Transformer for Speech Recognition with
  Parallel Schedule Sampling and Relative Positional Embedding</title>
    <summary>  Transformer showed promising results in many sequence to sequence
transformation tasks recently. It utilizes a number of feed-forward
self-attention layers in the encoder and decoder to replace recurrent neural
networks (RNN) in attention-based encoder decoder (AED). Self-attention layer
learns temporal dependence by incorporating sinusoidal positional embedding of
tokens in sequences for parallel computing. Quicker iteration speed in training
than sequential operation of RNN can be obtained. The deeper layer of
transformer also makes it perform better than RNN-based AED. However, this
parallelization makes it hard to apply schedule sampling training.
Self-attention with sinusoidal positional embedding may also cause performance
degradations for longer sequence that has similar acoustic or semantic
information at different positions. To address these problems, we propose to
use parallel schedule sampling (PSS) and relative positional embedding (RPE) to
help transformer generalize to unseen data. Our proposed methods achieve 7%
relative improvement for short utterances and 30% absolute gains for long
utterances on a 10,000-hour ASR task.
</summary>
    <author>
      <name>Pan Zhou</name>
    </author>
    <author>
      <name>Ruchao Fan</name>
    </author>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Jia Jia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ICASSP2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.00203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.14549v1</id>
    <updated>2019-10-31T15:51:04Z</updated>
    <published>2019-10-31T15:51:04Z</published>
    <title>Positional Attention-based Frame Identification with BERT: A Deep
  Learning Approach to Target Disambiguation and Semantic Frame Selection</title>
    <summary>  Semantic parsing is the task of transforming sentences from natural language
into formal representations of predicate-argument structures. Under this
research area, frame-semantic parsing has attracted much interest. This parsing
approach leverages the lexical information defined in FrameNet to associate
marked predicates or targets with semantic frames, thereby assigning semantic
roles to sentence components based on pre-specified frame elements in FrameNet.
In this paper, a deep neural network architecture known as Positional
Attention-based Frame Identification with BERT (PAFIBERT) is presented as a
solution to the frame identification subtask in frame-semantic parsing.
Although the importance of this subtask is well-established, prior research has
yet to find a robust solution that works satisfactorily for both in-domain and
out-of-domain data. This study thus set out to improve frame identification in
light of recent advancements of language modeling and transfer learning in
natural language processing. The proposed method is partially empowered by
BERT, a pre-trained language model that excels at capturing contextual
information in texts. By combining the language representation power of BERT
with a position-based attention mechanism, PAFIBERT is able to attend to
target-specific contexts in sentences for disambiguating targets and
associating them with the most suitable semantic frames. Under various
experimental settings, PAFIBERT outperformed existing solutions by a
significant margin, achieving new state-of-the-art results for both in-domain
and out-of-domain benchmark test sets.
</summary>
    <author>
      <name>Sang-Sang Tan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Nanyang Technological University, Singapore</arxiv:affiliation>
    </author>
    <author>
      <name>Jin-Cheon Na</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Nanyang Technological University, Singapore</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 7 figures, uses basic.sty</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.14549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.14549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.4; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.14353v1</id>
    <updated>2019-10-31T10:32:43Z</updated>
    <published>2019-10-31T10:32:43Z</published>
    <title>Transfer Learning from Transformers to Fake News Challenge Stance
  Detection (FNC-1) Task</title>
    <summary>  In this paper, we report improved results of the Fake News Challenge Stage 1
(FNC-1) stance detection task. This gain in performance is due to the
generalization power of large language models based on Transformer
architecture, invented, trained and publicly released over the last two years.
Specifically (1) we improved the FNC-1 best performing model adding BERT
sentence embedding of input sequences as a model feature, (2) we fine-tuned
BERT, XLNet, and RoBERTa transformers on FNC-1 extended dataset and obtained
state-of-the-art results on FNC-1 task.
</summary>
    <author>
      <name>Valeriya Slovikovskaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.14353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.14353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.14296v1</id>
    <updated>2019-10-31T08:14:51Z</updated>
    <published>2019-10-31T08:14:51Z</published>
    <title>LIMIT-BERT : Linguistic Informed Multi-Task BERT</title>
    <summary>  In this paper, we present a Linguistic Informed Multi-Task BERT (LIMIT-BERT)
for learning language representations across multiple linguistic tasks by
Multi-Task Learning (MTL). LIMIT-BERT includes five key linguistic syntax and
semantics tasks: Part-Of-Speech (POS) tags, constituent and dependency
syntactic parsing, span and dependency semantic role labeling (SRL). Besides,
LIMIT-BERT adopts linguistics mask strategy: Syntactic and Semantic Phrase
Masking which mask all of the tokens corresponding to a syntactic/semantic
phrase. Different from recent Multi-Task Deep Neural Networks (MT-DNN) (Liu et
al., 2019), our LIMIT-BERT is linguistically motivated and learning in a
semi-supervised method which provides large amounts of linguistic-task data as
same as BERT learning corpus. As a result, LIMIT-BERT not only improves
linguistic tasks performance but also benefits from a regularization effect and
linguistic information that leads to more general representations to help adapt
to new tasks and domains. LIMIT-BERT obtains new state-of-the-art or
competitive results on both span and dependency semantic parsing on Propbank
benchmarks and both dependency and constituent syntactic parsing on Penn
Treebank.
</summary>
    <author>
      <name>Junru Zhou</name>
    </author>
    <author>
      <name>Zhuosheng Zhang</name>
    </author>
    <author>
      <name>Hai Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1910.14296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.14296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.14243v1</id>
    <updated>2019-10-31T03:56:32Z</updated>
    <published>2019-10-31T03:56:32Z</published>
    <title>DiaNet: BERT and Hierarchical Attention Multi-Task Learning of
  Fine-Grained Dialect</title>
    <summary>  Prediction of language varieties and dialects is an important language
processing task, with a wide range of applications. For Arabic, the native
tongue of ~ 300 million people, most varieties remain unsupported. To ease this
bottleneck, we present a very large scale dataset covering 319 cities from all
21 Arab countries. We introduce a hierarchical attention multi-task learning
(HA-MTL) approach for dialect identification exploiting our data at the city,
state, and country levels. We also evaluate use of BERT on the three tasks,
comparing it to the MTL approach. We benchmark and release our data and models.
</summary>
    <author>
      <name>Muhammad Abdul-Mageed</name>
    </author>
    <author>
      <name>Chiyu Zhang</name>
    </author>
    <author>
      <name>AbdelRahim Elmadany</name>
    </author>
    <author>
      <name>Arun Rajendran</name>
    </author>
    <author>
      <name>Lyle Ungar</name>
    </author>
    <link href="http://arxiv.org/abs/1910.14243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.14243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.13923v3</id>
    <updated>2020-02-14T08:06:33Z</updated>
    <published>2019-10-30T15:20:07Z</published>
    <title>Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank
  Transformer</title>
    <summary>  Highly performing deep neural networks come at the cost of computational
complexity that limits their practicality for deployment on portable devices.
We propose the low-rank transformer (LRT), a memory-efficient and fast neural
architecture that significantly reduces the parameters and boosts the speed of
training and inference for end-to-end speech recognition. Our approach reduces
the number of parameters of the network by more than 50% and speeds up the
inference time by around 1.35x compared to the baseline transformer model. The
experiments show that our LRT model generalizes better and yields lower error
rates on both validation and test sets compared to an uncompressed transformer
model. The LRT model outperforms those from existing works on several datasets
in an end-to-end setting without using an external language model or acoustic
data.
</summary>
    <author>
      <name>Genta Indra Winata</name>
    </author>
    <author>
      <name>Samuel Cahyawijaya</name>
    </author>
    <author>
      <name>Zhaojiang Lin</name>
    </author>
    <author>
      <name>Zihan Liu</name>
    </author>
    <author>
      <name>Pascale Fung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first two authors contributed equally to this work. Accepted as
  an oral presentation in ICASSP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.13923v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.13923v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.13634v1</id>
    <updated>2019-10-30T02:46:04Z</updated>
    <published>2019-10-30T02:46:04Z</published>
    <title>An Augmented Transformer Architecture for Natural Language Generation
  Tasks</title>
    <summary>  The Transformer based neural networks have been showing significant
advantages on most evaluations of various natural language processing and other
sequence-to-sequence tasks due to its inherent architecture based
superiorities. Although the main architecture of the Transformer has been
continuously being explored, little attention was paid to the positional
encoding module. In this paper, we enhance the sinusoidal positional encoding
algorithm by maximizing the variances between encoded consecutive positions to
obtain additional promotion. Furthermore, we propose an augmented Transformer
architecture encoded with additional linguistic knowledge, such as the
Part-of-Speech (POS) tagging, to boost the performance on some natural language
generation tasks, e.g., the automatic translation and summarization tasks.
Experiments show that the proposed architecture attains constantly superior
results compared to the vanilla Transformer.
</summary>
    <author>
      <name>Hailiang Li</name>
    </author>
    <author>
      <name>Adele Y. C. Wang</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Du Tang</name>
    </author>
    <author>
      <name>Zhibin Lei</name>
    </author>
    <author>
      <name>Wenye Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be appeared in the conference workshop ICDM MLCS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.13634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.13634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.13215v3</id>
    <updated>2019-11-08T20:04:10Z</updated>
    <published>2019-10-29T11:56:12Z</published>
    <title>Transformer-based Cascaded Multimodal Speech Translation</title>
    <summary>  This paper describes the cascaded multimodal speech translation systems
developed by Imperial College London for the IWSLT 2019 evaluation campaign.
The architecture consists of an automatic speech recognition (ASR) system
followed by a Transformer-based multimodal machine translation (MMT) system.
While the ASR component is identical across the experiments, the MMT model
varies in terms of the way of integrating the visual context (simple
conditioning vs. attention), the type of visual features exploited (pooled,
convolutional, action categories) and the underlying architecture. For the
latter, we explore both the canonical transformer and its deliberation version
with additive and cascade variants which differ in how they integrate the
textual attention. Upon conducting extensive experiments, we found that (i) the
explored visual integration schemes often harm the translation performance for
the transformer and additive deliberation, but considerably improve the cascade
deliberation; (ii) the transformer and cascade deliberation integrate the
visual modality better than the additive deliberation, as shown by the
incongruence analysis.
</summary>
    <author>
      <name>Zixiu Wu</name>
    </author>
    <author>
      <name>Ozan Caglayan</name>
    </author>
    <author>
      <name>Julia Ive</name>
    </author>
    <author>
      <name>Josiah Wang</name>
    </author>
    <author>
      <name>Lucia Specia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IWSLT 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.13215v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.13215v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12995v3</id>
    <updated>2020-02-09T04:35:16Z</updated>
    <published>2019-10-28T22:41:55Z</published>
    <title>A Simple but Effective BERT Model for Dialog State Tracking on
  Resource-Limited Systems</title>
    <summary>  In a task-oriented dialog system, the goal of dialog state tracking (DST) is
to monitor the state of the conversation from the dialog history. Recently,
many deep learning based methods have been proposed for the task. Despite their
impressive performance, current neural architectures for DST are typically
heavily-engineered and conceptually complex, making it difficult to implement,
debug, and maintain them in a production setting. In this work, we propose a
simple but effective DST model based on BERT. In addition to its simplicity,
our approach also has a number of other advantages: (a) the number of
parameters does not grow with the ontology size (b) the model can operate in
situations where the domain ontology may change dynamically. Experimental
results demonstrate that our BERT-based model outperforms previous methods by a
large margin, achieving new state-of-the-art results on the standard WoZ 2.0
dataset. Finally, to make the model small and fast enough for
resource-restricted systems, we apply the knowledge distillation method to
compress our model. The final compressed model achieves comparable results with
the original model while being 8x smaller and 7x faster.
</summary>
    <author>
      <name>Tuan Manh Lai</name>
    </author>
    <author>
      <name>Quan Hung Tran</name>
    </author>
    <author>
      <name>Trung Bui</name>
    </author>
    <author>
      <name>Daisuke Kihara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICASSP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.12995v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12995v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12956v2</id>
    <updated>2020-01-23T05:51:41Z</updated>
    <published>2019-10-28T20:32:56Z</published>
    <title>Cross-Domain Ambiguity Detection using Linear Transformation of Word
  Embedding Spaces</title>
    <summary>  The requirements engineering process is a crucial stage of the software
development life cycle. It involves various stakeholders from different
professional backgrounds, particularly in the requirements elicitation phase.
Each stakeholder carries distinct domain knowledge, causing them to differently
interpret certain words, leading to cross-domain ambiguity. This can result in
misunderstanding amongst them and jeopardize the entire project. This paper
proposes a natural language processing approach to find potentially ambiguous
words for a given set of domains. The idea is to apply linear transformations
on word embedding models trained on different domain corpora, to bring them
into a unified embedding space. The approach then finds words with divergent
embeddings as they signify a variation in the meaning across the domains. It
can help a requirements analyst in preventing misunderstandings during
elicitation interviews and meetings by defining a set of potentially ambiguous
terms in advance. The paper also discusses certain problems with the existing
approaches and discusses how the proposed approach resolves them.
</summary>
    <author>
      <name>Vaibhav Jain</name>
    </author>
    <author>
      <name>Sanskar Jain</name>
    </author>
    <author>
      <name>Nishant Tanwar</name>
    </author>
    <link href="http://arxiv.org/abs/1910.12956v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12956v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12574v1</id>
    <updated>2019-10-28T12:13:38Z</updated>
    <published>2019-10-28T12:13:38Z</published>
    <title>A BERT-Based Transfer Learning Approach for Hate Speech Detection in
  Online Social Media</title>
    <summary>  Generated hateful and toxic content by a portion of users in social media is
a rising phenomenon that motivated researchers to dedicate substantial efforts
to the challenging direction of hateful content identification. We not only
need an efficient automatic hate speech detection model based on advanced
machine learning and natural language processing, but also a sufficiently large
amount of annotated data to train a model. The lack of a sufficient amount of
labelled hate speech data, along with the existing biases, has been the main
issue in this domain of research. To address these needs, in this study we
introduce a novel transfer learning approach based on an existing pre-trained
language model called BERT (Bidirectional Encoder Representations from
Transformers). More specifically, we investigate the ability of BERT at
capturing hateful context within social media content by using new fine-tuning
methods based on transfer learning. To evaluate our proposed approach, we use
two publicly available datasets that have been annotated for racism, sexism,
hate, or offensive content on Twitter. The results show that our solution
obtains considerable performance on these datasets in terms of precision and
recall in comparison to existing approaches. Consequently, our model can
capture some biases in data annotation and collection process and can
potentially lead us to a more accurate model.
</summary>
    <author>
      <name>Marzieh Mozafari</name>
    </author>
    <author>
      <name>Reza Farahbakhsh</name>
    </author>
    <author>
      <name>Noel Crespi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted in The 8th International Conference on
  Complex Networks and their Applications</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.12574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12391v1</id>
    <updated>2019-10-28T00:50:55Z</updated>
    <published>2019-10-28T00:50:55Z</published>
    <title>What does BERT Learn from Multiple-Choice Reading Comprehension
  Datasets?</title>
    <summary>  Multiple-Choice Reading Comprehension (MCRC) requires the model to read the
passage and question, and select the correct answer among the given options.
Recent state-of-the-art models have achieved impressive performance on multiple
MCRC datasets. However, such performance may not reflect the model's true
ability of language understanding and reasoning. In this work, we adopt two
approaches to investigate what BERT learns from MCRC datasets: 1) an
un-readable data attack, in which we add keywords to confuse BERT, leading to a
significant performance drop; and 2) an un-answerable data training, in which
we train BERT on partial or shuffled input. Under un-answerable data training,
BERT achieves unexpectedly high performance. Based on our experiments on the 5
key MCRC datasets - RACE, MCTest, MCScript, MCScript2.0, DREAM - we observe
that 1) fine-tuned BERT mainly learns how keywords lead to correct prediction,
instead of learning semantic understanding and reasoning; and 2) BERT does not
need correct syntactic information to solve the task; 3) there exists artifacts
in these datasets such that they can be solved even without the full context.
</summary>
    <author>
      <name>Chenglei Si</name>
    </author>
    <author>
      <name>Shuohang Wang</name>
    </author>
    <author>
      <name>Min-Yen Kan</name>
    </author>
    <author>
      <name>Jing Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.12391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12366v2</id>
    <updated>2020-01-27T03:20:52Z</updated>
    <published>2019-10-27T22:09:13Z</published>
    <title>Thieves on Sesame Street! Model Extraction of BERT-based APIs</title>
    <summary>  We study the problem of model extraction in natural language processing, in
which an adversary with only query access to a victim model attempts to
reconstruct a local copy of that model. Assuming that both the adversary and
victim model fine-tune a large pretrained language model such as BERT (Devlin
et al. 2019), we show that the adversary does not need any real training data
to successfully mount the attack. In fact, the attacker need not even use
grammatical or semantically meaningful queries: we show that random sequences
of words coupled with task-specific heuristics form effective queries for model
extraction on a diverse set of NLP tasks, including natural language inference
and question answering. Our work thus highlights an exploit only made feasible
by the shift towards transfer learning methods within the NLP community: for a
query budget of a few hundred dollars, an attacker can extract a model that
performs only slightly worse than the victim model. Finally, we study two
defense strategies against model extraction---membership classification and API
watermarking---which while successful against naive adversaries, are
ineffective against more sophisticated ones.
</summary>
    <author>
      <name>Kalpesh Krishna</name>
    </author>
    <author>
      <name>Gaurav Singh Tomar</name>
    </author>
    <author>
      <name>Ankur P. Parikh</name>
    </author>
    <author>
      <name>Nicolas Papernot</name>
    </author>
    <author>
      <name>Mohit Iyyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2020 Camera Ready (19 pages)</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.12366v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12366v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12647v1</id>
    <updated>2019-10-25T06:25:25Z</updated>
    <published>2019-10-25T06:25:25Z</published>
    <title>HUBERT Untangles BERT to Improve Transfer across NLP Tasks</title>
    <summary>  We introduce HUBERT which combines the structured-representational power of
Tensor-Product Representations (TPRs) and BERT, a pre-trained bidirectional
Transformer language model. We show that there is shared structure between
different NLP datasets that HUBERT, but not BERT, is able to learn and
leverage. We validate the effectiveness of our model on the GLUE benchmark and
HANS dataset. Our experiment results show that untangling data-specific
semantics from general language structure is key for better transfer among NLP
tasks.
</summary>
    <author>
      <name>Mehrad Moradshahi</name>
    </author>
    <author>
      <name>Hamid Palangi</name>
    </author>
    <author>
      <name>Monica S. Lam</name>
    </author>
    <author>
      <name>Paul Smolensky</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1910.12647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.11871v1</id>
    <updated>2019-10-25T05:28:17Z</updated>
    <published>2019-10-25T05:28:17Z</published>
    <title>Towards Online End-to-end Transformer Automatic Speech Recognition</title>
    <summary>  The Transformer self-attention network has recently shown promising
performance as an alternative to recurrent neural networks in end-to-end (E2E)
automatic speech recognition (ASR) systems. However, Transformer has a drawback
in that the entire input sequence is required to compute self-attention. We
have proposed a block processing method for the Transformer encoder by
introducing a context-aware inheritance mechanism. An additional context
embedding vector handed over from the previously processed block helps to
encode not only local acoustic information but also global linguistic, channel,
and speaker attributes. In this paper, we extend it towards an entire online
E2E ASR system by introducing an online decoding process inspired by monotonic
chunkwise attention (MoChA) into the Transformer decoder. Our novel MoChA
training and inference algorithms exploit the unique properties of Transformer,
whose attentions are not always monotonic or peaky, and have multiple heads and
residual connections of the decoder layers. Evaluations of the Wall Street
Journal (WSJ) and AISHELL-1 show that our proposed online Transformer decoder
outperforms conventional chunkwise approaches.
</summary>
    <author>
      <name>Emiru Tsunoo</name>
    </author>
    <author>
      <name>Yosuke Kashiwagi</name>
    </author>
    <author>
      <name>Toshiyuki Kumakura</name>
    </author>
    <author>
      <name>Shinji Watanabe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1910.07204</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.11871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12638v2</id>
    <updated>2020-02-02T15:08:39Z</updated>
    <published>2019-10-25T01:55:12Z</published>
    <title>Mockingjay: Unsupervised Speech Representation Learning with Deep
  Bidirectional Transformer Encoders</title>
    <summary>  We present Mockingjay as a new speech representation learning approach, where
bidirectional Transformer encoders are pre-trained on a large amount of
unlabeled speech. Previous speech representation methods learn through
conditioning on past frames and predicting information about future frames.
Whereas Mockingjay is designed to predict the current frame through jointly
conditioning on both past and future contexts. The Mockingjay representation
improves performance for a wide range of downstream tasks, including phoneme
classification, speaker recognition, and sentiment classification on spoken
content, while outperforming other approaches. Mockingjay is empirically
powerful and can be fine-tuned with downstream models, with only 2 epochs we
further improve performance dramatically. In a low resource setting with only
0.1% of labeled data, we outperform the result of Mel-features that uses all
100% labeled data.
</summary>
    <author>
      <name>Andy T. Liu</name>
    </author>
    <author>
      <name>Shu-wen Yang</name>
    </author>
    <author>
      <name>Po-Han Chi</name>
    </author>
    <author>
      <name>Po-chun Hsu</name>
    </author>
    <author>
      <name>Hung-yi Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICASSP 2020, Lecture Session</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.12638v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12638v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.11450v1</id>
    <updated>2019-10-24T23:00:12Z</updated>
    <published>2019-10-24T23:00:12Z</published>
    <title>An Empirical Study of Efficient ASR Rescoring with Transformers</title>
    <summary>  Neural language models (LMs) have been proved to significantly outperform
classical n-gram LMs for language modeling due to their superior abilities to
model long-range dependencies in text and handle data sparsity problems. And
recently, well configured deep Transformers have exhibited superior performance
over shallow stack of recurrent neural network layers for language modeling.
However, these state-of-the-art deep Transformer models were mostly engineered
to be deep with high model capacity, which makes it computationally inefficient
and challenging to be deployed into large-scale real-world applications.
Therefore, it is important to develop Transformer LMs that have relatively
small model sizes, while still retaining good performance of those much larger
models. In this paper, we aim to conduct empirical study on training
Transformers with small parameter sizes in the context of ASR rescoring. By
combining techniques including subword units, adaptive softmax, large-scale
model pre-training, and knowledge distillation, we show that we are able to
successfully train small Transformer LMs with significant relative word error
rate reductions (WERR) through n-best rescoring. In particular, our experiments
on a video speech recognition dataset show that we are able to achieve WERRs
ranging from 6.46% to 7.17% while only with 5.5% to 11.9% parameter sizes of
the well-known large GPT model [1], whose WERR with rescoring on the same
dataset is 7.58%.
</summary>
    <author>
      <name>Hongzhao Huang</name>
    </author>
    <author>
      <name>Fuchun Peng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.11450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.11218v1</id>
    <updated>2019-10-24T15:23:08Z</updated>
    <published>2019-10-24T15:23:08Z</published>
    <title>Promoting the Knowledge of Source Syntax in Transformer NMT Is Not
  Needed</title>
    <summary>  The utility of linguistic annotation in neural machine translation seemed to
had been established in past papers. The experiments were however limited to
recurrent sequence-to-sequence architectures and relatively small data
settings. We focus on the state-of-the-art Transformer model and use comparably
larger corpora. Specifically, we try to promote the knowledge of source-side
syntax using multi-task learning either through simple data manipulation
techniques or through a dedicated model component. In particular, we train one
of Transformer attention heads to produce source-side dependency tree. Overall,
our results cast some doubt on the utility of multi-task setups with linguistic
information. The data manipulation techniques, recommended in previous works,
prove ineffective in large data settings. The treatment of self-attention as
dependencies seems much more promising: it helps in translation and reveals
that Transformer model can very easily grasp the syntactic structure. An
important but curious result is, however, that identical gains are obtained by
using trivial "linear trees" instead of true dependencies. The reason for the
gain thus may not be coming from the added linguistic knowledge but from some
simpler regularizing effect we induced on self-attention matrices.
</summary>
    <author>
      <name>Thuong-Hai Pham</name>
    </author>
    <author>
      <name>Dominik Mach√°ƒçek</name>
    </author>
    <author>
      <name>Ond≈ôej Bojar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13053/CyS-23-3-3265</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13053/CyS-23-3-3265" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CICLING 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computac\'ion y Sistemas, Vol. 23, No. 3, 2019, pp. 923-934</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.11218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10781v1</id>
    <updated>2019-10-23T19:51:50Z</updated>
    <published>2019-10-23T19:51:50Z</published>
    <title>Hierarchical Transformers for Long Document Classification</title>
    <summary>  BERT, which stands for Bidirectional Encoder Representations from
Transformers, is a recently introduced language representation model based upon
the transfer learning paradigm. We extend its fine-tuning procedure to address
one of its major limitations - applicability to inputs longer than a few
hundred words, such as transcripts of human call conversations. Our method is
conceptually simple. We segment the input into smaller chunks and feed each of
them into the base model. Then, we propagate each output through a single
recurrent layer, or another transformer, followed by a softmax activation. We
obtain the final classification decision after the last segment has been
consumed. We show that both BERT extensions are quick to fine-tune and converge
after as little as 1 epoch of training on a small, domain-specific data set. We
successfully apply them in three different tasks involving customer call
satisfaction prediction and topic classification, and obtain a significant
improvement over the baseline models in two of them.
</summary>
    <author>
      <name>Raghavendra Pappagari</name>
    </author>
    <author>
      <name>Piotr ≈ªelasko</name>
    </author>
    <author>
      <name>Jes√∫s Villalba</name>
    </author>
    <author>
      <name>Yishay Carmiel</name>
    </author>
    <author>
      <name>Najim Dehak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 figures, 7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Automatic Speech Recognition and Understanding Workshop, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.10781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10697v1</id>
    <updated>2019-10-23T17:57:11Z</updated>
    <published>2019-10-23T17:57:11Z</published>
    <title>Correction of Automatic Speech Recognition with Transformer
  Sequence-to-sequence Model</title>
    <summary>  In this work, we introduce a simple yet efficient post-processing model for
automatic speech recognition (ASR). Our model has Transformer-based
encoder-decoder architecture which "translates" ASR model output into
grammatically and semantically correct text. We investigate different
strategies for regularizing and optimizing the model and show that extensive
data augmentation and the initialization with pre-trained weights are required
to achieve good performance. On the LibriSpeech benchmark, our method
demonstrates significant improvement in word error rate over the baseline
acoustic model with greedy decoding, especially on much noisier dev-other and
test-other portions of the evaluation dataset. Our model also outperforms
baseline with 6-gram language model re-scoring and approaches the performance
of re-scoring with Transformer-XL neural language model.
</summary>
    <author>
      <name>Oleksii Hrinchuk</name>
    </author>
    <author>
      <name>Mariya Popova</name>
    </author>
    <author>
      <name>Boris Ginsburg</name>
    </author>
    <link href="http://arxiv.org/abs/1910.10697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10683v2</id>
    <updated>2019-10-24T15:13:50Z</updated>
    <published>2019-10-23T17:37:36Z</published>
    <title>Exploring the Limits of Transfer Learning with a Unified Text-to-Text
  Transformer</title>
    <summary>  Transfer learning, where a model is first pre-trained on a data-rich task
before being fine-tuned on a downstream task, has emerged as a powerful
technique in natural language processing (NLP). The effectiveness of transfer
learning has given rise to a diversity of approaches, methodology, and
practice. In this paper, we explore the landscape of transfer learning
techniques for NLP by introducing a unified framework that converts every
language problem into a text-to-text format. Our systematic study compares
pre-training objectives, architectures, unlabeled datasets, transfer
approaches, and other factors on dozens of language understanding tasks. By
combining the insights from our exploration with scale and our new "Colossal
Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks
covering summarization, question answering, text classification, and more. To
facilitate future work on transfer learning for NLP, we release our dataset,
pre-trained models, and code.
</summary>
    <author>
      <name>Colin Raffel</name>
    </author>
    <author>
      <name>Noam Shazeer</name>
    </author>
    <author>
      <name>Adam Roberts</name>
    </author>
    <author>
      <name>Katherine Lee</name>
    </author>
    <author>
      <name>Sharan Narang</name>
    </author>
    <author>
      <name>Michael Matena</name>
    </author>
    <author>
      <name>Yanqi Zhou</name>
    </author>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Peter J. Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1910.10683v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10683v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10352v1</id>
    <updated>2019-10-23T04:57:51Z</updated>
    <published>2019-10-23T04:57:51Z</published>
    <title>A Transformer with Interleaved Self-attention and Convolution for Hybrid
  Acoustic Models</title>
    <summary>  Transformer with self-attention has achieved great success in the area of
nature language processing. Recently, there have been a few studies on
transformer for end-to-end speech recognition, while its application for hybrid
acoustic model is still very limited. In this paper, we revisit the
transformer-based hybrid acoustic model, and propose a model structure with
interleaved self-attention and 1D convolution, which is proven to have faster
convergence and higher recognition accuracy. We also study several aspects of
the transformer model, including the impact of the positional encoding feature,
dropout regularization, as well as training with and without time restriction.
We show competitive recognition results on the public Librispeech dataset when
compared to the Kaldi baseline at both cross entropy training and sequence
training stages. For reproducible research, we release our source code and
recipe within the PyKaldi2 toolbox.
</summary>
    <author>
      <name>Liang Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, submitted to ICASSP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.10352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10324v2</id>
    <updated>2020-02-13T06:07:18Z</updated>
    <published>2019-10-23T02:48:30Z</published>
    <title>Deja-vu: Double Feature Presentation and Iterated Loss in Deep
  Transformer Networks</title>
    <summary>  Deep acoustic models typically receive features in the first layer of the
network, and process increasingly abstract representations in the subsequent
layers. Here, we propose to feed the input features at multiple depths in the
acoustic model. As our motivation is to allow acoustic models to re-examine
their input features in light of partial hypotheses we introduce intermediate
model heads and loss function. We study this architecture in the context of
deep Transformer networks, and we use an attention mechanism over both the
previous layer activations and the input features. To train this model's
intermediate output hypothesis, we apply the objective function at each layer
right before feature re-use. We find that the use of such iterated loss
significantly improves performance by itself, as well as enabling input feature
re-use. We present results on both Librispeech, and a large scale video
dataset, with relative improvements of 10 - 20% for Librispeech and 3.2 - 13%
for videos.
</summary>
    <author>
      <name>Andros Tjandra</name>
    </author>
    <author>
      <name>Chunxi Liu</name>
    </author>
    <author>
      <name>Frank Zhang</name>
    </author>
    <author>
      <name>Xiaohui Zhang</name>
    </author>
    <author>
      <name>Yongqiang Wang</name>
    </author>
    <author>
      <name>Gabriel Synnaeve</name>
    </author>
    <author>
      <name>Satoshi Nakamura</name>
    </author>
    <author>
      <name>Geoffrey Zweig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in IEEE ICASSP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.10324v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10324v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10073v4</id>
    <updated>2020-02-14T20:49:40Z</updated>
    <published>2019-10-22T16:15:58Z</published>
    <title>Depth-Adaptive Transformer</title>
    <summary>  State of the art sequence-to-sequence models for large scale tasks perform a
fixed number of computations for each input sequence regardless of whether it
is easy or hard to process. In this paper, we train Transformer models which
can make output predictions at different stages of the network and we
investigate different ways to predict how much computation is required for a
particular sequence. Unlike dynamic computation in Universal Transformers,
which applies the same set of layers iteratively, we apply different layers at
every step to adjust both the amount of computation as well as the model
capacity. On IWSLT German-English translation our approach matches the accuracy
of a well tuned baseline Transformer while using less than a quarter of the
decoder layers.
</summary>
    <author>
      <name>Maha Elbayad</name>
    </author>
    <author>
      <name>Jiatao Gu</name>
    </author>
    <author>
      <name>Edouard Grave</name>
    </author>
    <author>
      <name>Michael Auli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.10073v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10073v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.09932v3</id>
    <updated>2019-10-31T13:54:23Z</updated>
    <published>2019-10-22T12:47:29Z</published>
    <title>Improving Transformer-based Speech Recognition Using Unsupervised
  Pre-training</title>
    <summary>  Speech recognition technologies are gaining enormous popularity in various
industrial applications. However, building a good speech recognition system
usually requires large amounts of transcribed data, which is expensive to
collect. To tackle this problem, an unsupervised pre-training method called
Masked Predictive Coding is proposed, which can be applied for unsupervised
pre-training with Transformer based model. Experiments on HKUST show that using
the same training data, we can achieve CER 23.3%, exceeding the best end-to-end
model by over 0.2% absolute CER. With more pre-training data, we can further
reduce the CER to 21.0%, or a 11.8% relative CER reduction over baseline.
</summary>
    <author>
      <name>Dongwei Jiang</name>
    </author>
    <author>
      <name>Xiaoning Lei</name>
    </author>
    <author>
      <name>Wubo Li</name>
    </author>
    <author>
      <name>Ne Luo</name>
    </author>
    <author>
      <name>Yuxuan Hu</name>
    </author>
    <author>
      <name>Wei Zou</name>
    </author>
    <author>
      <name>Xiangang Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.09932v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.09932v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.09799v1</id>
    <updated>2019-10-22T07:20:02Z</updated>
    <published>2019-10-22T07:20:02Z</published>
    <title>Transformer-based Acoustic Modeling for Hybrid Speech Recognition</title>
    <summary>  We propose and evaluate transformer-based acoustic models (AMs) for hybrid
speech recognition. Several modeling choices are discussed in this work,
including various positional embedding methods and an iterated loss to enable
training deep transformers. We also present a preliminary study of using
limited right context in transformer models, which makes it possible for
streaming applications. We demonstrate that on the widely used Librispeech
benchmark, our transformer-based AM outperforms the best published hybrid
result by 19% to 26% relative when the standard n-gram language model (LM) is
used. Combined with neural network LM for rescoring, our proposed approach
achieves state-of-the-art results on Librispeech. Our findings are also
confirmed on a much larger internal dataset.
</summary>
    <author>
      <name>Yongqiang Wang</name>
    </author>
    <author>
      <name>Abdelrahman Mohamed</name>
    </author>
    <author>
      <name>Duc Le</name>
    </author>
    <author>
      <name>Chunxi Liu</name>
    </author>
    <author>
      <name>Alex Xiao</name>
    </author>
    <author>
      <name>Jay Mahadeokar</name>
    </author>
    <author>
      <name>Hongzhao Huang</name>
    </author>
    <author>
      <name>Andros Tjandra</name>
    </author>
    <author>
      <name>Xiaohui Zhang</name>
    </author>
    <author>
      <name>Frank Zhang</name>
    </author>
    <author>
      <name>Christian Fuegen</name>
    </author>
    <author>
      <name>Geoffrey Zweig</name>
    </author>
    <author>
      <name>Michael L. Seltzer</name>
    </author>
    <link href="http://arxiv.org/abs/1910.09799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.09799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10492v1</id>
    <updated>2019-10-17T23:00:22Z</updated>
    <published>2019-10-17T23:00:22Z</published>
    <title>Question Classification with Deep Contextualized Transformer</title>
    <summary>  The latest work for Question and Answer problems is to use the Stanford Parse
Tree. We build on prior work and develop a new method to handle the Question
and Answer problem with the Deep Contextualized Transformer to manage some
aberrant expressions. We also conduct extensive evaluations of the SQuAD and
SwDA dataset and show significant improvement over QA problem classification of
industry needs. We also investigate the impact of different models for the
accuracy and efficiency of the problem answers. It shows that our new method is
more effective for solving QA problems with higher accuracy
</summary>
    <author>
      <name>Haozheng Luo</name>
    </author>
    <author>
      <name>Ningwei Liu</name>
    </author>
    <author>
      <name>Charles Feng</name>
    </author>
    <link href="http://arxiv.org/abs/1910.10492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.07973v2</id>
    <updated>2019-10-23T23:56:32Z</updated>
    <published>2019-10-17T15:33:26Z</published>
    <title>Universal Text Representation from BERT: An Empirical Study</title>
    <summary>  We present a systematic investigation of layer-wise BERT activations for
general-purpose text representations to understand what linguistic information
they capture and how transferable they are across different tasks.
Sentence-level embeddings are evaluated against two state-of-the-art models on
downstream and probing tasks from SentEval, while passage-level embeddings are
evaluated on four question-answering (QA) datasets under a learning-to-rank
problem setting. Embeddings from the pre-trained BERT model perform poorly in
semantic similarity and sentence surface information probing tasks. Fine-tuning
BERT on natural language inference data greatly improves the quality of the
embeddings. Combining embeddings from different BERT layers can further boost
performance. BERT embeddings outperform BM25 baseline significantly on factoid
QA datasets at the passage level, but fail to perform better than BM25 on
non-factoid datasets. For all QA datasets, there is a gap between
embedding-based method and in-domain fine-tuned BERT (we report new
state-of-the-art results on two datasets), which suggests deep interactions
between question and answer pairs are critical for those hard tasks.
</summary>
    <author>
      <name>Xiaofei Ma</name>
    </author>
    <author>
      <name>Zhiguo Wang</name>
    </author>
    <author>
      <name>Patrick Ng</name>
    </author>
    <author>
      <name>Ramesh Nallapati</name>
    </author>
    <author>
      <name>Bing Xiang</name>
    </author>
    <link href="http://arxiv.org/abs/1910.07973v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07973v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.07713v1</id>
    <updated>2019-10-17T05:19:29Z</updated>
    <published>2019-10-17T05:19:29Z</published>
    <title>BIG MOOD: Relating Transformers to Explicit Commonsense Knowledge</title>
    <summary>  We introduce a simple yet effective method of integrating contextual
embeddings with commonsense graph embeddings, dubbed BERT Infused Graphs:
Matching Over Other embeDdings. First, we introduce a preprocessing method to
improve the speed of querying knowledge bases. Then, we develop a method of
creating knowledge embeddings from each knowledge base. We introduce a method
of aligning tokens between two misaligned tokenization methods. Finally, we
contribute a method of contextualizing BERT after combining with knowledge base
embeddings. We also show BERTs tendency to correct lower accuracy question
types. Our model achieves a higher accuracy than BERT, and we score fifth on
the official leaderboard of the shared task and score the highest without any
additional language model pretraining.
</summary>
    <author>
      <name>Jeff Da</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP Commonsense (COIN)</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.07713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10485v2</id>
    <updated>2019-11-23T22:44:17Z</updated>
    <published>2019-10-17T01:29:12Z</published>
    <title>Fully Quantized Transformer for Improved Translation</title>
    <summary>  State-of-the-art neural machine translation methods employ massive amounts of
parameters. Drastically reducing computational costs of such methods without
affecting performance has been up to this point unsuccessful. To the best of
our knowledge, we are the first to propose a quantization strategy inclusive of
all components of the Transformer. We are also the first to show that it is
possible to avoid any loss in translation quality with a fully quantized
network. Indeed, our 8-bit models consistently score equal or higher BLEU than
the full-precision variant on multiple translation datasets. Comparing
ourselves to all previously proposed methods, we achieve state-of-the-art
quantization results.
</summary>
    <author>
      <name>Gabriele Prato</name>
    </author>
    <author>
      <name>Ella Charlaix</name>
    </author>
    <author>
      <name>Mehdi Rezagholizadeh</name>
    </author>
    <link href="http://arxiv.org/abs/1910.10485v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10485v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10488v1</id>
    <updated>2019-10-16T15:48:46Z</updated>
    <published>2019-10-16T15:48:46Z</published>
    <title>Injecting Hierarchy with U-Net Transformers</title>
    <summary>  The Transformer architecture has become increasingly popular over the past
couple of years, owing to its impressive performance on a number of natural
language processing (NLP) tasks. However, it may be argued that the Transformer
architecture lacks an explicit hierarchical representation, as all computations
occur on word-level representations alone, and therefore, learning structure
poses a challenge for Transformer models. In the present work, we introduce
hierarchical processing into the Transformer model, taking inspiration from the
U-Net architecture, popular in computer vision for its hierarchical view of
natural images. We propose a novel architecture that combines ideas from
Transformer and U-Net models to incorporate hierarchy at multiple levels of
abstraction. We empirically demonstrate that the proposed architecture
outperforms the vanilla Transformer and strong baselines in the chit-chat
dialogue and machine translation domains.
</summary>
    <author>
      <name>David Donahue</name>
    </author>
    <author>
      <name>Vladislav Lialin</name>
    </author>
    <author>
      <name>Anna Rumshisky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.10488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.07204v1</id>
    <updated>2019-10-16T08:04:07Z</updated>
    <published>2019-10-16T08:04:07Z</published>
    <title>Transformer ASR with Contextual Block Processing</title>
    <summary>  The Transformer self-attention network has recently shown promising
performance as an alternative to recurrent neural networks (RNNs) in end-to-end
(E2E) automatic speech recognition (ASR) systems. However, the Transformer has
a drawback in that the entire input sequence is required to compute
self-attention. In this paper, we propose a new block processing method for the
Transformer encoder by introducing a context-aware inheritance mechanism. An
additional context embedding vector handed over from the previously processed
block helps to encode not only local acoustic information but also global
linguistic, channel, and speaker attributes. We introduce a novel mask
technique to implement the context inheritance to train the model efficiently.
Evaluations of the Wall Street Journal (WSJ), Librispeech, VoxForge Italian,
and AISHELL-1 Mandarin speech recognition datasets show that our proposed
contextual block processing method outperforms naive block processing
consistently. Furthermore, the attention weight tendency of each layer is
analyzed to clarify how the added contextual inheritance mechanism models the
global information.
</summary>
    <author>
      <name>Emiru Tsunoo</name>
    </author>
    <author>
      <name>Yosuke Kashiwagi</name>
    </author>
    <author>
      <name>Toshiyuki Kumakura</name>
    </author>
    <author>
      <name>Shinji Watanabe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for ASRU 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.07204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.07179v4</id>
    <updated>2019-11-27T03:11:36Z</updated>
    <published>2019-10-16T05:50:35Z</published>
    <title>Content Enhanced BERT-based Text-to-SQL Generation</title>
    <summary>  We present a simple methods to leverage the table content for the BERT-based
model to solve the text-to-SQL problem. Based on the observation that some of
the table content match some words in question string and some of the table
header also match some words in question string, we encode two addition feature
vector for the deep model. Our methods also benefit the model inference in
testing time as the tables are almost the same in training and testing time. We
test our model on the WikiSQL dataset and outperform the BERT-based baseline by
3.7% in logic form and 3.7% in execution accuracy and achieve state-of-the-art.
</summary>
    <author>
      <name>Tong Guo</name>
    </author>
    <author>
      <name>Huilin Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">working in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.07179v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07179v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06431v1</id>
    <updated>2019-10-14T21:32:38Z</updated>
    <published>2019-10-14T21:32:38Z</published>
    <title>Whatcha lookin' at? DeepLIFTing BERT's Attention in Question Answering</title>
    <summary>  There has been great success recently in tackling challenging NLP tasks by
neural networks which have been pre-trained and fine-tuned on large amounts of
task data. In this paper, we investigate one such model, BERT for
question-answering, with the aim to analyze why it is able to achieve
significantly better results than other models. We run DeepLIFT on the model
predictions and test the outcomes to monitor shift in the attention values for
input. We also cluster the results to analyze any possible patterns similar to
human reasoning depending on the kind of input paragraph and question the model
is trying to answer.
</summary>
    <author>
      <name>Ekaterina Arkhangelskaia</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Saarland University</arxiv:affiliation>
    </author>
    <author>
      <name>Sourav Dutta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Saarland University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06360v1</id>
    <updated>2019-10-14T18:12:30Z</updated>
    <published>2019-10-14T18:12:30Z</published>
    <title>Pruning a BERT-based Question Answering Model</title>
    <summary>  We investigate compressing a BERT-based question answering system by pruning
parameters from the underlying BERT model. We start from models trained for
SQuAD 2.0 and introduce gates that allow selected parts of transformers to be
individually eliminated. Specifically, we investigate (1) reducing the number
of attention heads in each transformer, (2) reducing the intermediate width of
the feed-forward sublayer of each transformer, and (3) reducing the embedding
dimension. We compare several approaches for determining the values of these
gates. We find that a combination of pruning attention heads and the
feed-forward layer almost doubles the decoding speed, with only a 1.5 f-point
loss in accuracy.
</summary>
    <author>
      <name>J. S. McCarley</name>
    </author>
    <link href="http://arxiv.org/abs/1910.06360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06188v2</id>
    <updated>2019-10-17T17:15:24Z</updated>
    <published>2019-10-14T14:55:19Z</published>
    <title>Q8BERT: Quantized 8Bit BERT</title>
    <summary>  Recently, pre-trained Transformer based language models such as BERT and GPT,
have shown great improvement in many Natural Language Processing (NLP) tasks.
However, these models contain a large amount of parameters. The emergence of
even larger and more accurate models such as GPT2 and Megatron, suggest a trend
of large pre-trained Transformer models. However, using these large models in
production environments is a complex task requiring a large amount of compute,
memory and power resources. In this work we show how to perform
quantization-aware training during the fine-tuning phase of BERT in order to
compress BERT by $4\times$ with minimal accuracy loss. Furthermore, the
produced quantized model can accelerate inference speed if it is optimized for
8bit Integer supporting hardware.
</summary>
    <author>
      <name>Ofir Zafrir</name>
    </author>
    <author>
      <name>Guy Boudoukh</name>
    </author>
    <author>
      <name>Peter Izsak</name>
    </author>
    <author>
      <name>Moshe Wasserblat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages, Accepted at the 5th Workshop on Energy Efficient Machine
  Learning and Cognitive Computing - NeurIPS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06188v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06188v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.05895v2</id>
    <updated>2019-12-30T03:53:04Z</updated>
    <published>2019-10-14T02:23:43Z</published>
    <title>Transformers without Tears: Improving the Normalization of
  Self-Attention</title>
    <summary>  We evaluate three simple, normalization-centric changes to improve
Transformer training. First, we show that pre-norm residual connections
(PreNorm) and smaller initializations enable warmup-free, validation-based
training with large learning rates. Second, we propose $\ell_2$ normalization
with a single scale parameter (ScaleNorm) for faster training and better
performance. Finally, we reaffirm the effectiveness of normalizing word
embeddings to a fixed length (FixNorm). On five low-resource translation pairs
from TED Talks-based corpora, these changes always converge, giving an average
+1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on
IWSLT'15 English-Vietnamese. We observe sharper performance curves, more
consistent gradient norms, and a linear relationship between activation scaling
and decoder depth. Surprisingly, in the high-resource setting (WMT'14
English-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades
performance.
</summary>
    <author>
      <name>Toan Q. Nguyen</name>
    </author>
    <author>
      <name>Julian Salazar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.3525484</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.3525484" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IWSLT 2019 (oral); code is available at
  https://github.com/tnq177/transformers_without_tears</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.05895v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.05895v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.05786v2</id>
    <updated>2019-10-24T16:20:06Z</updated>
    <published>2019-10-13T16:54:21Z</published>
    <title>Progress Notes Classification and Keyword Extraction using
  Attention-based Deep Learning Models with BERT</title>
    <summary>  Various deep learning algorithms have been developed to analyze different
types of clinical data including clinical text classification and extracting
information from 'free text' and so on. However, automate the keyword
extraction from the clinical notes is still challenging. The challenges include
dealing with noisy clinical notes which contain various abbreviations, possible
typos, and unstructured sentences. The objective of this research is to
investigate the attention-based deep learning models to classify the
de-identified clinical progress notes extracted from a real-world EHR system.
The attention-based deep learning models can be used to interpret the models
and understand the critical words that drive the correct or incorrect
classification of the clinical progress notes. The attention-based models in
this research are capable of presenting the human interpretable text
classification models. The results show that the fine-tuned BERT with the
attention layer can achieve a high classification accuracy of 97.6%, which is
higher than the baseline fine-tuned BERT classification model. In this
research, we also demonstrate that the attention-based models can identify
relevant keywords that are strongly related to the clinical progress note
categories.
</summary>
    <author>
      <name>Matthew Tang</name>
    </author>
    <author>
      <name>Priyanka Gandhi</name>
    </author>
    <author>
      <name>Md Ahsanul Kabir</name>
    </author>
    <author>
      <name>Christopher Zou</name>
    </author>
    <author>
      <name>Jordyn Blakey</name>
    </author>
    <author>
      <name>Xiao Luo</name>
    </author>
    <link href="http://arxiv.org/abs/1910.05786v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.05786v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.05276v1</id>
    <updated>2019-10-11T16:10:55Z</updated>
    <published>2019-10-11T16:10:55Z</published>
    <title>exBERT: A Visual Analysis Tool to Explore Learned Representations in
  Transformers Models</title>
    <summary>  Large language models can produce powerful contextual representations that
lead to improvements across many NLP tasks. Since these models are typically
guided by a sequence of learned self attention mechanisms and may comprise
undesired inductive biases, it is paramount to be able to explore what the
attention has learned. While static analyses of these models lead to targeted
insights, interactive tools are more dynamic and can help humans better gain an
intuition for the model-internal reasoning process. We present exBERT, an
interactive tool named after the popular BERT language model, that provides
insights into the meaning of the contextual representations by matching a
human-specified input to similar contexts in a large annotated dataset. By
aggregating the annotations of the matching similar contexts, exBERT helps
intuitively explain what each attention-head has learned.
</summary>
    <author>
      <name>Benjamin Hoover</name>
    </author>
    <author>
      <name>Hendrik Strobelt</name>
    </author>
    <author>
      <name>Sebastian Gehrmann</name>
    </author>
    <link href="http://arxiv.org/abs/1910.05276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.05276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.03806v1</id>
    <updated>2019-10-09T06:35:59Z</updated>
    <published>2019-10-09T06:35:59Z</published>
    <title>Is Multilingual BERT Fluent in Language Generation?</title>
    <summary>  The multilingual BERT model is trained on 104 languages and meant to serve as
a universal language model and tool for encoding sentences. We explore how well
the model performs on several languages across several tasks: a diagnostic
classification probing the embeddings for a particular syntactic property, a
cloze task testing the language modelling ability to fill in gaps in a
sentence, and a natural language generation task testing for the ability to
produce coherent text fitting a given context. We find that the currently
available multilingual BERT model is clearly inferior to the monolingual
counterparts, and cannot in many cases serve as a substitute for a well-trained
monolingual model. We find that the English and German models perform well at
generation, whereas the multilingual model is lacking, in particular, for
Nordic languages.
</summary>
    <author>
      <name>Samuel R√∂nnqvist</name>
    </author>
    <author>
      <name>Jenna Kanerva</name>
    </author>
    <author>
      <name>Tapio Salakoski</name>
    </author>
    <author>
      <name>Filip Ginter</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In proceedings of the First NLPL Workshop on Deep Learning for
  Natural Language Processing (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.03806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.03806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.03771v4</id>
    <updated>2020-02-11T14:42:10Z</updated>
    <published>2019-10-09T03:23:22Z</published>
    <title>HuggingFace's Transformers: State-of-the-art Natural Language Processing</title>
    <summary>  Recent advances in modern Natural Language Processing (NLP) research have
been dominated by the combination of Transfer Learning methods with large-scale
language models, in particular based on the Transformer architecture. With them
came a paradigm shift in NLP with the starting point for training a model on a
downstream task moving from a blank specific model to a general-purpose
pretrained architecture. Still, creating these general-purpose models remains
an expensive and time-consuming process restricting the use of these methods to
a small sub-set of the wider NLP community. In this paper, we present
HuggingFace's Transformers library, a library for state-of-the-art NLP, making
these developments available to the community by gathering state-of-the-art
general-purpose pretrained models under a unified API together with an
ecosystem of libraries, examples, tutorials and scripts targeting many
downstream NLP tasks. HuggingFace's Transformers library features carefully
crafted model implementations and high-performance pretrained weights for two
main deep learning frameworks, PyTorch and TensorFlow, while supporting all the
necessary tools to analyze, evaluate and use these models in downstream tasks
such as text/token classification, questions answering and language generation
among others. The library has gained significant organic traction and adoption
among both the researcher and practitioner communities. We are committed at
HuggingFace to pursue the efforts to develop this toolkit with the ambition of
creating the standard library for building NLP systems. HuggingFace's
Transformers library is available at
\url{https://github.com/huggingface/transformers}.
</summary>
    <author>
      <name>Thomas Wolf</name>
    </author>
    <author>
      <name>Lysandre Debut</name>
    </author>
    <author>
      <name>Victor Sanh</name>
    </author>
    <author>
      <name>Julien Chaumond</name>
    </author>
    <author>
      <name>Clement Delangue</name>
    </author>
    <author>
      <name>Anthony Moi</name>
    </author>
    <author>
      <name>Pierric Cistac</name>
    </author>
    <author>
      <name>Tim Rault</name>
    </author>
    <author>
      <name>R√©mi Louf</name>
    </author>
    <author>
      <name>Morgan Funtowicz</name>
    </author>
    <author>
      <name>Jamie Brew</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure, more details at
  https://github.com/huggingface/transformers</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.03771v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.03771v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02974v3</id>
    <updated>2020-03-09T14:35:47Z</updated>
    <published>2019-10-07T18:03:14Z</published>
    <title>SMArT: Training Shallow Memory-aware Transformers for Robotic
  Explainability</title>
    <summary>  The ability to generate natural language explanations conditioned on the
visual perception is a crucial step towards autonomous agents which can explain
themselves and communicate with humans. While the research efforts in image and
video captioning are giving promising results, this is often done at the
expense of the computational requirements of the approaches, limiting their
applicability to real contexts. In this paper, we propose a fully-attentive
captioning algorithm which can provide state-of-the-art performances on
language generation while restricting its computational demands. Our model is
inspired by the Transformer model and employs only two Transformer layers in
the encoding and decoding stages. Further, it incorporates a novel memory-aware
encoding of image regions. Experiments demonstrate that our approach achieves
competitive results in terms of caption quality while featuring reduced
computational demands. Further, to evaluate its applicability on autonomous
agents, we conduct experiments on simulated scenes taken from the perspective
of domestic robots.
</summary>
    <author>
      <name>Marcella Cornia</name>
    </author>
    <author>
      <name>Lorenzo Baraldi</name>
    </author>
    <author>
      <name>Rita Cucchiara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICRA 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.02974v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02974v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02655v1</id>
    <updated>2019-10-07T07:58:26Z</updated>
    <published>2019-10-07T07:58:26Z</published>
    <title>BERT for Evidence Retrieval and Claim Verification</title>
    <summary>  Motivated by the promising performance of pre-trained language models, we
investigate BERT in an evidence retrieval and claim verification pipeline for
the FEVER fact extraction and verification challenge. To this end, we propose
to use two BERT models, one for retrieving potential evidence sentences
supporting or rejecting claims, and another for verifying claims based on the
predicted evidence sets. To train the BERT retrieval system, we use pointwise
and pairwise loss functions, and examine the effect of hard negative mining. A
second BERT model is trained to classify the samples as supported, refuted, and
not enough information. Our system achieves a new state of the art recall of
87.1 for retrieving top five sentences out of the FEVER documents consisting of
50K Wikipedia pages, and scores second in the official leaderboard with the
FEVER score of 69.7.
</summary>
    <author>
      <name>Amir Soleimani</name>
    </author>
    <author>
      <name>Christof Monz</name>
    </author>
    <author>
      <name>Marcel Worring</name>
    </author>
    <link href="http://arxiv.org/abs/1910.02655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02375v1</id>
    <updated>2019-10-06T05:06:38Z</updated>
    <published>2019-10-06T05:06:38Z</published>
    <title>Design and Use of Loop-Transformation Pragmas</title>
    <summary>  Adding a pragma directive into the source code is arguably easier than
rewriting it, for instance for loop unrolling. Moreover, if the application is
maintained for multiple platforms, their difference in performance
characteristics may require different code transformations. Code transformation
directives allow replacing the directives depending on the platform, i.e.
separation of code semantics and its performance optimization.
  In this paper, we explore the design space (syntax and semantics) of adding
such directive into a future OpenMP specification. Using a prototype
implementation in Clang, we demonstrate the usefulness of such directives on a
few benchmarks.
</summary>
    <author>
      <name>Michael Kruse</name>
    </author>
    <author>
      <name>Hal Finkel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-28596-8_9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-28596-8_9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IWOMP 2019, September 11-13, Auckland, preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.02375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02238v2</id>
    <updated>2019-10-17T16:19:22Z</updated>
    <published>2019-10-05T09:16:44Z</published>
    <title>How Transformer Revitalizes Character-based Neural Machine Translation:
  An Investigation on Japanese-Vietnamese Translation Systems</title>
    <summary>  While translating between East Asian languages, many works have discovered
clear advantages of using characters as the translation unit. Unfortunately,
traditional recurrent neural machine translation systems hinder the practical
usage of those character-based systems due to their architectural limitations.
They are unfavorable in handling extremely long sequences as well as highly
restricted in parallelizing the computations. In this paper, we demonstrate
that the new transformer architecture can perform character-based translation
better than the recurrent one. We conduct experiments on a low-resource
language pair: Japanese-Vietnamese. Our models considerably outperform the
state-of-the-art systems which employ word-based recurrent architectures.
</summary>
    <author>
      <name>Thi-Vinh Ngo</name>
    </author>
    <author>
      <name>Thanh-Le Ha</name>
    </author>
    <author>
      <name>Phuong-Thai Nguyen</name>
    </author>
    <author>
      <name>Le-Minh Nguyen</name>
    </author>
    <link href="http://arxiv.org/abs/1910.02238v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02238v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.03474v1</id>
    <updated>2019-10-04T09:20:48Z</updated>
    <published>2019-10-04T09:20:48Z</published>
    <title>Fine-grained Sentiment Classification using BERT</title>
    <summary>  Sentiment classification is an important process in understanding people's
perception towards a product, service, or topic. Many natural language
processing models have been proposed to solve the sentiment classification
problem. However, most of them have focused on binary sentiment classification.
In this paper, we use a promising deep learning model called BERT to solve the
fine-grained sentiment classification task. Experiments show that our model
outperforms other popular models for this task without sophisticated
architecture. We also demonstrate the effectiveness of transfer learning in
natural language processing in the process.
</summary>
    <author>
      <name>Manish Munikar</name>
    </author>
    <author>
      <name>Sushil Shakya</name>
    </author>
    <author>
      <name>Aakash Shrestha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE International Conference on Artificial Intelligence
  for Transforming Business and Society</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.03474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.03474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.01769v1</id>
    <updated>2019-10-04T01:01:26Z</updated>
    <published>2019-10-04T01:01:26Z</published>
    <title>Distilling Transformers into Simple Neural Networks with Unlabeled
  Transfer Data</title>
    <summary>  Recent advances in pre-training huge models on large amounts of text through
self supervision have obtained state-of-the-art results in various natural
language processing tasks. However, these huge and expensive models are
difficult to use in practise for downstream tasks. Some recent efforts use
knowledge distillation to compress these models. However, we see a gap between
the performance of the smaller student models as compared to that of the large
teacher. In this work, we leverage large amounts of in-domain unlabeled
transfer data in addition to a limited amount of labeled training instances to
bridge this gap. We show that simple RNN based student models even with hard
distillation can perform at par with the huge teachers given the transfer set.
The student performance can be further improved with soft distillation and
leveraging teacher intermediate representations. We show that our student
models can compress the huge teacher by up to 26x while still matching or even
marginally exceeding the teacher performance in low-resource settings with
small amount of labeled data.
</summary>
    <author>
      <name>Subhabrata Mukherjee</name>
    </author>
    <author>
      <name>Ahmed Hassan Awadallah</name>
    </author>
    <link href="http://arxiv.org/abs/1910.01769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.01769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.01108v4</id>
    <updated>2020-03-01T02:57:50Z</updated>
    <published>2019-10-02T17:56:28Z</published>
    <title>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and
  lighter</title>
    <summary>  As Transfer Learning from large-scale pre-trained models becomes more
prevalent in Natural Language Processing (NLP), operating these large models in
on-the-edge and/or under constrained computational training or inference
budgets remains challenging. In this work, we propose a method to pre-train a
smaller general-purpose language representation model, called DistilBERT, which
can then be fine-tuned with good performances on a wide range of tasks like its
larger counterparts. While most prior work investigated the use of distillation
for building task-specific models, we leverage knowledge distillation during
the pre-training phase and show that it is possible to reduce the size of a
BERT model by 40%, while retaining 97% of its language understanding
capabilities and being 60% faster. To leverage the inductive biases learned by
larger models during pre-training, we introduce a triple loss combining
language modeling, distillation and cosine-distance losses. Our smaller, faster
and lighter model is cheaper to pre-train and we demonstrate its capabilities
for on-device computations in a proof-of-concept experiment and a comparative
on-device study.
</summary>
    <author>
      <name>Victor Sanh</name>
    </author>
    <author>
      <name>Lysandre Debut</name>
    </author>
    <author>
      <name>Julien Chaumond</name>
    </author>
    <author>
      <name>Thomas Wolf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">February 2020 - Revision: fix bug in evaluation metrics, updated
  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at
  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing
  - NeurIPS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.01108v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.01108v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00883v2</id>
    <updated>2019-10-04T04:05:46Z</updated>
    <published>2019-10-02T11:34:58Z</published>
    <title>Exploiting BERT for End-to-End Aspect-based Sentiment Analysis</title>
    <summary>  In this paper, we investigate the modeling power of contextualized embeddings
from pre-trained language models, e.g. BERT, on the E2E-ABSA task.
Specifically, we build a series of simple yet insightful neural baselines to
deal with E2E-ABSA. The experimental results show that even with a simple
linear classification layer, our BERT-based architecture can outperform
state-of-the-art works. Besides, we also standardize the comparative study by
consistently utilizing a hold-out validation dataset for model selection, which
is largely ignored by previous works. Therefore, our work can serve as a
BERT-based benchmark for E2E-ABSA.
</summary>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Lidong Bing</name>
    </author>
    <author>
      <name>Wenxuan Zhang</name>
    </author>
    <author>
      <name>Wai Lam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NUT workshop@EMNLP-IJCNLP-2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.00883v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00883v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00702v1</id>
    <updated>2019-10-01T22:34:40Z</updated>
    <published>2019-10-01T22:34:40Z</published>
    <title>TransGCN:Coupling Transformation Assumptions with Graph Convolutional
  Networks for Link Prediction</title>
    <summary>  Link prediction is an important and frequently studied task that contributes
to an understanding of the structure of knowledge graphs (KGs) in statistical
relational learning. Inspired by the success of graph convolutional networks
(GCN) in modeling graph data, we propose a unified GCN framework, named
TransGCN, to address this task, in which relation and entity embeddings are
learned simultaneously. To handle heterogeneous relations in KGs, we introduce
a novel way of representing heterogeneous neighborhood by introducing
transformation assumptions on the relationship between the subject, the
relation, and the object of a triple. Specifically, a relation is treated as a
transformation operator transforming a head entity to a tail entity. Both
translation assumption in TransE and rotation assumption in RotatE are explored
in our framework. Additionally, instead of only learning entity embeddings in
the convolution-based encoder while learning relation embeddings in the decoder
as done by the state-of-art models, e.g., R-GCN, the TransGCN framework trains
relation embeddings and entity embeddings simultaneously during the graph
convolution operation, thus having fewer parameters compared with R-GCN.
Experiments show that our models outperform the-state-of-arts methods on both
FB15K-237 and WN18RR.
</summary>
    <author>
      <name>Ling Cai</name>
    </author>
    <author>
      <name>Bo Yan</name>
    </author>
    <author>
      <name>Gengchen Mai</name>
    </author>
    <author>
      <name>Krzysztof Janowicz</name>
    </author>
    <author>
      <name>Rui Zhu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3360901.3364441</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3360901.3364441" rel="related"/>
    <link href="http://arxiv.org/abs/1910.00702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06717v1</id>
    <updated>2019-10-01T19:21:26Z</updated>
    <published>2019-10-01T19:21:26Z</published>
    <title>Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and
  Performance for Low-Resource Machine Translation</title>
    <summary>  Neural sequence-to-sequence models, particularly the Transformer, are the
state of the art in machine translation. Yet these neural networks are very
sensitive to architecture and hyperparameter settings. Optimizing these
settings by grid or random search is computationally expensive because it
requires many training runs. In this paper, we incorporate architecture search
into a single training run through auto-sizing, which uses regularization to
delete neurons in a network over the course of training. On very low-resource
language pairs, we show that auto-sizing can improve BLEU scores by up to 3.9
points while removing one-third of the parameters from the model.
</summary>
    <author>
      <name>Kenton Murray</name>
    </author>
    <author>
      <name>Jeffery Kinnison</name>
    </author>
    <author>
      <name>Toan Q. Nguyen</name>
    </author>
    <author>
      <name>Walter Scheirer</name>
    </author>
    <author>
      <name>David Chiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 3rd Workshop on Neural Generation and Translation (WNGT 2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00486v2</id>
    <updated>2019-11-18T13:46:38Z</updated>
    <published>2019-10-01T15:36:27Z</published>
    <title>Dialogue Transformers</title>
    <summary>  We introduce a dialogue policy based on a transformer architecture, where the
self-attention mechanism operates over the sequence of dialogue turns. Recent
work has used hierarchical recurrent neural networks to encode multiple
utterances in a dialogue context, but we argue that a pure self-attention
mechanism is more suitable. By default, an RNN assumes that every item in a
sequence is relevant for producing an encoding of the full sequence, but a
single conversation can consist of multiple overlapping discourse segments as
speakers interleave multiple topics. A transformer picks which turns to include
in its encoding of the current dialogue state, and is naturally suited to
selectively ignoring or attending to dialogue history. We compare the
performance of the Transformer Embedding Dialogue (TED) policy to an LSTM and
to the REDP, which was specifically designed to overcome this limitation of
RNNs.
</summary>
    <author>
      <name>Vladimir Vlasov</name>
    </author>
    <author>
      <name>Johannes E. M. Mosig</name>
    </author>
    <author>
      <name>Alan Nichol</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.00486v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00486v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.12744v1</id>
    <updated>2019-09-27T15:23:17Z</updated>
    <published>2019-09-27T15:23:17Z</published>
    <title>On the use of BERT for Neural Machine Translation</title>
    <summary>  Exploiting large pretrained models for various NMT tasks have gained a lot of
visibility recently. In this work we study how BERT pretrained models could be
exploited for supervised Neural Machine Translation. We compare various ways to
integrate pretrained BERT model with NMT model and study the impact of the
monolingual data used for BERT training on the final translation quality. We
use WMT-14 English-German, IWSLT15 English-German and IWSLT14 English-Russian
datasets for these experiments. In addition to standard task test set
evaluation, we perform evaluation on out-of-domain test sets and noise injected
test sets, in order to assess how BERT pretrained representations affect model
robustness.
</summary>
    <author>
      <name>St√©phane Clinchant</name>
    </author>
    <author>
      <name>Kweon Woo Jung</name>
    </author>
    <author>
      <name>Vassilina Nikoulina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.12744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.12744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.11942v6</id>
    <updated>2020-02-09T03:00:18Z</updated>
    <published>2019-09-26T07:06:13Z</published>
    <title>ALBERT: A Lite BERT for Self-supervised Learning of Language
  Representations</title>
    <summary>  Increasing model size when pretraining natural language representations often
results in improved performance on downstream tasks. However, at some point
further model increases become harder due to GPU/TPU memory limitations and
longer training times. To address these problems, we present two
parameter-reduction techniques to lower memory consumption and increase the
training speed of BERT. Comprehensive empirical evidence shows that our
proposed methods lead to models that scale much better compared to the original
BERT. We also use a self-supervised loss that focuses on modeling
inter-sentence coherence, and show it consistently helps downstream tasks with
multi-sentence inputs. As a result, our best model establishes new
state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having
fewer parameters compared to BERT-large. The code and the pretrained models are
available at https://github.com/google-research/ALBERT.
</summary>
    <author>
      <name>Zhenzhong Lan</name>
    </author>
    <author>
      <name>Mingda Chen</name>
    </author>
    <author>
      <name>Sebastian Goodman</name>
    </author>
    <author>
      <name>Kevin Gimpel</name>
    </author>
    <author>
      <name>Piyush Sharma</name>
    </author>
    <author>
      <name>Radu Soricut</name>
    </author>
    <link href="http://arxiv.org/abs/1909.11942v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.11942v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.11898v1</id>
    <updated>2019-09-26T05:25:10Z</updated>
    <published>2019-09-26T05:25:10Z</published>
    <title>Fine-tune Bert for DocRED with Two-step Process</title>
    <summary>  Modelling relations between multiple entities has attracted increasing
attention recently, and a new dataset called DocRED has been collected in order
to accelerate the research on the document-level relation extraction. Current
baselines for this task uses BiLSTM to encode the whole document and are
trained from scratch. We argue that such simple baselines are not strong enough
to model to complex interaction between entities. In this paper, we further
apply a pre-trained language model (BERT) to provide a stronger baseline for
this task. We also find that solving this task in phases can further improve
the performance. The first step is to predict whether or not two entities have
a relation, the second step is to predict the specific relation.
</summary>
    <author>
      <name>Hong Wang</name>
    </author>
    <author>
      <name>Christfried Focke</name>
    </author>
    <author>
      <name>Rob Sylvester</name>
    </author>
    <author>
      <name>Nilesh Mishra</name>
    </author>
    <author>
      <name>William Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1909.11898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.11898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.11556v1</id>
    <updated>2019-09-25T15:35:03Z</updated>
    <published>2019-09-25T15:35:03Z</published>
    <title>Reducing Transformer Depth on Demand with Structured Dropout</title>
    <summary>  Overparameterized transformer networks have obtained state of the art results
in various natural language processing tasks, such as machine translation,
language modeling, and question answering. These models contain hundreds of
millions of parameters, necessitating a large amount of computation and making
them prone to overfitting. In this work, we explore LayerDrop, a form of
structured dropout, which has a regularization effect during training and
allows for efficient pruning at inference time. In particular, we show that it
is possible to select sub-networks of any depth from one large network without
having to finetune them and with limited impact on performance. We demonstrate
the effectiveness of our approach by improving the state of the art on machine
translation, language modeling, summarization, question answering, and language
understanding benchmarks. Moreover, we show that our approach leads to small
BERT-like models of higher quality compared to training from scratch or using
distillation.
</summary>
    <author>
      <name>Angela Fan</name>
    </author>
    <author>
      <name>Edouard Grave</name>
    </author>
    <author>
      <name>Armand Joulin</name>
    </author>
    <link href="http://arxiv.org/abs/1909.11556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.11556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.10681v2</id>
    <updated>2019-10-01T14:00:22Z</updated>
    <published>2019-09-24T02:08:29Z</published>
    <title>Knowledge-Enriched Transformer for Emotion Detection in Textual
  Conversations</title>
    <summary>  Messages in human conversations inherently convey emotions. The task of
detecting emotions in textual conversations leads to a wide range of
applications such as opinion mining in social networks. However, enabling
machines to analyze emotions in conversations is challenging, partly because
humans often rely on the context and commonsense knowledge to express emotions.
In this paper, we address these challenges by proposing a Knowledge-Enriched
Transformer (KET), where contextual utterances are interpreted using
hierarchical self-attention and external commonsense knowledge is dynamically
leveraged using a context-aware affective graph attention mechanism.
Experiments on multiple textual conversation datasets demonstrate that both
context and commonsense knowledge are consistently beneficial to the emotion
detection performance. In addition, the experimental results show that our KET
model outperforms the state-of-the-art models on most of the tested datasets in
F1 score.
</summary>
    <author>
      <name>Peixiang Zhong</name>
    </author>
    <author>
      <name>Di Wang</name>
    </author>
    <author>
      <name>Chunyan Miao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.10681v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.10681v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.10649v2</id>
    <updated>2020-02-27T15:06:49Z</updated>
    <published>2019-09-23T23:21:42Z</published>
    <title>Portuguese Named Entity Recognition using BERT-CRF</title>
    <summary>  Recent advances in language representation using neural networks have made it
viable to transfer the learned internal states of a trained model to downstream
natural language processing tasks, such as named entity recognition (NER) and
question answering. It has been shown that the leverage of pre-trained language
models improves the overall performance on many tasks and is highly beneficial
when labeled data is scarce. In this work, we train Portuguese BERT models and
employ a BERT-CRF architecture to the NER task on the Portuguese language,
combining the transfer capabilities of BERT with the structured predictions of
CRF. We explore feature-based and fine-tuning training strategies for the BERT
model. Our fine-tuning approach obtains new state-of-the-art results on the
HAREM I dataset, improving the F1-score by 1 point on the selective scenario (5
NE classes) and by 4 points on the total scenario (10 NE classes).
</summary>
    <author>
      <name>F√°bio Souza</name>
    </author>
    <author>
      <name>Rodrigo Nogueira</name>
    </author>
    <author>
      <name>Roberto Lotufo</name>
    </author>
    <link href="http://arxiv.org/abs/1909.10649v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.10649v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.10430v2</id>
    <updated>2019-10-01T13:26:04Z</updated>
    <published>2019-09-23T15:38:02Z</published>
    <title>Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with
  Contextualized Embeddings</title>
    <summary>  Contextualized word embeddings (CWE) such as provided by ELMo (Peters et al.,
2018), Flair NLP (Akbik et al., 2018), or BERT (Devlin et al., 2019) are a
major recent innovation in NLP. CWEs provide semantic vector representations of
words depending on their respective context. Their advantage over static word
embeddings has been shown for a number of tasks, such as text classification,
sequence tagging, or machine translation. Since vectors of the same word type
can vary depending on the respective context, they implicitly provide a model
for word sense disambiguation (WSD). We introduce a simple but effective
approach to WSD using a nearest neighbor classification on CWEs. We compare the
performance of different CWE models for the task and can report improvements
above the current state of the art for two standard WSD benchmark datasets. We
further show that the pre-trained BERT model is able to place polysemic words
into distinct 'sense' regions of the embedding space, while ELMo and Flair NLP
do not seem to possess this ability.
</summary>
    <author>
      <name>Gregor Wiedemann</name>
    </author>
    <author>
      <name>Steffen Remus</name>
    </author>
    <author>
      <name>Avi Chawla</name>
    </author>
    <author>
      <name>Chris Biemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures, 6 tables, Accepted for Konferenz zur
  Verarbeitung nat\"urlicher Sprache / Conference on Natural Language
  Processing (KONVENS) 2019, Erlangen/Germany</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.10430v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.10430v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.10351v4</id>
    <updated>2019-12-04T01:50:34Z</updated>
    <published>2019-09-23T13:05:35Z</published>
    <title>TinyBERT: Distilling BERT for Natural Language Understanding</title>
    <summary>  Language model pre-training, such as BERT, has significantly improved the
performances of many natural language processing tasks. However, pre-trained
language models are usually computationally expensive and memory intensive, so
it is difficult to effectively execute them on some resource-restricted
devices. To accelerate inference and reduce model size while maintaining
accuracy, we firstly propose a novel transformer distillation method that is a
specially designed knowledge distillation (KD) method for transformer-based
models. By leveraging this new KD method, the plenty of knowledge encoded in a
large teacher BERT can be well transferred to a small student TinyBERT.
Moreover, we introduce a new two-stage learning framework for TinyBERT, which
performs transformer distillation at both the pre-training and task-specific
learning stages. This framework ensures that TinyBERT can capture both the
general-domain and task-specific knowledge of the teacher BERT.TinyBERT is
empirically effective and achieves more than 96% the performance of teacher
BERTBASE on GLUE benchmark while being 7.5x smaller and 9.4x faster on
inference. TinyBERT is also significantly better than state-of-the-art
baselines on BERT distillation, with only about 28% parameters and about 31%
inference time of them.
</summary>
    <author>
      <name>Xiaoqi Jiao</name>
    </author>
    <author>
      <name>Yichun Yin</name>
    </author>
    <author>
      <name>Lifeng Shang</name>
    </author>
    <author>
      <name>Xin Jiang</name>
    </author>
    <author>
      <name>Xiao Chen</name>
    </author>
    <author>
      <name>Linlin Li</name>
    </author>
    <author>
      <name>Fang Wang</name>
    </author>
    <author>
      <name>Qun Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">code and model:
  https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT
  ; 13 pages, 2 figures, 9 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.10351v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.10351v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.09292v1</id>
    <updated>2019-09-20T01:53:19Z</updated>
    <published>2019-09-20T01:53:19Z</published>
    <title>BERT Meets Chinese Word Segmentation</title>
    <summary>  Chinese word segmentation (CWS) is a fundamental task for Chinese language
understanding. Recently, neural network-based models have attained superior
performance in solving the in-domain CWS task. Last year, Bidirectional Encoder
Representation from Transformers (BERT), a new language representation model,
has been proposed as a backbone model for many natural language tasks and
redefined the corresponding performance. The excellent performance of BERT
motivates us to apply it to solve the CWS task. By conducting intensive
experiments in the benchmark datasets from the second International Chinese
Word Segmentation Bake-off, we obtain several keen observations. BERT can
slightly improve the performance even when the datasets contain the issue of
labeling inconsistency. When applying sufficiently learned features, Softmax, a
simpler classifier, can attain the same performance as that of a more
complicated classifier, e.g., Conditional Random Field (CRF). The performance
of BERT usually increases as the model size increases. The features extracted
by BERT can be also applied as good candidates for other neural network models.
</summary>
    <author>
      <name>Haiqin Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages; 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.09292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.09292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.08402v1</id>
    <updated>2019-09-18T12:40:39Z</updated>
    <published>2019-09-18T12:40:39Z</published>
    <title>Enriching BERT with Knowledge Graph Embeddings for Document
  Classification</title>
    <summary>  In this paper, we focus on the classification of books using short
descriptive texts (cover blurbs) and additional metadata. Building upon BERT, a
deep neural language model, we demonstrate how to combine text representations
with metadata and knowledge graph embeddings, which encode author information.
Compared to the standard BERT approach we achieve considerably better results
for the classification task. For a more coarse-grained classification using
eight labels we achieve an F1- score of 87.20, while a detailed classification
using 343 labels yields an F1-score of 64.70. We make the source code and
trained models of our experiments publicly available
</summary>
    <author>
      <name>Malte Ostendorff</name>
    </author>
    <author>
      <name>Peter Bourgonje</name>
    </author>
    <author>
      <name>Maria Berger</name>
    </author>
    <author>
      <name>Julian Moreno-Schneider</name>
    </author>
    <author>
      <name>Georg Rehm</name>
    </author>
    <author>
      <name>Bela Gipp</name>
    </author>
    <link href="http://arxiv.org/abs/1909.08402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.08402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.08358v1</id>
    <updated>2019-09-18T11:15:22Z</updated>
    <published>2019-09-18T11:15:22Z</published>
    <title>Using BERT for Word Sense Disambiguation</title>
    <summary>  Word Sense Disambiguation (WSD), which aims to identify the correct sense of
a given polyseme, is a long-standing problem in NLP. In this paper, we propose
to use BERT to extract better polyseme representations for WSD and explore
several ways of combining BERT and the classifier. We also utilize sense
definitions to train a unified classifier for all words, which enables the
model to disambiguate unseen polysemes. Experiments show that our model
achieves the state-of-the-art results on the standard English All-word WSD
evaluation.
</summary>
    <author>
      <name>Jiaju Du</name>
    </author>
    <author>
      <name>Fanchao Qi</name>
    </author>
    <author>
      <name>Maosong Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1909.08358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.08358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.07755v3</id>
    <updated>2019-11-19T16:21:59Z</updated>
    <published>2019-09-17T13:01:12Z</published>
    <title>Span-based Joint Entity and Relation Extraction with Transformer
  Pre-training</title>
    <summary>  We introduce SpERT, an attention model for span-based joint entity and
relation extraction. Our key contribution is a light-weight reasoning on BERT
embeddings, which features entity recognition and filtering, as well as
relation classification with a localized, marker-free context representation.
The model is trained using strong within-sentence negative samples, which are
efficiently extracted in a single BERT pass. These aspects facilitate a search
over all spans in the sentence.
  In ablation studies, we demonstrate the benefits of pre-training, strong
negative sampling and localized context. Our model outperforms prior work by up
to 2.6% F1 score on several datasets for joint entity and relation extraction.
</summary>
    <author>
      <name>Markus Eberts</name>
    </author>
    <author>
      <name>Adrian Ulges</name>
    </author>
    <link href="http://arxiv.org/abs/1909.07755v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.07755v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.06775v1</id>
    <updated>2019-09-15T10:33:17Z</updated>
    <published>2019-09-15T10:33:17Z</published>
    <title>Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing</title>
    <summary>  This paper investigates the problem of learning cross-lingual representations
in a contextual space. We propose Cross-Lingual BERT Transformation (CLBT), a
simple and efficient approach to generate cross-lingual contextualized word
embeddings based on publicly available pre-trained BERT models (Devlin et al.,
2018). In this approach, a linear transformation is learned from contextual
word alignments to align the contextualized embeddings independently trained in
different languages. We demonstrate the effectiveness of this approach on
zero-shot cross-lingual transfer parsing. Experiments show that our embeddings
substantially outperform the previous state-of-the-art that uses static
embeddings. We further compare our approach with XLM (Lample and Conneau,
2019), a recently proposed cross-lingual language model trained with massive
parallel data, and achieve highly competitive results.
</summary>
    <author>
      <name>Yuxuan Wang</name>
    </author>
    <author>
      <name>Wanxiang Che</name>
    </author>
    <author>
      <name>Jiang Guo</name>
    </author>
    <author>
      <name>Yijia Liu</name>
    </author>
    <author>
      <name>Ting Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear at EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.06775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.06695v1</id>
    <updated>2019-09-14T23:21:56Z</updated>
    <published>2019-09-14T23:21:56Z</published>
    <title>Ouroboros: On Accelerating Training of Transformer-Based Language Models</title>
    <summary>  Language models are essential for natural language processing (NLP) tasks,
such as machine translation and text summarization. Remarkable performance has
been demonstrated recently across many NLP domains via a Transformer-based
language model with over a billion parameters, verifying the benefits of model
size. Model parallelism is required if a model is too large to fit in a single
computing device. Current methods for model parallelism either suffer from
backward locking in backpropagation or are not applicable to language models.
We propose the first model-parallel algorithm that speeds the training of
Transformer-based language models. We also prove that our proposed algorithm is
guaranteed to converge to critical points for non-convex problems. Extensive
experiments on Transformer and Transformer-XL language models demonstrate that
the proposed algorithm obtains a much faster speedup beyond data parallelism,
with comparable or better accuracy. Code to reproduce experiments is to be
found at \url{https://github.com/LaraQianYang/Ouroboros}.
</summary>
    <author>
      <name>Qian Yang</name>
    </author>
    <author>
      <name>Zhouyuan Huo</name>
    </author>
    <author>
      <name>Wenlin Wang</name>
    </author>
    <author>
      <name>Heng Huang</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the proceedings of Neural Information Processing Systems
  Conference (2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.06695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.06639v2</id>
    <updated>2019-11-01T20:38:07Z</updated>
    <published>2019-09-14T17:49:37Z</published>
    <title>Tree Transformer: Integrating Tree Structures into Self-Attention</title>
    <summary>  Pre-training Transformer from large-scale raw texts and fine-tuning on the
desired task have achieved state-of-the-art results on diverse NLP tasks.
However, it is unclear what the learned attention captures. The attention
computed by attention heads seems not to match human intuitions about
hierarchical structures. This paper proposes Tree Transformer, which adds an
extra constraint to attention heads of the bidirectional Transformer encoder in
order to encourage the attention heads to follow tree structures. The tree
structures can be automatically induced from raw texts by our proposed
"Constituent Attention" module, which is simply implemented by self-attention
between two adjacent words. With the same training procedure identical to BERT,
the experiments demonstrate the effectiveness of Tree Transformer in terms of
inducing tree structures, better language modeling, and further learning more
explainable attention scores.
</summary>
    <author>
      <name>Yau-Shian Wang</name>
    </author>
    <author>
      <name>Hung-Yi Lee</name>
    </author>
    <author>
      <name>Yun-Nung Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.06639v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06639v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.06317v2</id>
    <updated>2019-09-28T11:11:38Z</updated>
    <published>2019-09-13T16:27:08Z</published>
    <title>A Comparative Study on Transformer vs RNN in Speech Applications</title>
    <summary>  Sequence-to-sequence models have been widely used in end-to-end speech
processing, for example, automatic speech recognition (ASR), speech translation
(ST), and text-to-speech (TTS). This paper focuses on an emergent
sequence-to-sequence model called Transformer, which achieves state-of-the-art
performance in neural machine translation and other natural language processing
applications. We undertook intensive studies in which we experimentally
compared and analyzed Transformer and conventional recurrent neural networks
(RNN) in a total of 15 ASR, one multilingual ASR, one ST, and two TTS
benchmarks. Our experiments revealed various training tips and significant
performance benefits obtained with Transformer for each task including the
surprising superiority of Transformer in 13/15 ASR benchmarks in comparison
with RNN. We are preparing to release Kaldi-style reproducible recipes using
open source and publicly available datasets for all the ASR, ST, and TTS tasks
for the community to succeed our exciting outcomes.
</summary>
    <author>
      <name>Shigeki Karita</name>
    </author>
    <author>
      <name>Nanxin Chen</name>
    </author>
    <author>
      <name>Tomoki Hayashi</name>
    </author>
    <author>
      <name>Takaaki Hori</name>
    </author>
    <author>
      <name>Hirofumi Inaguma</name>
    </author>
    <author>
      <name>Ziyan Jiang</name>
    </author>
    <author>
      <name>Masao Someki</name>
    </author>
    <author>
      <name>Nelson Enrique Yalta Soplin</name>
    </author>
    <author>
      <name>Ryuichi Yamamoto</name>
    </author>
    <author>
      <name>Xiaofei Wang</name>
    </author>
    <author>
      <name>Shinji Watanabe</name>
    </author>
    <author>
      <name>Takenori Yoshimura</name>
    </author>
    <author>
      <name>Wangyou Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ASRU 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Automatic Speech Recognition and Understanding Workshop 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1909.06317v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06317v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.05840v2</id>
    <updated>2019-09-25T00:13:30Z</updated>
    <published>2019-09-12T17:45:59Z</published>
    <title>Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT</title>
    <summary>  Transformer based architectures have become de-facto models used for a range
of Natural Language Processing tasks. In particular, the BERT based models
achieved significant accuracy gain for GLUE tasks, CoNLL-03 and SQuAD. However,
BERT based models have a prohibitive memory footprint and latency. As a result,
deploying BERT based models in resource constrained environments has become a
challenging task. In this work, we perform an extensive analysis of fine-tuned
BERT models using second order Hessian information, and we use our results to
propose a novel method for quantizing BERT models to ultra low precision. In
particular, we propose a new group-wise quantization scheme, and we use a
Hessian based mix-precision method to compress the model further. We
extensively test our proposed method on BERT downstream tasks of SST-2, MNLI,
CoNLL-03, and SQuAD. We can achieve comparable performance to baseline with at
most $2.3\%$ performance degradation, even with ultra-low precision
quantization down to 2 bits, corresponding up to $13\times$ compression of the
model parameters, and up to $4\times$ compression of the embedding table as
well as activations. Among all tasks, we observed the highest performance loss
for BERT fine-tuned on SQuAD. By probing into the Hessian based analysis as
well as visualization, we show that this is related to the fact that current
training/fine-tuning strategy of BERT does not converge for SQuAD.
</summary>
    <author>
      <name>Sheng Shen</name>
    </author>
    <author>
      <name>Zhen Dong</name>
    </author>
    <author>
      <name>Jiayu Ye</name>
    </author>
    <author>
      <name>Linjian Ma</name>
    </author>
    <author>
      <name>Zhewei Yao</name>
    </author>
    <author>
      <name>Amir Gholami</name>
    </author>
    <author>
      <name>Michael W. Mahoney</name>
    </author>
    <author>
      <name>Kurt Keutzer</name>
    </author>
    <link href="http://arxiv.org/abs/1909.05840v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.05840v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.05858v2</id>
    <updated>2019-09-20T20:08:56Z</updated>
    <published>2019-09-11T17:57:18Z</published>
    <title>CTRL: A Conditional Transformer Language Model for Controllable
  Generation</title>
    <summary>  Large-scale language models show promising text generation capabilities, but
users cannot easily control particular aspects of the generated text. We
release CTRL, a 1.63 billion-parameter conditional transformer language model,
trained to condition on control codes that govern style, content, and
task-specific behavior. Control codes were derived from structure that
naturally co-occurs with raw text, preserving the advantages of unsupervised
learning while providing more explicit control over text generation. These
codes also allow CTRL to predict which parts of the training data are most
likely given a sequence. This provides a potential method for analyzing large
amounts of data via model-based source attribution. We have released multiple
full-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.
</summary>
    <author>
      <name>Nitish Shirish Keskar</name>
    </author>
    <author>
      <name>Bryan McCann</name>
    </author>
    <author>
      <name>Lav R. Varshney</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <link href="http://arxiv.org/abs/1909.05858v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.05858v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.04925v1</id>
    <updated>2019-09-11T08:55:09Z</updated>
    <published>2019-09-11T08:55:09Z</published>
    <title>How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer
  Representations</title>
    <summary>  Bidirectional Encoder Representations from Transformers (BERT) reach
state-of-the-art results in a variety of Natural Language Processing tasks.
However, understanding of their internal functioning is still insufficient and
unsatisfactory. In order to better understand BERT and other Transformer-based
models, we present a layer-wise analysis of BERT's hidden states. Unlike
previous research, which mainly focuses on explaining Transformer models by
their attention weights, we argue that hidden states contain equally valuable
information. Specifically, our analysis focuses on models fine-tuned on the
task of Question Answering (QA) as an example of a complex downstream task. We
inspect how QA models transform token vectors in order to find the correct
answer. To this end, we apply a set of general and QA-specific probing tasks
that reveal the information stored in each representation layer. Our
qualitative analysis of hidden state visualizations provides additional
insights into BERT's reasoning process. Our results show that the
transformations within BERT go through phases that are related to traditional
pipeline tasks. The system can therefore implicitly incorporate task-specific
information into its token representations. Furthermore, our analysis reveals
that fine-tuning has little impact on the models' semantic abilities and that
prediction errors can be recognized in the vector representations of even early
layers.
</summary>
    <author>
      <name>Betty van Aken</name>
    </author>
    <author>
      <name>Benjamin Winter</name>
    </author>
    <author>
      <name>Alexander L√∂ser</name>
    </author>
    <author>
      <name>Felix A. Gers</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3357384.3358028</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3357384.3358028" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CIKM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.04925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.04925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.04181v3</id>
    <updated>2019-10-31T06:54:50Z</updated>
    <published>2019-09-09T22:08:12Z</published>
    <title>BERT-Based Arabic Social Media Author Profiling</title>
    <summary>  We report our models for detecting age, language variety, and gender from
social media data in the context of the Arabic author profiling and deception
detection shared task (APDA). We build simple models based on pre-trained
bidirectional encoders from transformers (BERT). We first fine-tune the
pre-trained BERT model on each of the three datasets with shared task released
data. Then we augment shared task data with in-house data for gender and
dialect, showing the utility of augmenting training data. Our best models on
the shared task test data are acquired with a majority voting of various BERT
models trained under different data conditions. We acquire 54.72% accuracy for
age, 93.75% for dialect, 81.67% for gender, and 40.97% joint accuracy across
the three tasks.
</summary>
    <author>
      <name>Chiyu Zhang</name>
    </author>
    <author>
      <name>Muhammad Abdul-Mageed</name>
    </author>
    <link href="http://arxiv.org/abs/1909.04181v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.04181v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.05017v2</id>
    <updated>2019-09-14T20:02:20Z</updated>
    <published>2019-09-09T19:48:53Z</published>
    <title>Question Generation by Transformers</title>
    <summary>  A machine learning model was developed to automatically generate questions
from Wikipedia passages using transformers, an attention-based model eschewing
the paradigm of existing recurrent neural networks (RNNs). The model was
trained on the inverted Stanford Question Answering Dataset (SQuAD), which is a
reading comprehension dataset consisting of 100,000+ questions posed by
crowdworkers on a set of Wikipedia articles. After training, the question
generation model is able to generate simple questions relevant to unseen
passages and answers containing an average of 8 words per question. The word
error rate (WER) was used as a metric to compare the similarity between SQuAD
questions and the model-generated questions. Although the high average WER
suggests that the questions generated differ from the original SQuAD questions,
the questions generated are mostly grammatically correct and plausible in their
own right.
</summary>
    <author>
      <name>Kettip Kriangchaivech</name>
    </author>
    <author>
      <name>Artit Wangperawong</name>
    </author>
    <link href="http://arxiv.org/abs/1909.05017v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.05017v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03526v3</id>
    <updated>2019-10-31T06:57:28Z</updated>
    <published>2019-09-08T18:31:42Z</published>
    <title>Multi-Task Bidirectional Transformer Representations for Irony Detection</title>
    <summary>  Supervised deep learning requires large amounts of training data. In the
context of the FIRE2019 Arabic irony detection shared task (IDAT@FIRE2019), we
show how we mitigate this need by fine-tuning the pre-trained bidirectional
encoders from transformers (BERT) on gold data in a multi-task setting. We
further improve our models by by further pre-training BERT on `in-domain' data,
thus alleviating an issue of dialect mismatch in the Google-released BERT
model. Our best model acquires 82.4 macro F1 score, and has the unique
advantage of being feature-engineering free (i.e., based exclusively on deep
learning).
</summary>
    <author>
      <name>Chiyu Zhang</name>
    </author>
    <author>
      <name>Muhammad Abdul-Mageed</name>
    </author>
    <link href="http://arxiv.org/abs/1909.03526v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03526v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03508v1</id>
    <updated>2019-09-08T16:57:26Z</updated>
    <published>2019-09-08T16:57:26Z</published>
    <title>Transformer to CNN: Label-scarce distillation for efficient text
  classification</title>
    <summary>  Significant advances have been made in Natural Language Processing (NLP)
modelling since the beginning of 2018. The new approaches allow for accurate
results, even when there is little labelled data, because these NLP models can
benefit from training on both task-agnostic and task-specific unlabelled data.
However, these advantages come with significant size and computational costs.
This workshop paper outlines how our proposed convolutional student
architecture, having been trained by a distillation process from a large-scale
model, can achieve 300x inference speedup and 39x reduction in parameter count.
In some cases, the student model performance surpasses its teacher on the
studied tasks.
</summary>
    <author>
      <name>Yew Ken Chia</name>
    </author>
    <author>
      <name>Sam Witteveen</name>
    </author>
    <author>
      <name>Martin Andrews</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted paper for CDNNRIA workshop at NeurIPS 2018. (3 pages +
  references)</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.03508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03415v1</id>
    <updated>2019-09-08T09:47:56Z</updated>
    <published>2019-09-08T09:47:56Z</published>
    <title>Commonsense Knowledge + BERT for Level 2 Reading Comprehension Ability
  Test</title>
    <summary>  Commonsense knowledge plays an important role when we read. The performance
of BERT on SQuAD dataset shows that the accuracy of BERT can be better than
human users. However, it does not mean that computers can surpass the human
being in reading comprehension. CommonsenseQA is a large-scale dataset which is
designed based on commonsense knowledge. BERT only achieved an accuracy of
55.9% on it. The result shows that computers cannot apply commonsense knowledge
like human beings to answer questions. Comprehension Ability Test (CAT) divided
the reading comprehension ability at four levels. We can achieve human like
comprehension ability level by level. BERT has performed well at level 1 which
does not require common knowledge. In this research, we propose a system which
aims to allow computers to read articles and answer related questions with
commonsense knowledge like a human being for CAT level 2. This system consists
of three parts. Firstly, we built a commonsense knowledge graph; and then
automatically constructed the commonsense knowledge question dataset according
to it. Finally, BERT is combined with the commonsense knowledge to achieve the
reading comprehension ability at CAT level 2. Experiments show that it can pass
the CAT as long as the required common knowledge is included in the knowledge
base.
</summary>
    <author>
      <name>Yidan Hu</name>
    </author>
    <author>
      <name>Gongqi Lin</name>
    </author>
    <author>
      <name>Yuan Miao</name>
    </author>
    <author>
      <name>Chunyan Miao</name>
    </author>
    <link href="http://arxiv.org/abs/1909.03415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03405v1</id>
    <updated>2019-09-08T08:55:09Z</updated>
    <published>2019-09-08T08:55:09Z</published>
    <title>Symmetric Regularization based BERT for Pair-wise Semantic Reasoning</title>
    <summary>  The ability of semantic reasoning over the sentence pair is essential for
many natural language understanding tasks, e.g., natural language inference and
machine reading comprehension. A recent significant improvement in these tasks
comes from BERT. As reported, the next sentence prediction (NSP) in BERT, which
learns the contextual relationship between two sentences, is of great
significance for downstream problems with sentence-pair input. Despite the
effectiveness of NSP, we suggest that NSP still lacks the essential signal to
distinguish between entailment and shallow correlation. To remedy this, we
propose to augment the NSP task to a 3-class categorization task, which
includes a category for previous sentence prediction (PSP). The involvement of
PSP encourages the model to focus on the informative semantics to determine the
sentence order, thereby improves the ability of semantic understanding. This
simple modification yields remarkable improvement against vanilla BERT. To
further incorporate the document-level information, the scope of NSP and PSP is
expanded into a broader range, i.e., NSP and PSP also include close but
nonsuccessive sentences, the noise of which is mitigated by the label-smoothing
technique. Both qualitative and quantitative experimental results demonstrate
the effectiveness of the proposed method. Our method consistently improves the
performance on the NLI and MRC benchmarks, including the challenging HANS
dataset~\cite{hans}, suggesting that the document-level task is still promising
for the pre-training.
</summary>
    <author>
      <name>Xingyi Cheng</name>
    </author>
    <author>
      <name>Weidi Xu</name>
    </author>
    <author>
      <name>Kunlong Chen</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Bin Bi</name>
    </author>
    <author>
      <name>Ming Yan</name>
    </author>
    <author>
      <name>Chen Wu</name>
    </author>
    <author>
      <name>Luo Si</name>
    </author>
    <author>
      <name>Wei Chu</name>
    </author>
    <author>
      <name>Taifeng Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.03405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03223v1</id>
    <updated>2019-09-07T09:14:18Z</updated>
    <published>2019-09-07T09:14:18Z</published>
    <title>Deleter: Leveraging BERT to Perform Unsupervised Successive Text
  Compression</title>
    <summary>  Text compression has diverse applications such as Summarization, Reading
Comprehension and Text Editing. However, almost all existing approaches require
either hand-crafted features, syntactic labels or parallel data. Even for one
that achieves this task in an unsupervised setting, its architecture
necessitates a task-specific autoencoder. Moreover, these models only generate
one compressed sentence for each source input, so that adapting to different
style requirements (e.g. length) for the final output usually implies
retraining the model from scratch. In this work, we propose a fully
unsupervised model, Deleter, that is able to discover an "optimal deletion
path" for an arbitrary sentence, where each intermediate sequence along the
path is a coherent subsequence of the previous one. This approach relies
exclusively on a pretrained bidirectional language model (BERT) to score each
candidate deletion based on the average Perplexity of the resulting sentence
and performs progressive greedy lookahead search to select the best deletion
for each step. We apply Deleter to the task of extractive Sentence Compression,
and found that our model is competitive with state-of-the-art supervised models
trained on 1.02 million in-domain examples with similar compression ratio.
Qualitative analysis, as well as automatic and human evaluations both verify
that our model produces high-quality compression.
</summary>
    <author>
      <name>Tong Niu</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure (presented @ WeCNLP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.03223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03193v2</id>
    <updated>2019-09-11T06:03:30Z</updated>
    <published>2019-09-07T06:09:25Z</published>
    <title>KG-BERT: BERT for Knowledge Graph Completion</title>
    <summary>  Knowledge graphs are important resources for many artificial intelligence
tasks but often suffer from incompleteness. In this work, we propose to use
pre-trained language models for knowledge graph completion. We treat triples in
knowledge graphs as textual sequences and propose a novel framework named
Knowledge Graph Bidirectional Encoder Representations from Transformer
(KG-BERT) to model these triples. Our method takes entity and relation
descriptions of a triple as input and computes scoring function of the triple
with the KG-BERT language model. Experimental results on multiple benchmark
knowledge graphs show that our method can achieve state-of-the-art performance
in triple classification, link prediction and relation prediction tasks.
</summary>
    <author>
      <name>Liang Yao</name>
    </author>
    <author>
      <name>Chengsheng Mao</name>
    </author>
    <author>
      <name>Yuan Luo</name>
    </author>
    <link href="http://arxiv.org/abs/1909.03193v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03193v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03186v1</id>
    <updated>2019-09-07T04:33:26Z</updated>
    <published>2019-09-07T04:33:26Z</published>
    <title>On Extractive and Abstractive Neural Document Summarization with
  Transformer Language Models</title>
    <summary>  We present a method to produce abstractive summaries of long documents that
exceed several thousand words via neural abstractive summarization. We perform
a simple extractive step before generating a summary, which is then used to
condition the transformer language model on relevant information before being
tasked with generating a summary. We show that this extractive step
significantly improves summarization results. We also show that this approach
produces more abstractive summaries compared to prior work that employs a copy
mechanism while still achieving higher rouge scores. Note: The abstract above
was not written by the authors, it was generated by one of the models presented
in this paper.
</summary>
    <author>
      <name>Sandeep Subramanian</name>
    </author>
    <author>
      <name>Raymond Li</name>
    </author>
    <author>
      <name>Jonathan Pilault</name>
    </author>
    <author>
      <name>Christopher Pal</name>
    </author>
    <link href="http://arxiv.org/abs/1909.03186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03104v2</id>
    <updated>2020-01-08T22:32:19Z</updated>
    <published>2019-09-06T19:44:48Z</published>
    <title>Efficient Sentence Embedding using Discrete Cosine Transform</title>
    <summary>  Vector averaging remains one of the most popular sentence embedding methods
in spite of its obvious disregard for syntactic structure. While more complex
sequential or convolutional networks potentially yield superior classification
performance, the improvements in classification accuracy are typically mediocre
compared to the simple vector averaging. As an efficient alternative, we
propose the use of discrete cosine transform (DCT) to compress word sequences
in an order-preserving manner. The lower order DCT coefficients represent the
overall feature patterns in sentences, which results in suitable embeddings for
tasks that could benefit from syntactic features. Our results in semantic
probing tasks demonstrate that DCT embeddings indeed preserve more syntactic
information compared with vector averaging. With practically equivalent
complexity, the model yields better overall performance in downstream
classification tasks that correlate with syntactic features, which illustrates
the capacity of DCT to preserve word order information.
</summary>
    <author>
      <name>Nada Almarwani</name>
    </author>
    <author>
      <name>Hanan Aldarmaki</name>
    </author>
    <author>
      <name>Mona Diab</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in EMNLP 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1909.03104v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03104v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02635v1</id>
    <updated>2019-09-05T21:13:37Z</updated>
    <published>2019-09-05T21:13:37Z</published>
    <title>Effective Use of Transformer Networks for Entity Tracking</title>
    <summary>  Tracking entities in procedural language requires understanding the
transformations arising from actions on entities as well as those entities'
interactions. While self-attention-based pre-trained language encoders like GPT
and BERT have been successfully applied across a range of natural language
understanding tasks, their ability to handle the nuances of procedural texts is
still untested. In this paper, we explore the use of pre-trained transformer
networks for entity tracking tasks in procedural text. First, we test standard
lightweight approaches for prediction with pre-trained transformers, and find
that these approaches underperform even simple baselines. We show that much
stronger results can be attained by restructuring the input to guide the
transformer model to focus on a particular entity. Second, we assess the degree
to which transformer networks capture the process dynamics, investigating such
factors as merged entities and oblique entity references. On two different
tasks, ingredient detection in recipes and QA over scientific processes, we
achieve state-of-the-art results, but our models still largely attend to
shallow context clues and do not form complex representations of intermediate
entity or process state.
</summary>
    <author>
      <name>Aditya Gupta</name>
    </author>
    <author>
      <name>Greg Durrett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.02635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02597v2</id>
    <updated>2019-09-19T18:13:06Z</updated>
    <published>2019-09-05T18:58:51Z</published>
    <title>Investigating BERT's Knowledge of Language: Five Analysis Methods with
  NPIs</title>
    <summary>  Though state-of-the-art sentence representation models can perform tasks
requiring significant knowledge of grammar, it is an open question how best to
evaluate their grammatical knowledge. We explore five experimental methods
inspired by prior work evaluating pretrained sentence representation models. We
use a single linguistic phenomenon, negative polarity item (NPI) licensing in
English, as a case study for our experiments. NPIs like "any" are grammatical
only if they appear in a licensing environment like negation ("Sue doesn't have
any cats" vs. "Sue has any cats"). This phenomenon is challenging because of
the variety of NPI licensing environments that exist. We introduce an
artificially generated dataset that manipulates key features of NPI licensing
for the experiments. We find that BERT has significant knowledge of these
features, but its success varies widely across different experimental methods.
We conclude that a variety of methods is necessary to reveal all relevant
aspects of a model's grammatical knowledge in a given domain.
</summary>
    <author>
      <name>Alex Warstadt</name>
    </author>
    <author>
      <name>Yu Cao</name>
    </author>
    <author>
      <name>Ioana Grosu</name>
    </author>
    <author>
      <name>Wei Peng</name>
    </author>
    <author>
      <name>Hagen Blix</name>
    </author>
    <author>
      <name>Yining Nie</name>
    </author>
    <author>
      <name>Anna Alsop</name>
    </author>
    <author>
      <name>Shikha Bordia</name>
    </author>
    <author>
      <name>Haokun Liu</name>
    </author>
    <author>
      <name>Alicia Parrish</name>
    </author>
    <author>
      <name>Sheng-Fu Wang</name>
    </author>
    <author>
      <name>Jason Phang</name>
    </author>
    <author>
      <name>Anhad Mohananey</name>
    </author>
    <author>
      <name>Phu Mon Htut</name>
    </author>
    <author>
      <name>Paloma Jeretiƒç</name>
    </author>
    <author>
      <name>Samuel R. Bowman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2019; Added link to code+dataset</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.02597v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02597v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02279v1</id>
    <updated>2019-09-05T09:22:25Z</updated>
    <published>2019-09-05T09:22:25Z</published>
    <title>Accelerating Transformer Decoding via a Hybrid of Self-attention and
  Recurrent Neural Network</title>
    <summary>  Due to the highly parallelizable architecture, Transformer is faster to train
than RNN-based models and popularly used in machine translation tasks. However,
at inference time, each output word requires all the hidden states of the
previously generated words, which limits the parallelization capability, and
makes it much slower than RNN-based ones. In this paper, we systematically
analyze the time cost of different components of both the Transformer and
RNN-based model. Based on it, we propose a hybrid network of self-attention and
RNN structures, in which, the highly parallelizable self-attention is utilized
as the encoder, and the simpler RNN structure is used as the decoder. Our
hybrid network can decode 4-times faster than the Transformer. In addition,
with the help of knowledge distillation, our hybrid network achieves comparable
translation quality to the original Transformer.
</summary>
    <author>
      <name>Chengyi Wang</name>
    </author>
    <author>
      <name>Shuangzhi Wu</name>
    </author>
    <author>
      <name>Shujie Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1909.02279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02273v1</id>
    <updated>2019-09-05T09:17:37Z</updated>
    <published>2019-09-05T09:17:37Z</published>
    <title>Source Dependency-Aware Transformer with Supervised Self-Attention</title>
    <summary>  Recently, Transformer has achieved the state-of-the-art performance on many
machine translation tasks. However, without syntax knowledge explicitly
considered in the encoder, incorrect context information that violates the
syntax structure may be integrated into source hidden states, leading to
erroneous translations. In this paper, we propose a novel method to incorporate
source dependencies into the Transformer. Specifically, we adopt the source
dependency tree and define two matrices to represent the dependency relations.
Based on the matrices, two heads in the multi-head self-attention module are
trained in a supervised manner and two extra cross entropy losses are
introduced into the training objective function. Under this training objective,
the model is trained to learn the source dependency relations directly. Without
requiring pre-parsed input during inference, our model can generate better
translations with the dependency-aware context information. Experiments on
bi-directional Chinese-to-English, English-to-Japanese and English-to-German
translation tasks show that our proposed method can significantly improve the
Transformer baseline.
</summary>
    <author>
      <name>Chengyi Wang</name>
    </author>
    <author>
      <name>Shuangzhi Wu</name>
    </author>
    <author>
      <name>Shujie Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.02273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02209v3</id>
    <updated>2020-02-04T09:43:22Z</updated>
    <published>2019-09-05T04:47:10Z</published>
    <title>Semantics-aware BERT for Language Understanding</title>
    <summary>  The latest work on language representations carefully integrates
contextualized features into language model training, which enables a series of
success especially in various machine reading comprehension and natural
language inference tasks. However, the existing language representation models
including ELMo, GPT and BERT only exploit plain context-sensitive features such
as character or word embeddings. They rarely consider incorporating structured
semantic information which can provide rich semantics for language
representation. To promote natural language understanding, we propose to
incorporate explicit contextual semantics from pre-trained semantic role
labeling, and introduce an improved language representation model,
Semantics-aware BERT (SemBERT), which is capable of explicitly absorbing
contextual semantics over a BERT backbone. SemBERT keeps the convenient
usability of its BERT precursor in a light fine-tuning way without substantial
task-specific modifications. Compared with BERT, semantics-aware BERT is as
simple in concept but more powerful. It obtains new state-of-the-art or
substantially improves results on ten reading comprehension and language
inference tasks.
</summary>
    <author>
      <name>Zhuosheng Zhang</name>
    </author>
    <author>
      <name>Yuwei Wu</name>
    </author>
    <author>
      <name>Hai Zhao</name>
    </author>
    <author>
      <name>Zuchao Li</name>
    </author>
    <author>
      <name>Shuailiang Zhang</name>
    </author>
    <author>
      <name>Xi Zhou</name>
    </author>
    <author>
      <name>Xiang Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.02209v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02209v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02074v1</id>
    <updated>2019-09-04T19:54:06Z</updated>
    <published>2019-09-04T19:54:06Z</published>
    <title>Jointly Learning to Align and Translate with Transformer Models</title>
    <summary>  The state of the art in machine translation (MT) is governed by neural
approaches, which typically provide superior translation accuracy over
statistical approaches. However, on the closely related task of word alignment,
traditional statistical word alignment models often remain the go-to solution.
In this paper, we present an approach to train a Transformer model to produce
both accurate translations and alignments. We extract discrete alignments from
the attention probabilities learnt during regular neural machine translation
model training and leverage them in a multi-task framework to optimize towards
translation and alignment objectives. We demonstrate that our approach produces
competitive results compared to GIZA++ trained IBM alignment models without
sacrificing translation accuracy and outperforms previous attempts on
Transformer model based word alignment. Finally, by incorporating IBM model
alignments into our multi-task training, we report significantly better
alignment accuracies compared to GIZA++ on three publicly available data sets.
</summary>
    <author>
      <name>Sarthak Garg</name>
    </author>
    <author>
      <name>Stephan Peitz</name>
    </author>
    <author>
      <name>Udhyakumar Nallasamy</name>
    </author>
    <author>
      <name>Matthias Paulik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures. To appear at EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.02074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.01380v1</id>
    <updated>2019-09-03T18:06:03Z</updated>
    <published>2019-09-03T18:06:03Z</published>
    <title>The Bottom-up Evolution of Representations in the Transformer: A Study
  with Machine Translation and Language Modeling Objectives</title>
    <summary>  We seek to understand how the representations of individual tokens and the
structure of the learned feature space evolve between layers in deep neural
networks under different learning objectives. We focus on the Transformers for
our analysis as they have been shown effective on various tasks, including
machine translation (MT), standard left-to-right language models (LM) and
masked language modeling (MLM). Previous work used black-box probing tasks to
show that the representations learned by the Transformer differ significantly
depending on the objective. In this work, we use canonical correlation analysis
and mutual information estimators to study how information flows across
Transformer layers and how this process depends on the choice of learning
objective. For example, as you go from bottom to top layers, information about
the past in left-to-right language models gets vanished and predictions about
the future get formed. In contrast, for MLM, representations initially acquire
information about the context around the token, partially forgetting the token
identity and producing a more generalized token representation. The token
identity then gets recreated at the top MLM layers.
</summary>
    <author>
      <name>Elena Voita</name>
    </author>
    <author>
      <name>Rico Sennrich</name>
    </author>
    <author>
      <name>Ivan Titov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2019 (camera-ready)</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.01380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.01380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00931v1</id>
    <updated>2019-09-03T03:06:46Z</updated>
    <published>2019-09-03T03:06:46Z</published>
    <title>Transfer Fine-Tuning: A BERT Case Study</title>
    <summary>  A semantic equivalence assessment is defined as a task that assesses semantic
equivalence in a sentence pair by binary judgment (i.e., paraphrase
identification) or grading (i.e., semantic textual similarity measurement). It
constitutes a set of tasks crucial for research on natural language
understanding. Recently, BERT realized a breakthrough in sentence
representation learning (Devlin et al., 2019), which is broadly transferable to
various NLP tasks. While BERT's performance improves by increasing its model
size, the required computational power is an obstacle preventing practical
applications from adopting the technology. Herein, we propose to inject phrasal
paraphrase relations into BERT in order to generate suitable representations
for semantic equivalence assessment instead of increasing the model size.
Experiments on standard natural language understanding tasks confirm that our
method effectively improves a smaller BERT model while maintaining the model
size. The generated model exhibits superior performance compared to a larger
BERT model on semantic equivalence assessment tasks. Furthermore, it achieves
larger performance gains on tasks with limited training datasets for
fine-tuning, which is a property desirable for transfer learning.
</summary>
    <author>
      <name>Yuki Arase</name>
    </author>
    <author>
      <name>Junichi Tsujii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2019) *Retitled from "Injecting Phrasal Paraphrase Relation
  into Sentence Representation for Semantic Equivalence Assessment"</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.00931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00578v1</id>
    <updated>2019-09-02T07:30:53Z</updated>
    <published>2019-09-02T07:30:53Z</published>
    <title>SumQE: a BERT-based Summary Quality Estimation Model</title>
    <summary>  We propose SumQE, a novel Quality Estimation model for summarization based on
BERT. The model addresses linguistic quality aspects that are only indirectly
captured by content-based approaches to summary evaluation, without involving
comparison with human references. SumQE achieves very high correlations with
human ratings, outperforming simpler models addressing these linguistic
aspects. Predictions of the SumQE model can be used for system development, and
to inform users of the quality of automatically produced summaries and other
types of generated text.
</summary>
    <author>
      <name>Stratos Xenouleas</name>
    </author>
    <author>
      <name>Prodromos Malakasiotis</name>
    </author>
    <author>
      <name>Marianna Apidianaki</name>
    </author>
    <author>
      <name>Ion Androutsopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Conference on Empirical Methods in Natural
  Language Processing and the 9th International Joint Conference on Natural
  Language Processing (EMNLP-IJCNLP 2019), Hong Kong, China, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.00578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00512v1</id>
    <updated>2019-09-02T01:51:46Z</updated>
    <published>2019-09-02T01:51:46Z</published>
    <title>How Contextual are Contextualized Word Representations? Comparing the
  Geometry of BERT, ELMo, and GPT-2 Embeddings</title>
    <summary>  Replacing static word embeddings with contextualized word representations has
yielded significant improvements on many NLP tasks. However, just how
contextual are the contextualized representations produced by models such as
ELMo and BERT? Are there infinitely many context-specific representations for
each word, or are words essentially assigned one of a finite number of
word-sense representations? For one, we find that the contextualized
representations of all words are not isotropic in any layer of the
contextualizing model. While representations of the same word in different
contexts still have a greater cosine similarity than those of two different
words, this self-similarity is much lower in upper layers. This suggests that
upper layers of contextualizing models produce more context-specific
representations, much like how upper layers of LSTMs produce more task-specific
representations. In all layers of ELMo, BERT, and GPT-2, on average, less than
5% of the variance in a word's contextualized representations can be explained
by a static embedding for that word, providing some justification for the
success of contextualized representations.
</summary>
    <author>
      <name>Kawin Ethayarajh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.00512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00504v2</id>
    <updated>2019-09-05T17:09:30Z</updated>
    <published>2019-09-02T01:36:33Z</published>
    <title>Rotate King to get Queen: Word Relationships as Orthogonal
  Transformations in Embedding Space</title>
    <summary>  A notable property of word embeddings is that word relationships can exist as
linear substructures in the embedding space. For example, $\textit{gender}$
corresponds to $\vec{\textit{woman}} - \vec{\textit{man}}$ and
$\vec{\textit{queen}} - \vec{\textit{king}}$. This, in turn, allows word
analogies to be solved arithmetically: $\vec{\textit{king}} -
\vec{\textit{man}} + \vec{\textit{woman}} \approx \vec{\textit{queen}}$. This
property is notable because it suggests that models trained on word embeddings
can easily learn such relationships as geometric translations. However, there
is no evidence that models $\textit{exclusively}$ represent relationships in
this manner. We document an alternative way in which downstream models might
learn these relationships: orthogonal and linear transformations. For example,
given a translation vector for $\textit{gender}$, we can find an orthogonal
matrix $R$, representing a rotation and reflection, such that
$R(\vec{\textit{king}}) \approx \vec{\textit{queen}}$ and
$R(\vec{\textit{man}}) \approx \vec{\textit{woman}}$. Analogical reasoning
using orthogonal transformations is almost as accurate as using vector
arithmetic; using linear transformations is more accurate than both. Our
findings suggest that these transformations can be as good a representation of
word relationships as translation vectors.
</summary>
    <author>
      <name>Kawin Ethayarajh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.00504v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00504v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00325v1</id>
    <updated>2019-09-01T05:26:30Z</updated>
    <published>2019-09-01T05:26:30Z</published>
    <title>Repurposing Decoder-Transformer Language Models for Abstractive
  Summarization</title>
    <summary>  Neural network models have shown excellent fluency and performance when
applied to abstractive summarization. Many approaches to neural abstractive
summarization involve the introduction of significant inductive bias,
exemplified through the use of components such as pointer-generator
architectures, coverage, and partially extractive procedures, designed to mimic
the process by which humans summarize documents. We show that it is possible to
attain competitive performance by instead directly viewing summarization as a
language modeling problem and effectively leveraging transfer learning. We
introduce a simple procedure built upon decoder-transformers to obtain highly
competitive ROUGE scores for summarization performance using a language
modeling loss alone, with no beam-search or other decoding-time optimization,
and instead relying on efficient nucleus sampling and greedy decoding.
</summary>
    <author>
      <name>Luke de Oliveira</name>
    </author>
    <author>
      <name>Alfredo L√°inez Rodrigo</name>
    </author>
    <link href="http://arxiv.org/abs/1909.00325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00252v1</id>
    <updated>2019-08-31T18:01:29Z</updated>
    <published>2019-08-31T18:01:29Z</published>
    <title>Humor Detection: A Transformer Gets the Last Laugh</title>
    <summary>  Much previous work has been done in attempting to identify humor in text. In
this paper we extend that capability by proposing a new task: assessing whether
or not a joke is humorous. We present a novel way of approaching this problem
by building a model that learns to identify humorous jokes based on ratings
gleaned from Reddit pages, consisting of almost 16,000 labeled instances. Using
these ratings to determine the level of humor, we then employ a Transformer
architecture for its advantages in learning from sentence context. We
demonstrate the effectiveness of this approach and show results that are
comparable to human performance. We further demonstrate our model's increased
capabilities on humor identification problems, such as the previously created
datasets for short jokes and puns. These experiments show that this method
outperforms all previous work done on these tasks, with an F-measure of 93.1%
for the Puns dataset and 98.6% on the Short Jokes dataset.
</summary>
    <author>
      <name>Orion Weller</name>
    </author>
    <author>
      <name>Kevin Seppi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.00252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00136v1</id>
    <updated>2019-08-31T05:45:20Z</updated>
    <published>2019-08-31T05:45:20Z</published>
    <title>Modeling Graph Structure in Transformer for Better AMR-to-Text
  Generation</title>
    <summary>  Recent studies on AMR-to-text generation often formalize the task as a
sequence-to-sequence (seq2seq) learning problem by converting an Abstract
Meaning Representation (AMR) graph into a word sequence. Graph structures are
further modeled into the seq2seq framework in order to utilize the structural
information in the AMR graphs. However, previous approaches only consider the
relations between directly connected concepts while ignoring the rich structure
in AMR graphs. In this paper we eliminate such a strong limitation and propose
a novel structure-aware self-attention approach to better modeling the
relations between indirectly connected concepts in the state-of-the-art seq2seq
model, i.e., the Transformer. In particular, a few different methods are
explored to learn structural representations between two concepts. Experimental
results on English AMR benchmark datasets show that our approach significantly
outperforms the state of the art with 29.66 and 31.82 BLEU scores on LDC2015E86
and LDC2017T10, respectively. To the best of our knowledge, these are the best
results achieved so far by supervised models on the benchmarks.
</summary>
    <author>
      <name>Jie Zhu</name>
    </author>
    <author>
      <name>Junhui Li</name>
    </author>
    <author>
      <name>Muhua Zhu</name>
    </author>
    <author>
      <name>Longhua Qian</name>
    </author>
    <author>
      <name>Min Zhang</name>
    </author>
    <author>
      <name>Guodong Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.00136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00109v2</id>
    <updated>2019-09-12T21:55:23Z</updated>
    <published>2019-08-31T02:30:24Z</published>
    <title>Giving BERT a Calculator: Finding Operations and Arguments with Reading
  Comprehension</title>
    <summary>  Reading comprehension models have been successfully applied to extractive
text answers, but it is unclear how best to generalize these models to
abstractive numerical answers. We enable a BERT-based reading comprehension
model to perform lightweight numerical reasoning. We augment the model with a
predefined set of executable 'programs' which encompass simple arithmetic as
well as extraction. Rather than having to learn to manipulate numbers directly,
the model can pick a program and execute it. On the recent Discrete Reasoning
Over Passages (DROP) dataset, designed to challenge reading comprehension
models, we show a 33% absolute improvement by adding shallow programs. The
model can learn to predict new operations when appropriate in a math word
problem setting (Roy and Roth, 2015) with very few training examples.
</summary>
    <author>
      <name>Daniel Andor</name>
    </author>
    <author>
      <name>Luheng He</name>
    </author>
    <author>
      <name>Kenton Lee</name>
    </author>
    <author>
      <name>Emily Pitler</name>
    </author>
    <link href="http://arxiv.org/abs/1909.00109v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00109v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00100v1</id>
    <updated>2019-08-31T00:39:12Z</updated>
    <published>2019-08-31T00:39:12Z</published>
    <title>Small and Practical BERT Models for Sequence Labeling</title>
    <summary>  We propose a practical scheme to train a single multilingual sequence
labeling model that yields state of the art results and is small and fast
enough to run on a single CPU. Starting from a public multilingual BERT
checkpoint, our final model is 6x smaller and 27x faster, and has higher
accuracy than a state-of-the-art multilingual baseline. We show that our model
especially outperforms on low-resource languages, and works on codemixed input
text without being explicitly trained on codemixed examples. We showcase the
effectiveness of our method by reporting on part-of-speech tagging and
morphological prediction on 70 treebanks and 48 languages.
</summary>
    <author>
      <name>Henry Tsai</name>
    </author>
    <author>
      <name>Jason Riesa</name>
    </author>
    <author>
      <name>Melvin Johnson</name>
    </author>
    <author>
      <name>Naveen Arivazhagan</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Amelia Archer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages including appendices; accepted to appear at EMNLP-IJCNLP
  2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.00100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00015v2</id>
    <updated>2019-09-06T16:55:20Z</updated>
    <published>2019-08-30T18:06:14Z</published>
    <title>Adaptively Sparse Transformers</title>
    <summary>  Attention mechanisms have become ubiquitous in NLP. Recent architectures,
notably the Transformer, learn powerful context-aware word representations
through layered, multi-headed attention. The multiple heads learn diverse types
of word relationships. However, with standard softmax attention, all attention
heads are dense, assigning a non-zero weight to all context words. In this
work, we introduce the adaptively sparse Transformer, wherein attention heads
have flexible, context-dependent sparsity patterns. This sparsity is
accomplished by replacing softmax with $\alpha$-entmax: a differentiable
generalization of softmax that allows low-scoring words to receive precisely
zero weight. Moreover, we derive a method to automatically learn the $\alpha$
parameter -- which controls the shape and sparsity of $\alpha$-entmax --
allowing attention heads to choose between focused or spread-out behavior. Our
adaptively sparse Transformer improves interpretability and head diversity when
compared to softmax Transformers on machine translation datasets. Findings of
the quantitative and qualitative analysis of our approach include that heads in
different layers learn different sparsity preferences and tend to be more
diverse in their attention distributions than softmax Transformers.
Furthermore, at no cost in accuracy, sparsity in attention heads helps to
uncover different head specializations.
</summary>
    <author>
      <name>Gon√ßalo M. Correia</name>
    </author>
    <author>
      <name>Vlad Niculae</name>
    </author>
    <author>
      <name>Andr√© F. T. Martins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference on Empirical Methods in Natural Language Processing
  (EMNLP), 2019, Hong Kong, China</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.00015v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00015v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.11860v2</id>
    <updated>2019-11-19T10:17:52Z</updated>
    <published>2019-08-30T17:44:30Z</published>
    <title>Adapt or Get Left Behind: Domain Adaptation through BERT Language Model
  Finetuning for Aspect-Target Sentiment Classification</title>
    <summary>  Aspect-Target Sentiment Classification (ATSC) is a subtask of Aspect-Based
Sentiment Analysis (ABSA), which has many applications e.g. in e-commerce,
where data and insights from reviews can be leveraged to create value for
businesses and customers. Recently, deep transfer-learning methods have been
applied successfully to a myriad of Natural Language Processing (NLP) tasks,
including ATSC. Building on top of the prominent BERT language model, we
approach ATSC using a two-step procedure: self-supervised domain-specific BERT
language model finetuning, followed by supervised task-specific finetuning. Our
findings on how to best exploit domain-specific language model finetuning
enable us to produce new state-of-the-art performance on the SemEval 2014 Task
4 restaurants dataset. In addition, to explore the real-world robustness of our
models, we perform cross-domain evaluation. We show that a cross-domain adapted
BERT language model performs significantly better than strong baseline models
like vanilla BERT-base and XLNet-base. Finally, we conduct a case study to
interpret model prediction errors.
</summary>
    <author>
      <name>Alexander Rietzler</name>
    </author>
    <author>
      <name>Sebastian Stabinger</name>
    </author>
    <author>
      <name>Paul Opitz</name>
    </author>
    <author>
      <name>Stefan Engl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.11860v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.11860v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.11365v1</id>
    <updated>2019-08-29T17:50:55Z</updated>
    <published>2019-08-29T17:50:55Z</published>
    <title>Improving Deep Transformer with Depth-Scaled Initialization and Merged
  Attention</title>
    <summary>  The general trend in NLP is towards increasing model capacity and performance
via deeper neural networks. However, simply stacking more layers of the popular
Transformer architecture for machine translation results in poor convergence
and high computational overhead. Our empirical analysis suggests that
convergence is poor due to gradient vanishing caused by the interaction between
residual connections and layer normalization. We propose depth-scaled
initialization (DS-Init), which decreases parameter variance at the
initialization stage, and reduces output variance of residual connections so as
to ease gradient back-propagation through normalization layers. To address
computational cost, we propose a merged attention sublayer (MAtt) which
combines a simplified averagebased self-attention sublayer and the
encoderdecoder attention sublayer on the decoder side. Results on WMT and IWSLT
translation tasks with five translation directions show that deep Transformers
with DS-Init and MAtt can substantially outperform their base counterpart in
terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the
decoding speed of the baseline model thanks to the efficiency improvements of
MAtt.
</summary>
    <author>
      <name>Biao Zhang</name>
    </author>
    <author>
      <name>Ivan Titov</name>
    </author>
    <author>
      <name>Rico Sennrich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.11365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.11365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.11125v1</id>
    <updated>2019-08-29T09:47:48Z</updated>
    <published>2019-08-29T09:47:48Z</published>
    <title>Probing Representations Learned by Multimodal Recurrent and Transformer
  Models</title>
    <summary>  Recent literature shows that large-scale language modeling provides excellent
reusable sentence representations with both recurrent and self-attentive
architectures. However, there has been less clarity on the commonalities and
differences in the representational properties induced by the two
architectures. It also has been shown that visual information serves as one of
the means for grounding sentence representations. In this paper, we present a
meta-study assessing the representational quality of models where the training
signal is obtained from different modalities, in particular, language modeling,
image features prediction, and both textual and multimodal machine translation.
We evaluate textual and visual features of sentence representations obtained
using predominant approaches on image retrieval and semantic textual
similarity. Our experiments reveal that on moderate-sized datasets, a sentence
counterpart in a target language or visual modality provides much stronger
training signal for sentence representation than language modeling.
Importantly, we observe that while the Transformer models achieve superior
machine translation quality, representations from the recurrent neural network
based models perform significantly better over tasks focused on semantic
relevance.
</summary>
    <author>
      <name>Jind≈ôich Libovick√Ω</name>
    </author>
    <author>
      <name>Pranava Madhyastha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.11125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.11125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.11020v1</id>
    <updated>2019-08-29T02:20:26Z</updated>
    <published>2019-08-29T02:20:26Z</published>
    <title>Regularized Context Gates on Transformer for Machine Translation</title>
    <summary>  Context gates are effective to control the contributions from the source and
target contexts in the recurrent neural network (RNN) based neural machine
translation (NMT). However, it is challenging to extend them into the advanced
Transformer architecture, which is more complicated than RNN. This paper first
provides a method to identify source and target contexts and then introduce a
gate mechanism to control the source and target contributions in Transformer.
In addition, to further reduce the bias problem in the gate mechanism, this
paper proposes a regularization method to guide the learning of the gates with
supervision automatically generated using pointwise mutual information.
Extensive experiments on 4 translation datasets demonstrate that the proposed
model obtains an averaged gain of 1.0 BLEU score over strong Transformer
baseline.
</summary>
    <author>
      <name>Xintong Li</name>
    </author>
    <author>
      <name>Lemao Liu</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Guoping Huang</name>
    </author>
    <author>
      <name>Max Meng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.11020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.11020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10924v1</id>
    <updated>2019-08-28T19:42:37Z</updated>
    <published>2019-08-28T19:42:37Z</published>
    <title>Solving Math Word Problems with Double-Decoder Transformer</title>
    <summary>  This paper proposes a Transformer-based model to generate equations for math
word problems. It achieves much better results than RNN models when copy and
align mechanisms are not used, and can outperform complex copy and align RNN
models. We also show that training a Transformer jointly in a generation task
with two decoders, left-to-right and right-to-left, is beneficial. Such a
Transformer performs better than the one with just one decoder not only because
of the ensemble effect, but also because it improves the encoder training
procedure. We also experiment with adding reinforcement learning to our model,
showing improved performance compared to MLE training.
</summary>
    <author>
      <name>Yuanliang Meng</name>
    </author>
    <author>
      <name>Anna Rumshisky</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10084v1</id>
    <updated>2019-08-27T08:50:17Z</updated>
    <published>2019-08-27T08:50:17Z</published>
    <title>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
    <summary>  BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new
state-of-the-art performance on sentence-pair regression tasks like semantic
textual similarity (STS). However, it requires that both sentences are fed into
the network, which causes a massive computational overhead: Finding the most
similar pair in a collection of 10,000 sentences requires about 50 million
inference computations (~65 hours) with BERT. The construction of BERT makes it
unsuitable for semantic similarity search as well as for unsupervised tasks
like clustering.
  In this publication, we present Sentence-BERT (SBERT), a modification of the
pretrained BERT network that use siamese and triplet network structures to
derive semantically meaningful sentence embeddings that can be compared using
cosine-similarity. This reduces the effort for finding the most similar pair
from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while
maintaining the accuracy from BERT.
  We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning
tasks, where it outperforms other state-of-the-art sentence embeddings methods.
</summary>
    <author>
      <name>Nils Reimers</name>
    </author>
    <author>
      <name>Iryna Gurevych</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.10084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.09892v1</id>
    <updated>2019-08-26T19:56:08Z</updated>
    <published>2019-08-26T19:56:08Z</published>
    <title>Does BERT agree? Evaluating knowledge of structure dependence through
  agreement relations</title>
    <summary>  Learning representations that accurately model semantics is an important goal
of natural language processing research. Many semantic phenomena depend on
syntactic structure. Recent work examines the extent to which state-of-the-art
models for pre-training representations, such as BERT, capture such
structure-dependent phenomena, but is largely restricted to one phenomenon in
English: number agreement between subjects and verbs. We evaluate BERT's
sensitivity to four types of structure-dependent agreement relations in a new
semi-automatically curated dataset across 26 languages. We show that both the
single-language and multilingual BERT models capture syntax-sensitive agreement
patterns well in general, but we also highlight the specific linguistic
contexts in which their performance degrades.
</summary>
    <author>
      <name>Geoff Bacon</name>
    </author>
    <author>
      <name>Terry Regier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.09892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.09368v1</id>
    <updated>2019-08-25T17:34:40Z</updated>
    <published>2019-08-25T17:34:40Z</published>
    <title>Transforming Delete, Retrieve, Generate Approach for Controlled Text
  Style Transfer</title>
    <summary>  Text style transfer is the task of transferring the style of text having
certain stylistic attributes, while preserving non-stylistic or content
information. In this work we introduce the Generative Style Transformer (GST) -
a new approach to rewriting sentences to a target style in the absence of
parallel style corpora. GST leverages the power of both, large unsupervised
pre-trained language models as well as the Transformer. GST is a part of a
larger `Delete Retrieve Generate' framework, in which we also propose a novel
method of deleting style attributes from the source sentence by exploiting the
inner workings of the Transformer. Our models outperform state-of-art systems
across 5 datasets on sentiment, gender and political slant transfer. We also
propose the use of the GLEU metric as an automatic metric of evaluation of
style transfer, which we found to compare better with human ratings than the
predominantly used BLEU score.
</summary>
    <author>
      <name>Akhilesh Sudhakar</name>
    </author>
    <author>
      <name>Bhargav Upadhyay</name>
    </author>
    <author>
      <name>Arjun Maheswaran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 Tables, 2 Figures, Accepted at 2019 Conference on
  Empirical Methods in Natural Language Processing (EMNLP - 2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.09368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.09355v1</id>
    <updated>2019-08-25T16:13:24Z</updated>
    <published>2019-08-25T16:13:24Z</published>
    <title>Patient Knowledge Distillation for BERT Model Compression</title>
    <summary>  Pre-trained language models such as BERT have proven to be highly effective
for natural language processing (NLP) tasks. However, the high demand for
computing resources in training such models hinders their application in
practice. In order to alleviate this resource hunger in large-scale model
training, we propose a Patient Knowledge Distillation approach to compress an
original large model (teacher) into an equally-effective lightweight shallow
network (student). Different from previous knowledge distillation methods,
which only use the output from the last layer of the teacher network for
distillation, our student model patiently learns from multiple intermediate
layers of the teacher model for incremental knowledge extraction, following two
strategies: ($i$) PKD-Last: learning from the last $k$ layers; and ($ii$)
PKD-Skip: learning from every $k$ layers. These two patient distillation
schemes enable the exploitation of rich information in the teacher's hidden
layers, and encourage the student model to patiently learn from and imitate the
teacher through a multi-layer distillation process. Empirically, this
translates into improved results on multiple NLP tasks with significant gain in
training efficiency, without sacrificing model accuracy.
</summary>
    <author>
      <name>Siqi Sun</name>
    </author>
    <author>
      <name>Yu Cheng</name>
    </author>
    <author>
      <name>Zhe Gan</name>
    </author>
    <author>
      <name>Jingjing Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.09355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.09091v4</id>
    <updated>2019-12-22T23:58:16Z</updated>
    <published>2019-08-24T05:07:36Z</published>
    <title>BERT for Coreference Resolution: Baselines and Analysis</title>
    <summary>  We apply BERT to coreference resolution, achieving strong improvements on the
OntoNotes (+3.9 F1) and GAP (+11.5 F1) benchmarks. A qualitative analysis of
model predictions indicates that, compared to ELMo and BERT-base, BERT-large is
particularly better at distinguishing between related but distinct entities
(e.g., President and CEO). However, there is still room for improvement in
modeling document-level context, conversations, and mention paraphrasing. Our
code and models are publicly available.
</summary>
    <author>
      <name>Mandar Joshi</name>
    </author>
    <author>
      <name>Omer Levy</name>
    </author>
    <author>
      <name>Daniel S. Weld</name>
    </author>
    <author>
      <name>Luke Zettlemoyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fix test set numbers for e2e-coref on GAP</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.09091v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09091v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.08594v3</id>
    <updated>2019-08-29T23:11:08Z</updated>
    <published>2019-08-23T00:58:21Z</published>
    <title>Training Optimus Prime, M.D.: Generating Medical Certification Items by
  Fine-Tuning OpenAI's gpt2 Transformer Model</title>
    <summary>  This article describes new results of an application using transformer-based
language models to automated item generation (AIG), an area of ongoing interest
in the domain of certification testing as well as in educational measurement
and psychological testing. OpenAI's gpt2 pre-trained 345M parameter language
model was retrained using the public domain text mining set of PubMed articles
and subsequently used to generate item stems (case vignettes) as well as
distractor proposals for multiple-choice items. This case study shows promise
and produces draft text that can be used by human item writers as input for
authoring. Future experiments with more recent transformer models (such as
Grover, TransformerXL) using existing item pools are expected to improve
results further and to facilitate the development of assessment materials.
</summary>
    <author>
      <name>Matthias von Davier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.08594v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08594v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.08167v2</id>
    <updated>2019-10-02T02:28:53Z</updated>
    <published>2019-08-22T02:00:53Z</published>
    <title>Multi-passage BERT: A Globally Normalized BERT Model for Open-domain
  Question Answering</title>
    <summary>  BERT model has been successfully applied to open-domain QA tasks. However,
previous work trains BERT by viewing passages corresponding to the same
question as independent training instances, which may cause incomparable scores
for answers from different passages. To tackle this issue, we propose a
multi-passage BERT model to globally normalize answer scores across all
passages of the same question, and this change enables our QA model find better
answers by utilizing more passages. In addition, we find that splitting
articles into passages with the length of 100 words by sliding window improves
performance by 4%. By leveraging a passage ranker to select high-quality
passages, multi-passage BERT gains additional 2%. Experiments on four standard
benchmarks showed that our multi-passage BERT outperforms all state-of-the-art
models on all benchmarks. In particular, on the OpenSQuAD dataset, our model
gains 21.4% EM and 21.5% $F_1$ over all non-BERT models, and 5.8% EM and 6.5%
$F_1$ over BERT-based models.
</summary>
    <author>
      <name>Zhiguo Wang</name>
    </author>
    <author>
      <name>Patrick Ng</name>
    </author>
    <author>
      <name>Xiaofei Ma</name>
    </author>
    <author>
      <name>Ramesh Nallapati</name>
    </author>
    <author>
      <name>Bing Xiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.08167v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08167v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.07721v2</id>
    <updated>2019-10-22T08:51:17Z</updated>
    <published>2019-08-21T06:56:08Z</published>
    <title>Fine-tuning BERT for Joint Entity and Relation Extraction in Chinese
  Medical Text</title>
    <summary>  Entity and relation extraction is the necessary step in structuring medical
text. However, the feature extraction ability of the bidirectional long short
term memory network in the existing model does not achieve the best effect. At
the same time, the language model has achieved excellent results in more and
more natural language processing tasks. In this paper, we present a focused
attention model for the joint entity and relation extraction task. Our model
integrates well-known BERT language model into joint learning through dynamic
range attention mechanism, thus improving the feature representation ability of
shared parameter layer. Experimental results on coronary angiography texts
collected from Shuguang Hospital show that the F1-score of named entity
recognition and relation classification tasks reach 96.89% and 88.51%, which
are better than state-of-the-art methods 1.65% and 1.22%, respectively.
</summary>
    <author>
      <name>Kui Xue</name>
    </author>
    <author>
      <name>Yangming Zhou</name>
    </author>
    <author>
      <name>Zhiyuan Ma</name>
    </author>
    <author>
      <name>Tong Ruan</name>
    </author>
    <author>
      <name>Huanhuan Zhang</name>
    </author>
    <author>
      <name>Ping He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, submitted to BIBM 2019, accepted as a regular
  paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.07721v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.07721v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.08593v2</id>
    <updated>2019-09-11T16:26:37Z</updated>
    <published>2019-08-21T04:27:38Z</published>
    <title>Revealing the Dark Secrets of BERT</title>
    <summary>  BERT-based architectures currently give state-of-the-art performance on many
NLP tasks, but little is known about the exact mechanisms that contribute to
its success. In the current work, we focus on the interpretation of
self-attention, which is one of the fundamental underlying components of BERT.
Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we
propose the methodology and carry out a qualitative and quantitative analysis
of the information encoded by the individual BERT's heads. Our findings suggest
that there is a limited set of attention patterns that are repeated across
different heads, indicating the overall model overparametrization. While
different heads consistently use the same attention patterns, they have varying
impact on performance across different tasks. We show that manually disabling
attention in certain heads leads to a performance improvement over the regular
fine-tuned BERT models.
</summary>
    <author>
      <name>Olga Kovaleva</name>
    </author>
    <author>
      <name>Alexey Romanov</name>
    </author>
    <author>
      <name>Anna Rogers</name>
    </author>
    <author>
      <name>Anna Rumshisky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EMNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.08593v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08593v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.07490v3</id>
    <updated>2019-12-03T19:30:19Z</updated>
    <published>2019-08-20T17:05:18Z</published>
    <title>LXMERT: Learning Cross-Modality Encoder Representations from
  Transformers</title>
    <summary>  Vision-and-language reasoning requires an understanding of visual concepts,
language semantics, and, most importantly, the alignment and relationships
between these two modalities. We thus propose the LXMERT (Learning
Cross-Modality Encoder Representations from Transformers) framework to learn
these vision-and-language connections. In LXMERT, we build a large-scale
Transformer model that consists of three encoders: an object relationship
encoder, a language encoder, and a cross-modality encoder. Next, to endow our
model with the capability of connecting vision and language semantics, we
pre-train the model with large amounts of image-and-sentence pairs, via five
diverse representative pre-training tasks: masked language modeling, masked
object prediction (feature regression and label classification), cross-modality
matching, and image question answering. These tasks help in learning both
intra-modality and cross-modality relationships. After fine-tuning from our
pre-trained parameters, our model achieves the state-of-the-art results on two
visual question answering datasets (i.e., VQA and GQA). We also show the
generalizability of our pre-trained cross-modality model by adapting it to a
challenging visual-reasoning task, NLVR2, and improve the previous best result
by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies
to prove that both our novel model components and pre-training strategies
significantly contribute to our strong results; and also present several
attention visualizations for the different encoders. Code and pre-trained
models publicly available at: https://github.com/airsplay/lxmert
</summary>
    <author>
      <name>Hao Tan</name>
    </author>
    <author>
      <name>Mohit Bansal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2019 (14 pages; with new attention visualizations)</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.07490v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.07490v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.07245v4</id>
    <updated>2020-01-05T11:33:23Z</updated>
    <published>2019-08-20T09:37:42Z</published>
    <title>GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge</title>
    <summary>  Word Sense Disambiguation (WSD) aims to find the exact sense of an ambiguous
word in a particular context. Traditional supervised methods rarely take into
consideration the lexical resources like WordNet, which are widely utilized in
knowledge-based methods. Recent studies have shown the effectiveness of
incorporating gloss (sense definition) into neural networks for WSD. However,
compared with traditional word expert supervised methods, they have not
achieved much improvement. In this paper, we focus on how to better leverage
gloss knowledge in a supervised neural WSD system. We construct context-gloss
pairs and propose three BERT-based models for WSD. We fine-tune the pre-trained
BERT model on SemCor3.0 training corpus and the experimental results on several
English all-words WSD benchmark datasets show that our approach outperforms the
state-of-the-art systems.
</summary>
    <author>
      <name>Luyao Huang</name>
    </author>
    <author>
      <name>Chi Sun</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP-IJCNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.07245v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.07245v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.06780v1</id>
    <updated>2019-08-19T13:14:02Z</updated>
    <published>2019-08-19T13:14:02Z</published>
    <title>A Study of BERT for Non-Factoid Question-Answering under Passage Length
  Constraints</title>
    <summary>  We study the use of BERT for non-factoid question-answering, focusing on the
passage re-ranking task under varying passage lengths. To this end, we explore
the fine-tuning of BERT in different learning-to-rank setups, comprising both
point-wise and pair-wise methods, resulting in substantial improvements over
the state-of-the-art. We then analyze the effectiveness of BERT for different
passage lengths and suggest how to cope with large passages.
</summary>
    <author>
      <name>Yosi Mass</name>
    </author>
    <author>
      <name>Haggai Roitman</name>
    </author>
    <author>
      <name>Shai Erera</name>
    </author>
    <author>
      <name>Or Rivlin</name>
    </author>
    <author>
      <name>Bar Weiner</name>
    </author>
    <author>
      <name>David Konopnicki</name>
    </author>
    <link href="http://arxiv.org/abs/1908.06780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.06264v1</id>
    <updated>2019-08-17T08:59:51Z</updated>
    <published>2019-08-17T08:59:51Z</published>
    <title>EmotionX-IDEA: Emotion BERT -- an Affectional Model for Conversation</title>
    <summary>  In this paper, we investigate the emotion recognition ability of the
pre-training language model, namely BERT. By the nature of the framework of
BERT, a two-sentence structure, we adapt BERT to continues dialogue emotion
prediction tasks, which rely heavily on the sentence-level context-aware
understanding. The experiments show that by mapping the continues dialogue into
a causal utterance pair, which is constructed by the utterance and the reply
utterance, models can better capture the emotions of the reply utterance. The
present method has achieved 0.815 and 0.885 micro F1 score in the testing
dataset of Friends and EmotionPush, respectively.
</summary>
    <author>
      <name>Yen-Hao Huang</name>
    </author>
    <author>
      <name>Ssu-Rui Lee</name>
    </author>
    <author>
      <name>Mau-Yun Ma</name>
    </author>
    <author>
      <name>Yi-Hsin Chen</name>
    </author>
    <author>
      <name>Ya-Wen Yu</name>
    </author>
    <author>
      <name>Yi-Shin Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EmotionX 2019, the shared task of SocialNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.06264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.05908v2</id>
    <updated>2019-09-26T07:56:07Z</updated>
    <published>2019-08-16T09:29:43Z</published>
    <title>BERT-Based Multi-Head Selection for Joint Entity-Relation Extraction</title>
    <summary>  In this paper, we report our method for the Information Extraction task in
2019 Language and Intelligence Challenge. We incorporate BERT into the
multi-head selection framework for joint entity-relation extraction. This model
extends existing approaches from three perspectives. First, BERT is adopted as
a feature extraction layer at the bottom of the multi-head selection framework.
We further optimize BERT by introducing a semantic-enhanced task during BERT
pre-training. Second, we introduce a large-scale Baidu Baike corpus for entity
recognition pre-training, which is of weekly supervised learning since there is
no actual named entity label. Third, soft label embedding is proposed to
effectively transmit information between entity recognition and relation
extraction. Combining these three contributions, we enhance the information
extracting ability of the multi-head selection model and achieve F1-score 0.876
on testset-1 with a single model. By ensembling four variants of our model, we
finally achieve F1 score 0.892 (1st place) on testset-1 and F1 score 0.8924
(2nd place) on testset-2.
</summary>
    <author>
      <name>Weipeng Huang</name>
    </author>
    <author>
      <name>Xingyi Cheng</name>
    </author>
    <author>
      <name>Taifeng Wang</name>
    </author>
    <author>
      <name>Wei Chu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at NLPCC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.05908v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.05908v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.05787v1</id>
    <updated>2019-08-15T22:51:21Z</updated>
    <published>2019-08-15T22:51:21Z</published>
    <title>M-BERT: Injecting Multimodal Information in the BERT Structure</title>
    <summary>  Multimodal language analysis is an emerging research area in natural language
processing that models language in a multimodal manner. It aims to understand
language from the modalities of text, visual, and acoustic by modeling both
intra-modal and cross-modal interactions. BERT (Bidirectional Encoder
Representations from Transformers) provides strong contextual language
representations after training on large-scale unlabeled corpora. Fine-tuning
the vanilla BERT model has shown promising results in building state-of-the-art
models for diverse NLP tasks like question answering and language inference.
However, fine-tuning BERT in the presence of information from other modalities
remains an open research problem. In this paper, we inject multimodal
information within the input space of BERT network for modeling multimodal
language. The proposed injection method allows BERT to reach a new state of the
art of $84.38\%$ binary accuracy on CMU-MOSI dataset (multimodal sentiment
analysis) with a gap of 5.98 percent to the previous state of the art and 1.02
percent to the text-only BERT.
</summary>
    <author>
      <name>Wasifur Rahman</name>
    </author>
    <author>
      <name>Md Kamrul Hasan</name>
    </author>
    <author>
      <name>Amir Zadeh</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <author>
      <name>Mohammed Ehsan Hoque</name>
    </author>
    <link href="http://arxiv.org/abs/1908.05787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.05787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.05646v1</id>
    <updated>2019-08-15T17:20:20Z</updated>
    <published>2019-08-15T17:20:20Z</published>
    <title>SenseBERT: Driving Some Sense into BERT</title>
    <summary>  Self-supervision techniques have allowed neural language models to advance
the frontier in Natural Language Understanding. However, existing
self-supervision techniques operate at the word-form level, which serves as a
surrogate for the underlying semantic content. This paper proposes a method to
employ self-supervision directly at the word-sense level. Our model, named
SenseBERT, is pre-trained to predict not only the masked words but also their
WordNet supersenses. Accordingly, we attain a lexical-semantic level language
model, without the use of human annotation. SenseBERT achieves significantly
improved lexical understanding, as we demonstrate by experimenting on SemEval,
and by attaining a state of the art result on the Word in Context (WiC) task.
Our approach is extendable to other linguistic signals, which can be similarly
integrated into the pre-training process, leading to increasingly semantically
informed language models.
</summary>
    <author>
      <name>Yoav Levine</name>
    </author>
    <author>
      <name>Barak Lenz</name>
    </author>
    <author>
      <name>Or Dagan</name>
    </author>
    <author>
      <name>Dan Padnos</name>
    </author>
    <author>
      <name>Or Sharir</name>
    </author>
    <author>
      <name>Shai Shalev-Shwartz</name>
    </author>
    <author>
      <name>Amnon Shashua</name>
    </author>
    <author>
      <name>Yoav Shoham</name>
    </author>
    <link href="http://arxiv.org/abs/1908.05646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.05646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.05620v1</id>
    <updated>2019-08-15T16:11:45Z</updated>
    <published>2019-08-15T16:11:45Z</published>
    <title>Visualizing and Understanding the Effectiveness of BERT</title>
    <summary>  Language model pre-training, such as BERT, has achieved remarkable results in
many NLP tasks. However, it is unclear why the pre-training-then-fine-tuning
paradigm can improve performance and generalization capability across different
tasks. In this paper, we propose to visualize loss landscapes and optimization
trajectories of fine-tuning BERT on specific datasets. First, we find that
pre-training reaches a good initial point across downstream tasks, which leads
to wider optima and easier optimization compared with training from scratch. We
also demonstrate that the fine-tuning procedure is robust to overfitting, even
though BERT is highly over-parameterized for downstream tasks. Second, the
visualization results indicate that fine-tuning BERT tends to generalize better
because of the flat and wide optima, and the consistency between the training
loss surface and the generalization error surface. Third, the lower layers of
BERT are more invariant during fine-tuning, which suggests that the layers that
are close to input learn more transferable representations of language.
</summary>
    <author>
      <name>Yaru Hao</name>
    </author>
    <author>
      <name>Li Dong</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <author>
      <name>Ke Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by EMNLP-19</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.05620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.05620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.05679v1</id>
    <updated>2019-08-15T14:08:24Z</updated>
    <published>2019-08-15T14:08:24Z</published>
    <title>Transformer-based Automatic Post-Editing with a Context-Aware Encoding
  Approach for Multi-Source Inputs</title>
    <summary>  Recent approaches to the Automatic Post-Editing (APE) research have shown
that better results are obtained by multi-source models, which jointly encode
both source (src) and machine translation output (mt) to produce post-edited
sentence (pe). Along this trend, we present a new multi-source APE model based
on the Transformer. To construct effective joint representations, our model
internally learns to incorporate src context into mt representation. With this
approach, we achieve a significant improvement over baseline systems, as well
as the state-of-the-art multi-source APE model. Moreover, to demonstrate the
capability of our model to incorporate src context, we show that the word
alignment of the unknown MT system is successfully captured in our encoding
results.
</summary>
    <author>
      <name>WonKee Lee</name>
    </author>
    <author>
      <name>Junsu Park</name>
    </author>
    <author>
      <name>Byung-Hyun Go</name>
    </author>
    <author>
      <name>Jong-Hyeok Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.05679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.05679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.05672v3</id>
    <updated>2019-08-30T11:26:20Z</updated>
    <published>2019-08-15T03:33:50Z</published>
    <title>Towards Making the Most of BERT in Neural Machine Translation</title>
    <summary>  GPT-2 and BERT demonstrate the effectiveness of using pre-trained language
models (LMs) on various natural language processing tasks. However, LM
fine-tuning often suffers from catastrophic forgetting when applied to
resource-rich tasks. In this work, we introduce a concerted training framework
(\method) that is the key to integrate the pre-trained LMs to neural machine
translation (NMT). Our proposed Cnmt consists of three techniques: a)
asymptotic distillation to ensure that the NMT model can retain the previous
pre-trained knowledge; \item a dynamic switching gate to avoid catastrophic
forgetting of pre-trained knowledge; and b)a strategy to adjust the learning
paces according to a scheduled policy. Our experiments in machine translation
show \method gains of up to 3 BLEU score on the WMT14 English-German language
pair which even surpasses the previous state-of-the-art pre-training aided NMT
by 1.4 BLEU score. While for the large WMT14 English-French task with 40
millions of sentence-pairs, our base model still significantly improves upon
the state-of-the-art Transformer big model by more than 1 BLEU score.
</summary>
    <author>
      <name>Jiacheng Yang</name>
    </author>
    <author>
      <name>Mingxuan Wang</name>
    </author>
    <author>
      <name>Hao Zhou</name>
    </author>
    <author>
      <name>Chengqi Zhao</name>
    </author>
    <author>
      <name>Yong Yu</name>
    </author>
    <author>
      <name>Weinan Zhang</name>
    </author>
    <author>
      <name>Lei Li</name>
    </author>
    <link href="http://arxiv.org/abs/1908.05672v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.05672v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.04943v2</id>
    <updated>2020-02-23T05:03:05Z</updated>
    <published>2019-08-14T03:45:15Z</published>
    <title>Establishing Strong Baselines for the New Decade: Sequence Tagging,
  Syntactic and Semantic Parsing with BERT</title>
    <summary>  This paper presents new state-of-the-art models for three tasks,
part-of-speech tagging, syntactic parsing, and semantic parsing, using the
cutting-edge contextualized embedding framework known as BERT. For each task,
we first replicate and simplify the current state-of-the-art approach to
enhance its model efficiency. We then evaluate our simplified approaches on
those three tasks using token embeddings generated by BERT. 12 datasets in both
English and Chinese are used for our experiments. The BERT models outperform
the previously best-performing models by 2.5% on average (7.5% for the most
significant case). Moreover, an in-depth analysis on the impact of BERT
embeddings is provided using self-attention, which helps understanding in this
rich yet representation. All models and source codes are available in public so
that researchers can improve upon and utilize them to establish strong
baselines for the next decade.
</summary>
    <author>
      <name>Han He</name>
    </author>
    <author>
      <name>Jinho D. Choi</name>
    </author>
    <link href="http://arxiv.org/abs/1908.04943v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04943v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.04812v1</id>
    <updated>2019-08-13T18:24:29Z</updated>
    <published>2019-08-13T18:24:29Z</published>
    <title>Domain Adaptive Training BERT for Response Selection</title>
    <summary>  We focus on multi-turn response selection in a retrieval-based dialog system.
In this paper, we utilize the powerful pre-trained language model
Bi-directional Encoder Representations from Transformer (BERT) for a multi-turn
dialog system and propose a highly effective post-training method on
domain-specific corpus. Although BERT is easily adopted to various NLP tasks
and outperforms previous baselines of each task, it still has limitations if a
task corpus is too focused on a certain domain. Post-training on
domain-specific corpus (e.g., Ubuntu Corpus) helps the model to train
contextualized representations and words that do not appear in general corpus
(e.g.,English Wikipedia). Experiment results show that our approach achieves
new state-of-the-art on two response selection benchmark datasets (i.e.,Ubuntu
Corpus V1, Advising Corpus) performance improvement by 5.9% and 6% on Recall@1.
</summary>
    <author>
      <name>Taesun Whang</name>
    </author>
    <author>
      <name>Dongyub Lee</name>
    </author>
    <author>
      <name>Chanhee Lee</name>
    </author>
    <author>
      <name>Kisu Yang</name>
    </author>
    <author>
      <name>Dongsuk Oh</name>
    </author>
    <author>
      <name>HeuiSeok Lim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.04812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.04211v4</id>
    <updated>2020-02-07T17:44:52Z</updated>
    <published>2019-08-12T15:48:34Z</published>
    <title>On Identifiability in Transformers</title>
    <summary>  In this paper we delve deep in the Transformer architecture by investigating
two of its core components: self-attention and contextual embeddings. In
particular, we study the identifiability of attention weights and token
embeddings, and the aggregation of context into hidden tokens. We show that,
for sequences longer than the attention head dimension, attention weights are
not identifiable. We propose effective attention as a complementary tool for
improving explanatory interpretations based on attention. Furthermore, we show
that input tokens retain to a large degree their identity across the model. We
also find evidence suggesting that identity information is mainly encoded in
the angle of the embeddings and gradually decreases with depth. Finally, we
demonstrate strong mixing of input information in the generation of contextual
embeddings by means of a novel quantification method based on gradient
attribution. Overall, we show that self-attention distributions are not
directly interpretable and present tools to better understand and further
investigate Transformer models.
</summary>
    <author>
      <name>Gino Brunner</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Dami√°n Pascual</name>
    </author>
    <author>
      <name>Oliver Richter</name>
    </author>
    <author>
      <name>Massimiliano Ciaramita</name>
    </author>
    <author>
      <name>Roger Wattenhofer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.04211v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04211v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7, I.7.0" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.7.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.03548v1</id>
    <updated>2019-08-09T17:19:43Z</updated>
    <published>2019-08-09T17:19:43Z</published>
    <title>BERT-based Ranking for Biomedical Entity Normalization</title>
    <summary>  Developing high-performance entity normalization algorithms that can
alleviate the term variation problem is of great interest to the biomedical
community. Although deep learning-based methods have been successfully applied
to biomedical entity normalization, they often depend on traditional
context-independent word embeddings. Bidirectional Encoder Representations from
Transformers (BERT), BERT for Biomedical Text Mining (BioBERT) and BERT for
Clinical Text Mining (ClinicalBERT) were recently introduced to pre-train
contextualized word representation models using bidirectional Transformers,
advancing the state-of-the-art for many natural language processing tasks. In
this study, we proposed an entity normalization architecture by fine-tuning the
pre-trained BERT / BioBERT / ClinicalBERT models and conducted extensive
experiments to evaluate the effectiveness of the pre-trained models for
biomedical entity normalization using three different types of datasets. Our
experimental results show that the best fine-tuned models consistently
outperformed previous methods and advanced the state-of-the-art for biomedical
entity normalization, with up to 1.17% increase in accuracy.
</summary>
    <author>
      <name>Zongcheng Ji</name>
    </author>
    <author>
      <name>Qiang Wei</name>
    </author>
    <author>
      <name>Hua Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.03548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.03548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02451v1</id>
    <updated>2019-08-07T06:02:17Z</updated>
    <published>2019-08-07T06:02:17Z</published>
    <title>TinySearch -- Semantics based Search Engine using Bert Embeddings</title>
    <summary>  Existing search engines use keyword matching or tf-idf based matching to map
the query to the web-documents and rank them. They also consider other factors
such as page rank, hubs-and-authority scores, knowledge graphs to make the
results more meaningful. However, the existing search engines fail to capture
the meaning of query when it becomes large and complex. BERT, introduced by
Google in 2018, provides embeddings for words as well as sentences. In this
paper, I have developed a semantics-oriented search engine using neural
networks and BERT embeddings that can search for query and rank the documents
in the order of the most meaningful to least meaningful. The results shows
improvement over one existing search engine for complex queries for given set
of documents.
</summary>
    <author>
      <name>Manish Patel</name>
    </author>
    <link href="http://arxiv.org/abs/1908.02451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02404v1</id>
    <updated>2019-08-07T00:04:58Z</updated>
    <published>2019-08-07T00:04:58Z</published>
    <title>Fast and Accurate Capitalization and Punctuation for Automatic Speech
  Recognition Using Transformer and Chunk Merging</title>
    <summary>  In recent years, studies on automatic speech recognition (ASR) have shown
outstanding results that reach human parity on short speech segments. However,
there are still difficulties in standardizing the output of ASR such as
capitalization and punctuation restoration for long-speech transcription. The
problems obstruct readers to understand the ASR output semantically and also
cause difficulties for natural language processing models such as NER, POS and
semantic parsing. In this paper, we propose a method to restore the punctuation
and capitalization for long-speech ASR transcription. The method is based on
Transformer models and chunk merging that allows us to (1), build a single
model that performs punctuation and capitalization in one go, and (2), perform
decoding in parallel while improving the prediction accuracy. Experiments on
British National Corpus showed that the proposed approach outperforms existing
methods in both accuracy and decoding speed.
</summary>
    <author>
      <name>Binh Nguyen</name>
    </author>
    <author>
      <name>Vu Bao Hung Nguyen</name>
    </author>
    <author>
      <name>Hien Nguyen</name>
    </author>
    <author>
      <name>Pham Ngoc Phuong</name>
    </author>
    <author>
      <name>The-Loc Nguyen</name>
    </author>
    <author>
      <name>Quoc Truong Do</name>
    </author>
    <author>
      <name>Luong Chi Mai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01767v3</id>
    <updated>2020-03-08T23:10:16Z</updated>
    <published>2019-08-04T16:48:24Z</published>
    <title>Exploring Neural Net Augmentation to BERT for Question Answering on
  SQUAD 2.0</title>
    <summary>  Enhancing machine capabilities to answer questions has been a topic of
considerable focus in recent years of NLP research. Language models like
Embeddings from Language Models (ELMo)[1] and Bidirectional Encoder
Representations from Transformers (BERT) [2] have been very successful in
developing general purpose language models that can be optimized for a large
number of downstream language tasks. In this work, we focused on augmenting the
pre-trained BERT language model with different output neural net architectures
and compared their performance on question answering task posed by the Stanford
Question Answering Dataset 2.0 (SQUAD 2.0) [3]. Additionally, we also
fine-tuned the pre-trained BERT model parameters to demonstrate its
effectiveness in adapting to specialized language tasks. Our best output
network, is the contextualized CNN that performs on both the unanswerable and
answerable question answering tasks with F1 scores of 75.32 and 64.85
respectively.
</summary>
    <author>
      <name>Suhas Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code bug found</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.01767v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01767v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.00449v1</id>
    <updated>2019-08-01T15:05:41Z</updated>
    <published>2019-08-01T15:05:41Z</published>
    <title>Tree-Transformer: A Transformer-Based Method for Correction of
  Tree-Structured Data</title>
    <summary>  Many common sequential data sources, such as source code and natural
language, have a natural tree-structured representation. These trees can be
generated by fitting a sequence to a grammar, yielding a hierarchical ordering
of the tokens in the sequence. This structure encodes a high degree of
syntactic information, making it ideal for problems such as grammar correction.
However, little work has been done to develop neural networks that can operate
on and exploit tree-structured data. In this paper we present the
Tree-Transformer \textemdash{} a novel neural network architecture designed to
translate between arbitrary input and output trees. We applied this
architecture to correction tasks in both the source code and natural language
domains. On source code, our model achieved an improvement of $25\%$
$\text{F}0.5$ over the best sequential method. On natural language, we achieved
comparable results to the most complex state of the art systems, obtaining a
$10\%$ improvement in recall on the CoNLL 2014 benchmark and the highest to
date $\text{F}0.5$ score on the AESW benchmark of $50.43$.
</summary>
    <author>
      <name>Jacob Harer</name>
    </author>
    <author>
      <name>Chris Reale</name>
    </author>
    <author>
      <name>Peter Chin</name>
    </author>
    <link href="http://arxiv.org/abs/1908.00449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.00308v1</id>
    <updated>2019-08-01T10:27:29Z</updated>
    <published>2019-08-01T10:27:29Z</published>
    <title>MSnet: A BERT-based Network for Gendered Pronoun Resolution</title>
    <summary>  The pre-trained BERT model achieves a remarkable state of the art across a
wide range of tasks in natural language processing. For solving the gender bias
in gendered pronoun resolution task, I propose a novel neural network model
based on the pre-trained BERT. This model is a type of mention score classifier
and uses an attention mechanism with no parameters to compute the contextual
representation of entity span, and a vector to represent the triple-wise
semantic similarity among the pronoun and the entities. In stage 1 of the
gendered pronoun resolution task, a variant of this model, trained in the
fine-tuning approach, reduced the multi-class logarithmic loss to 0.3033 in the
5-fold cross-validation of training set and 0.2795 in testing set. Besides,
this variant won the 2nd place with a score at 0.17289 in stage 2 of the task.
The code in this paper is available at:
https://github.com/ziliwang/MSnet-for-Gendered-PronounResolution
</summary>
    <author>
      <name>Zili Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages; 1 figures; accepted by 1st ACL Workshop on Gender Bias for
  NLP at ACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.00308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.13528v1</id>
    <updated>2019-07-31T14:37:32Z</updated>
    <published>2019-07-31T14:37:32Z</published>
    <title>What BERT is not: Lessons from a new suite of psycholinguistic
  diagnostics for language models</title>
    <summary>  Pre-training by language modeling has become a popular and successful
approach to NLP tasks, but we have yet to understand exactly what linguistic
capacities these pre-training processes confer upon models. In this paper we
introduce a suite of diagnostics drawn from human language experiments, which
allow us to ask targeted questions about the information used by language
models for generating predictions in context. As a case study, we apply these
diagnostics to the popular BERT model, finding that it can generally
distinguish good from bad completions involving shared category or role
reversal, albeit with less sensitivity than humans, and it robustly retrieves
noun hypernyms, but it struggles with challenging inferences and role-based
event prediction -- and in particular, it shows clear insensitivity to the
contextual impacts of negation.
</summary>
    <author>
      <name>Allyson Ettinger</name>
    </author>
    <link href="http://arxiv.org/abs/1907.13528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.13528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.12750v1</id>
    <updated>2019-07-30T06:17:19Z</updated>
    <published>2019-07-30T06:17:19Z</published>
    <title>English-Czech Systems in WMT19: Document-Level Transformer</title>
    <summary>  We describe our NMT systems submitted to the WMT19 shared task in
English-Czech news translation. Our systems are based on the Transformer model
implemented in either Tensor2Tensor (T2T) or Marian framework.
  We aimed at improving the adequacy and coherence of translated documents by
enlarging the context of the source and target. Instead of translating each
sentence independently, we split the document into possibly overlapping
multi-sentence segments. In case of the T2T implementation, this
"document-level"-trained system achieves a $+0.6$ BLEU improvement ($p&lt;0.05$)
relative to the same system applied on isolated sentences. To assess the
potential effect document-level models might have on lexical coherence, we
performed a semi-automatic analysis, which revealed only a few sentences
improved in this aspect. Thus, we cannot draw any conclusions from this weak
evidence.
</summary>
    <author>
      <name>Martin Popel</name>
    </author>
    <author>
      <name>Dominik Mach√°ƒçek</name>
    </author>
    <author>
      <name>Michal Auersperger</name>
    </author>
    <author>
      <name>Ond≈ôej Bojar</name>
    </author>
    <author>
      <name>Pavel Pecina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WMT19</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.12750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.12679v1</id>
    <updated>2019-07-29T22:53:59Z</updated>
    <published>2019-07-29T22:53:59Z</published>
    <title>Machine Translation Evaluation with BERT Regressor</title>
    <summary>  We introduce the metric using BERT (Bidirectional Encoder Representations
from Transformers) (Devlin et al., 2019) for automatic machine translation
evaluation. The experimental results of the WMT-2017 Metrics Shared Task
dataset show that our metric achieves state-of-the-art performance in
segment-level metrics task for all to-English language pairs.
</summary>
    <author>
      <name>Hiroki Shimanaka</name>
    </author>
    <author>
      <name>Tomoyuki Kajiwara</name>
    </author>
    <author>
      <name>Mamoru Komachi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.12679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.11932v4</id>
    <updated>2020-01-23T07:16:25Z</updated>
    <published>2019-07-27T15:07:04Z</published>
    <title>Is BERT Really Robust? A Strong Baseline for Natural Language Attack on
  Text Classification and Entailment</title>
    <summary>  Machine learning algorithms are often vulnerable to adversarial examples that
have imperceptible alterations from the original counterparts but can fool the
state-of-the-art models. It is helpful to evaluate or even improve the
robustness of these models by exposing the maliciously crafted adversarial
examples. In this paper, we present TextFooler, a simple but strong baseline to
generate natural adversarial text. By applying it to two fundamental natural
language tasks, text classification and textual entailment, we successfully
attacked three target models, including the powerful pre-trained BERT, and the
widely used convolutional and recurrent neural networks. We demonstrate the
advantages of this framework in three ways: (1) effective---it outperforms
state-of-the-art attacks in terms of success rate and perturbation rate, (2)
utility-preserving---it preserves semantic content and grammaticality, and
remains correctly classified by humans, and (3) efficient---it generates
adversarial text with computational complexity linear to the text length. *The
code, pre-trained target models, and test examples are available at
https://github.com/jind11/TextFooler.
</summary>
    <author>
      <name>Di Jin</name>
    </author>
    <author>
      <name>Zhijing Jin</name>
    </author>
    <author>
      <name>Joey Tianyi Zhou</name>
    </author>
    <author>
      <name>Peter Szolovits</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2020 (Oral)</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.11932v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11932v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01841v2</id>
    <updated>2019-09-04T23:08:10Z</updated>
    <published>2019-07-26T21:53:09Z</published>
    <title>DLGNet: A Transformer-based Model for Dialogue Response Generation</title>
    <summary>  Neural dialogue models, despite their successes, still suffer from lack of
relevance, diversity, and in many cases coherence in their generated responses.
These issues can attributed to reasons including (1) short-range model
architectures that capture limited temporal dependencies, (2) limitations of
the maximum likelihood training objective, (3) the concave entropy profile of
dialogue datasets resulting in short and generic responses, and (4) the
out-of-vocabulary problem leading to generation of a large number of &lt;UNK&gt;
tokens. On the other hand, transformer-based models such as GPT-2 have
demonstrated an excellent ability to capture long-range structures in language
modeling tasks. In this paper, we present DLGNet, a transformer-based model for
dialogue modeling. We specifically examine the use of DLGNet for multi-turn
dialogue response generation. In our experiments, we evaluate DLGNet on the
open-domain Movie Triples dataset and the closed-domain Ubuntu Dialogue
dataset. DLGNet models, although trained with only the maximum likelihood
objective, achieve significant improvements over state-of-the-art multi-turn
dialogue models. They also produce best performance to date on the two datasets
based on several metrics, including BLEU, ROUGE, and distinct n-gram. Our
analysis shows that the performance improvement is mostly due to the
combination of (1) the long-range transformer architecture with (2) the
injection of random informative paddings. Other contributing factors include
the joint modeling of dialogue context and response, and the 100% tokenization
coverage from the byte pair encoding (BPE).
</summary>
    <author>
      <name>Oluwatobi Olabiyi</name>
    </author>
    <author>
      <name>Erik T. Mueller</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01841v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01841v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.11692v1</id>
    <updated>2019-07-26T17:48:29Z</updated>
    <published>2019-07-26T17:48:29Z</published>
    <title>RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
    <summary>  Language model pretraining has led to significant performance gains but
careful comparison between different approaches is challenging. Training is
computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the
final results. We present a replication study of BERT pretraining (Devlin et
al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can
match or exceed the performance of every model published after it. Our best
model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results
highlight the importance of previously overlooked design choices, and raise
questions about the source of recently reported improvements. We release our
models and code.
</summary>
    <author>
      <name>Yinhan Liu</name>
    </author>
    <author>
      <name>Myle Ott</name>
    </author>
    <author>
      <name>Naman Goyal</name>
    </author>
    <author>
      <name>Jingfei Du</name>
    </author>
    <author>
      <name>Mandar Joshi</name>
    </author>
    <author>
      <name>Danqi Chen</name>
    </author>
    <author>
      <name>Omer Levy</name>
    </author>
    <author>
      <name>Mike Lewis</name>
    </author>
    <author>
      <name>Luke Zettlemoyer</name>
    </author>
    <author>
      <name>Veselin Stoyanov</name>
    </author>
    <link href="http://arxiv.org/abs/1907.11692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.09669v1</id>
    <updated>2019-07-23T03:05:39Z</updated>
    <published>2019-07-23T03:05:39Z</published>
    <title>EmotionX-HSU: Adopting Pre-trained BERT for Emotion Classification</title>
    <summary>  This paper describes our approach to the EmotionX-2019, the shared task of
SocialNLP 2019. To detect emotion for each utterance of two datasets from the
TV show Friends and Facebook chat log EmotionPush, we propose two-step deep
learning based methodology: (i) encode each of the utterance into a sequence of
vectors that represent its meaning; and (ii) use a simply softmax classifier to
predict one of the emotions amongst four candidates that an utterance may
carry. Notice that the source of labeled utterances is not rich, we utilise a
well-trained model, known as BERT, to transfer part of the knowledge learned
from a large amount of corpus to our model. We then focus on fine-tuning our
model until it well fits to the in-domain data. The performance of the proposed
model is evaluated by micro-F1 scores, i.e., 79.1% and 86.2% for the testsets
of Friends and EmotionPush, respectively. Our model ranks 3rd among 11
submissions.
</summary>
    <author>
      <name>Linkai Luo</name>
    </author>
    <author>
      <name>Yue Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1907.09669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.08854v3</id>
    <updated>2019-07-31T22:09:11Z</updated>
    <published>2019-07-20T18:49:36Z</published>
    <title>Incremental Transformer with Deliberation Decoder for Document Grounded
  Conversations</title>
    <summary>  Document Grounded Conversations is a task to generate dialogue responses when
chatting about the content of a given document. Obviously, document knowledge
plays a critical role in Document Grounded Conversations, while existing
dialogue models do not exploit this kind of knowledge effectively enough. In
this paper, we propose a novel Transformer-based architecture for multi-turn
document grounded conversations. In particular, we devise an Incremental
Transformer to encode multi-turn utterances along with knowledge in related
documents. Motivated by the human cognitive process, we design a two-pass
decoder (Deliberation Decoder) to improve context coherence and knowledge
correctness. Our empirical study on a real-world Document Grounded Dataset
proves that responses generated by our model significantly outperform
competitive baselines on both context coherence and knowledge relevance.
</summary>
    <author>
      <name>Zekang Li</name>
    </author>
    <author>
      <name>Cheng Niu</name>
    </author>
    <author>
      <name>Fandong Meng</name>
    </author>
    <author>
      <name>Yang Feng</name>
    </author>
    <author>
      <name>Qian Li</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a long paper at ACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.08854v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08854v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.06226v4</id>
    <updated>2019-08-16T01:48:46Z</updated>
    <published>2019-07-14T14:19:22Z</published>
    <title>A Simple BERT-Based Approach for Lexical Simplification</title>
    <summary>  Lexical simplification (LS) aims to replace complex words in a given sentence
with their simpler alternatives of equivalent meaning. Recently unsupervised
lexical simplification approaches only rely on the complex word itself
regardless of the given sentence to generate candidate substitutions, which
will inevitably produce a large number of spurious candidates. We present a
simple BERT-based LS approach that makes use of the pre-trained unsupervised
deep bidirectional representations BERT. Despite being entirely unsupervised,
experimental results show that our approach obtains obvious improvement than
these baselines leveraging linguistic databases and parallel corpus,
outperforming the state-of-the-art by more than 11 Accuracy points on three
well-known benchmarks.
</summary>
    <author>
      <name>Jipeng Qiang</name>
    </author>
    <author>
      <name>Yun Li</name>
    </author>
    <author>
      <name>Yi Zhu</name>
    </author>
    <author>
      <name>Yunhao Yuan</name>
    </author>
    <author>
      <name>Xindong Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1907.06226v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.06226v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.05572v1</id>
    <updated>2019-07-12T04:01:57Z</updated>
    <published>2019-07-12T04:01:57Z</published>
    <title>R-Transformer: Recurrent Neural Network Enhanced Transformer</title>
    <summary>  Recurrent Neural Networks have long been the dominating choice for sequence
modeling. However, it severely suffers from two issues: impotent in capturing
very long-term dependencies and unable to parallelize the sequential
computation procedure. Therefore, many non-recurrent sequence models that are
built on convolution and attention operations have been proposed recently.
Notably, models with multi-head attention such as Transformer have demonstrated
extreme effectiveness in capturing long-term dependencies in a variety of
sequence modeling tasks. Despite their success, however, these models lack
necessary components to model local structures in sequences and heavily rely on
position embeddings that have limited effects and require a considerable amount
of design efforts. In this paper, we propose the R-Transformer which enjoys the
advantages of both RNNs and the multi-head attention mechanism while avoids
their respective drawbacks. The proposed model can effectively capture both
local structures and global long-term dependencies in sequences without any use
of position embeddings. We evaluate R-Transformer through extensive experiments
with data from a wide range of domains and the empirical results show that
R-Transformer outperforms the state-of-the-art methods by a large margin in
most of the tasks. We have made the code publicly available at
\url{https://github.com/DSE-MSU/R-transformer}.
</summary>
    <author>
      <name>Zhiwei Wang</name>
    </author>
    <author>
      <name>Yao Ma</name>
    </author>
    <author>
      <name>Zitao Liu</name>
    </author>
    <author>
      <name>Jiliang Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1907.05572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.05048v1</id>
    <updated>2019-07-11T08:41:53Z</updated>
    <published>2019-07-11T08:41:53Z</published>
    <title>No Word is an Island -- A Transformation Weighting Model for Semantic
  Composition</title>
    <summary>  Composition models of distributional semantics are used to construct phrase
representations from the representations of their words. Composition models are
typically situated on two ends of a spectrum. They either have a small number
of parameters but compose all phrases in the same way, or they perform
word-specific compositions at the cost of a far larger number of parameters. In
this paper we propose transformation weighting (TransWeight), a composition
model that consistently outperforms existing models on nominal compounds,
adjective-noun phrases and adverb-adjective phrases in English, German and
Dutch. TransWeight drastically reduces the number of parameters needed compared
to the best model in the literature by composing similar words in the same way.
</summary>
    <author>
      <name>Corina Dima</name>
    </author>
    <author>
      <name>Dani√´l de Kok</name>
    </author>
    <author>
      <name>Neele Witte</name>
    </author>
    <author>
      <name>Erhard Hinrichs</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Author final version of article accepted for publication in TACL</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.05048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.06585v1</id>
    <updated>2019-07-08T13:08:12Z</updated>
    <published>2019-07-08T13:08:12Z</published>
    <title>Parallelism Theorem and Derived Rules for Parallel Coherent
  Transformations</title>
    <summary>  An Independent Parallelism Theorem is proven in the theory of adhesive HLR
categories. It shows the bijective correspondence between sequential
independent and parallel independent direct derivations in the Weak
Double-Pushout framework, see [2]. The parallel derivations are expressed by
means of Parallel Coherent Transformations (PCTs), hence without assuming the
existence of coproducts compatible with M as in the standard Parallelism
Theorem. It is aslo shown that a derived rule can be extracted from any PCT, in
the sense that to any direct derivation of this rule corresponds a valid PCT.
</summary>
    <author>
      <name>Thierry Boy de la Tour</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.06585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.06585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; F.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.03040v1</id>
    <updated>2019-07-05T22:41:02Z</updated>
    <published>2019-07-05T22:41:02Z</published>
    <title>BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional
  Encoder Representations from Transformer</title>
    <summary>  An important yet rarely tackled problem in dialogue state tracking (DST) is
scalability for dynamic ontology (e.g., movie, restaurant) and unseen slot
values. We focus on a specific condition, where the ontology is unknown to the
state tracker, but the target slot value (except for none and dontcare),
possibly unseen during training, can be found as word segment in the dialogue
context. Prior approaches often rely on candidate generation from n-gram
enumeration or slot tagger outputs, which can be inefficient or suffer from
error propagation. We propose BERT-DST, an end-to-end dialogue state tracker
which directly extracts slot values from the dialogue context. We use BERT as
dialogue context encoder whose contextualized language representations are
suitable for scalable DST to identify slot values from their semantic context.
Furthermore, we employ encoder parameter sharing across all slots with two
advantages: (1) Number of parameters does not grow linearly with the ontology.
(2) Language representation knowledge can be transferred among slots. Empirical
evaluation shows BERT-DST with cross-slot parameter sharing outperforms prior
work on the benchmark scalable DST datasets Sim-M and Sim-R, and achieves
competitive performance on the standard DSTC2 and WOZ 2.0 datasets.
</summary>
    <author>
      <name>Guan-Lin Chao</name>
    </author>
    <author>
      <name>Ian Lane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Interspeech 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.03040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.03040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.02884v1</id>
    <updated>2019-07-05T15:11:29Z</updated>
    <published>2019-07-05T15:11:29Z</published>
    <title>Multi-lingual Intent Detection and Slot Filling in a Joint BERT-based
  Model</title>
    <summary>  Intent Detection and Slot Filling are two pillar tasks in Spoken Natural
Language Understanding. Common approaches adopt joint Deep Learning
architectures in attention-based recurrent frameworks. In this work, we aim at
exploiting the success of "recurrence-less" models for these tasks. We
introduce Bert-Joint, i.e., a multi-lingual joint text classification and
sequence labeling framework. The experimental evaluation over two well-known
English benchmarks demonstrates the strong performances that can be obtained
with this model, even when few annotated data is available. Moreover, we
annotated a new dataset for the Italian language, and we observed similar
performances without the need for changing the model.
</summary>
    <author>
      <name>Giuseppe Castellucci</name>
    </author>
    <author>
      <name>Valentina Bellomaria</name>
    </author>
    <author>
      <name>Andrea Favalli</name>
    </author>
    <author>
      <name>Raniero Romagnoli</name>
    </author>
    <link href="http://arxiv.org/abs/1907.02884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.01356v2</id>
    <updated>2019-07-03T03:48:46Z</updated>
    <published>2019-07-02T13:35:37Z</published>
    <title>Predicting Retrosynthetic Reaction using Self-Corrected Transformer
  Neural Networks</title>
    <summary>  Synthesis planning is the process of recursively decomposing target molecules
into available precursors. Computer-aided retrosynthesis can potentially assist
chemists in designing synthetic routes, but at present it is cumbersome and
provides results of dissatisfactory quality. In this study, we develop a
template-free self-corrected retrosynthesis predictor (SCROP) to perform a
retrosynthesis prediction task trained by using the Transformer neural network
architecture. In the method, the retrosynthesis planning is converted as a
machine translation problem between molecular linear notations of reactants and
the products. Coupled with a neural network-based syntax corrector, our method
achieves an accuracy of 59.0% on a standard benchmark dataset, which increases
&gt;21% over other deep learning methods, and &gt;6% over template-based methods.
More importantly, our method shows an accuracy 1.7 times higher than other
state-of-the-art methods for compounds not appearing in the training set.
</summary>
    <author>
      <name>Shuangjia Zheng</name>
    </author>
    <author>
      <name>Jiahua Rao</name>
    </author>
    <author>
      <name>Zhongyue Zhang</name>
    </author>
    <author>
      <name>Jun Xu</name>
    </author>
    <author>
      <name>Yuedong Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1907.01356v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01356v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.01166v1</id>
    <updated>2019-07-02T04:54:17Z</updated>
    <published>2019-07-02T04:54:17Z</published>
    <title>Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue
  Systems</title>
    <summary>  Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is
conducted based on visual and audio aspects of a given video, is significantly
more challenging than traditional image or text-grounded dialogue systems
because (1) feature space of videos span across multiple picture frames, making
it difficult to obtain semantic information; and (2) a dialogue agent must
perceive and process information from different modalities (audio, video,
caption, etc.) to obtain a comprehensive understanding. Most existing work is
based on RNNs and sequence-to-sequence architectures, which are not very
effective for capturing complex long-term dependencies (like in videos). To
overcome this, we propose Multimodal Transformer Networks (MTN) to encode
videos and incorporate information from different modalities. We also propose
query-aware attention through an auto-encoder to extract query-aware features
from non-text modalities. We develop a training procedure to simulate
token-level decoding to improve the quality of generated responses during
inference. We get state of the art performance on Dialogue System Technology
Challenge 7 (DSTC7). Our model also generalizes to another multimodal
visual-grounded dialogue task, and obtains promising performance. We
implemented our models using PyTorch and the code is released at
https://github.com/henryhungle/MTN.
</summary>
    <author>
      <name>Hung Le</name>
    </author>
    <author>
      <name>Doyen Sahoo</name>
    </author>
    <author>
      <name>Nancy F. Chen</name>
    </author>
    <author>
      <name>Steven C. H. Hoi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18653/v1/P19-1564</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18653/v1/P19-1564" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACL 2019 (Long Paper)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Association for Computational Linguistics (2019) 5612-5623</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1907.01166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00570v2</id>
    <updated>2019-07-08T14:57:07Z</updated>
    <published>2019-07-01T06:46:43Z</published>
    <title>Do Transformer Attention Heads Provide Transparency in Abstractive
  Summarization?</title>
    <summary>  Learning algorithms become more powerful, often at the cost of increased
complexity. In response, the demand for algorithms to be transparent is
growing. In NLP tasks, attention distributions learned by attention-based deep
learning models are used to gain insights in the models' behavior. To which
extent is this perspective valid for all NLP tasks? We investigate whether
distributions calculated by different attention heads in a transformer
architecture can be used to improve transparency in the task of abstractive
summarization. To this end, we present both a qualitative and quantitative
analysis to investigate the behavior of the attention heads. We show that some
attention heads indeed specialize towards syntactically and semantically
distinct input. We propose an approach to evaluate to which extent the
Transformer model relies on specifically learned attention distributions. We
also discuss what this implies for using attention distributions as a means of
transparency.
</summary>
    <author>
      <name>Joris Baan</name>
    </author>
    <author>
      <name>Maartje ter Hoeve</name>
    </author>
    <author>
      <name>Marlies van der Wees</name>
    </author>
    <author>
      <name>Anne Schuth</name>
    </author>
    <author>
      <name>Maarten de Rijke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at FACTS-IR 2019, SIGIR</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.00570v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00570v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.12035v1</id>
    <updated>2019-06-28T04:08:15Z</updated>
    <published>2019-06-28T04:08:15Z</published>
    <title>Multi-Criteria Chinese Word Segmentation with Transformer</title>
    <summary>  Different linguistic perspectives cause many diverse segmentation criteria
for Chinese word segmentation (CWS). Most existing methods focus on improving
the performance of single-criterion CWS. However, it is interesting to exploit
these heterogeneous segmentation criteria and mine their common underlying
knowledge. In this paper, we propose a concise and effective model for
multi-criteria CWS, which utilizes a shared fully-connected self-attention
model to segment the sentence according to a criterion indicator. Experiments
on eight datasets with heterogeneous segmentation criteria show that the
performance of each corpus obtains a significant improvement, compared to
single-criterion learning.
</summary>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Hengzhi Pei</name>
    </author>
    <author>
      <name>Hang Yan</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.12035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.12035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.11565v2</id>
    <updated>2019-08-22T09:30:45Z</updated>
    <published>2019-06-27T11:46:48Z</published>
    <title>EmotionX-KU: BERT-Max based Contextual Emotion Classifier</title>
    <summary>  We propose a contextual emotion classifier based on a transferable language
model and dynamic max pooling, which predicts the emotion of each utterance in
a dialogue. A representative emotion analysis task, EmotionX, requires to
consider contextual information from colloquial dialogues and to deal with a
class imbalance problem. To alleviate these problems, our model leverages the
self-attention based transferable language model and the weighted cross entropy
loss. Furthermore, we apply post-training and fine-tuning mechanisms to enhance
the domain adaptability of our model and utilize several machine learning
techniques to improve its performance. We conduct experiments on two
emotion-labeled datasets named Friends and EmotionPush. As a result, our model
outperforms the previous state-of-the-art model and also shows competitive
performance in the EmotionX 2019 challenge. The code will be available in the
Github page.
</summary>
    <author>
      <name>Kisu Yang</name>
    </author>
    <author>
      <name>Dongyub Lee</name>
    </author>
    <author>
      <name>Taesun Whang</name>
    </author>
    <author>
      <name>Seolhwa Lee</name>
    </author>
    <author>
      <name>Heuiseok Lim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 7th International Workshop on Natural Language Processing for
  Social Media (in conjunction with IJCAI 2019); figure modified</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.11565v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11565v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.11511v1</id>
    <updated>2019-06-27T09:05:32Z</updated>
    <published>2019-06-27T09:05:32Z</published>
    <title>Inducing Syntactic Trees from BERT Representations</title>
    <summary>  We use the English model of BERT and explore how a deletion of one word in a
sentence changes representations of other words. Our hypothesis is that
removing a reducible word (e.g. an adjective) does not affect the
representation of other words so much as removing e.g. the main verb, which
makes the sentence ungrammatical and of "high surprise" for the language model.
We estimate reducibilities of individual words and also of longer continuous
phrases (word n-grams), study their syntax-related properties, and then also
use them to induce full dependency trees.
</summary>
    <author>
      <name>Rudolf Rosa</name>
    </author>
    <author>
      <name>David Mareƒçek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted abstract for the BlackboxNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.11511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.11024v1</id>
    <updated>2019-06-26T12:27:05Z</updated>
    <published>2019-06-26T12:27:05Z</published>
    <title>Sharing Attention Weights for Fast Transformer</title>
    <summary>  Recently, the Transformer machine translation system has shown strong results
by stacking attention layers on both the source and target-language sides. But
the inference of this model is slow due to the heavy use of dot-product
attention in auto-regressive decoding. In this paper we speed up Transformer
via a fast and lightweight attention model. More specifically, we share
attention weights in adjacent layers and enable the efficient re-use of hidden
states in a vertical manner. Moreover, the sharing policy can be jointly
learned with the MT model. We test our approach on ten WMT and NIST OpenMT
tasks. Experimental results show that it yields an average of 1.3X speed-up
(with almost no decrease in BLEU) on top of a state-of-the-art implementation
that has already adopted a cache for fast inference. Also, our approach obtains
a 1.8X speed-up when it works with the \textsc{Aan} model. This is even 16
times faster than the baseline with no use of the attention cache.
</summary>
    <author>
      <name>Tong Xiao</name>
    </author>
    <author>
      <name>Yinqiao Li</name>
    </author>
    <author>
      <name>Jingbo Zhu</name>
    </author>
    <author>
      <name>Zhengtao Yu</name>
    </author>
    <author>
      <name>Tongran Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.11024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.09777v3</id>
    <updated>2019-11-06T13:53:14Z</updated>
    <published>2019-06-24T08:28:37Z</published>
    <title>A Tensorized Transformer for Language Modeling</title>
    <summary>  Latest development of neural models has connected the encoder and decoder
through a self-attention mechanism. In particular, Transformer, which is solely
based on self-attention, has led to breakthroughs in Natural Language
Processing (NLP) tasks. However, the multi-head attention mechanism, as a key
component of Transformer, limits the effective deployment of the model to a
resource-limited setting. In this paper, based on the ideas of tensor
decomposition and parameters sharing, we propose a novel self-attention model
(namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We
test and verify the proposed attention method on three language modeling tasks
(i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task
(i.e., WMT-2016 English-German). Multi-linear attention can not only largely
compress the model parameters but also obtain performance improvements,
compared with a number of language modeling approaches, such as Transformer,
Transformer-XL, and Transformer with tensor train decomposition.
</summary>
    <author>
      <name>Xindian Ma</name>
    </author>
    <author>
      <name>Peng Zhang</name>
    </author>
    <author>
      <name>Shuai Zhang</name>
    </author>
    <author>
      <name>Nan Duan</name>
    </author>
    <author>
      <name>Yuexian Hou</name>
    </author>
    <author>
      <name>Dawei Song</name>
    </author>
    <author>
      <name>Ming Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by NeurIPS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.09777v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.09777v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.09543v1</id>
    <updated>2019-06-23T02:56:02Z</updated>
    <published>2019-06-23T02:56:02Z</published>
    <title>Cross-lingual Data Transformation and Combination for Text
  Classification</title>
    <summary>  Text classification is a fundamental task for text data mining. In order to
train a generalizable model, a large volume of text must be collected. To
address data insufficiency, cross-lingual data may occasionally be necessary.
Cross-lingual data sources may however suffer from data incompatibility, as
text written in different languages can hold distinct word sequences and
semantic patterns. Machine translation and word embedding alignment provide an
effective way to transform and combine data for cross-lingual data training. To
the best of our knowledge, there has been little work done on evaluating how
the methodology used to conduct semantic space transformation and data
combination affects the performance of classification models trained from
cross-lingual resources. In this paper, we systematically evaluated the
performance of two commonly used CNN (Convolutional Neural Network) and RNN
(Recurrent Neural Network) text classifiers with differing data transformation
and combination strategies. Monolingual models were trained from English and
French alongside their translated and aligned embeddings. Our results suggested
that semantic space transformation may conditionally promote the performance of
monolingual models. Bilingual models were trained from a combination of both
English and French. Our results indicate that a cross-lingual classification
model can significantly benefit from cross-lingual data by learning from
translated or aligned embedding spaces.
</summary>
    <author>
      <name>Jun Jiang</name>
    </author>
    <author>
      <name>Shumao Pang</name>
    </author>
    <author>
      <name>Xia Zhao</name>
    </author>
    <author>
      <name>Liwei Wang</name>
    </author>
    <author>
      <name>Andrew Wen</name>
    </author>
    <author>
      <name>Hongfang Liu</name>
    </author>
    <author>
      <name>Qianjin Feng</name>
    </author>
    <link href="http://arxiv.org/abs/1906.09543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.09543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.08101v2</id>
    <updated>2019-10-29T03:44:25Z</updated>
    <published>2019-06-19T13:54:25Z</published>
    <title>Pre-Training with Whole Word Masking for Chinese BERT</title>
    <summary>  Bidirectional Encoder Representations from Transformers (BERT) has shown
marvelous improvements across various NLP tasks. Recently, an upgraded version
of BERT has been released with Whole Word Masking (WWM), which mitigate the
drawbacks of masking partial WordPiece tokens in pre-training BERT. In this
technical report, we adapt whole word masking in Chinese text, that masking the
whole word instead of masking Chinese characters, which could bring another
challenge in Masked Language Model (MLM) pre-training task. The proposed models
are verified on various NLP tasks, across sentence-level to document-level,
including machine reading comprehension (CMRC 2018, DRCD, CJRC), natural
language inference (XNLI), sentiment classification (ChnSentiCorp), sentence
pair matching (LCQMC, BQ Corpus), and document classification (THUCNews).
Experimental results on these datasets show that the whole word masking could
bring another significant gain. Moreover, we also examine the effectiveness of
the Chinese pre-trained models: BERT, ERNIE, BERT-wwm, BERT-wwm-ext,
RoBERTa-wwm-ext, and RoBERTa-wwm-ext-large. We release all the pre-trained
models: \url{https://github.com/ymcui/Chinese-BERT-wwm
</summary>
    <author>
      <name>Yiming Cui</name>
    </author>
    <author>
      <name>Wanxiang Che</name>
    </author>
    <author>
      <name>Ting Liu</name>
    </author>
    <author>
      <name>Bing Qin</name>
    </author>
    <author>
      <name>Ziqing Yang</name>
    </author>
    <author>
      <name>Shijin Wang</name>
    </author>
    <author>
      <name>Guoping Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.08101v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.08101v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.08646v1</id>
    <updated>2019-06-19T11:04:51Z</updated>
    <published>2019-06-19T11:04:51Z</published>
    <title>Fine-tuning Pre-Trained Transformer Language Models to Distantly
  Supervised Relation Extraction</title>
    <summary>  Distantly supervised relation extraction is widely used to extract relational
facts from text, but suffers from noisy labels. Current relation extraction
methods try to alleviate the noise by multi-instance learning and by providing
supporting linguistic and contextual information to more efficiently guide the
relation classification. While achieving state-of-the-art results, we observed
these models to be biased towards recognizing a limited set of relations with
high precision, while ignoring those in the long tail. To address this gap, we
utilize a pre-trained language model, the OpenAI Generative Pre-trained
Transformer (GPT) [Radford et al., 2018]. The GPT and similar models have been
shown to capture semantic and syntactic features, and also a notable amount of
"common-sense" knowledge, which we hypothesize are important features for
recognizing a more diverse set of relations. By extending the GPT to the
distantly supervised setting, and fine-tuning it on the NYT10 dataset, we show
that it predicts a larger set of distinct relation types with high confidence.
Manual and automated evaluation of our model shows that it achieves a
state-of-the-art AUC score of 0.422 on the NYT10 dataset, and performs
especially well at higher recall levels.
</summary>
    <author>
      <name>Christoph Alt</name>
    </author>
    <author>
      <name>Marc H√ºbner</name>
    </author>
    <author>
      <name>Leonhard Hennig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of ACL 2019 (11 pages). arXiv admin note:
  text overlap with arXiv:1906.03088</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.08646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.08646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.07651v1</id>
    <updated>2019-06-18T15:46:08Z</updated>
    <published>2019-06-18T15:46:08Z</published>
    <title>Scheduled Sampling for Transformers</title>
    <summary>  Scheduled sampling is a technique for avoiding one of the known problems in
sequence-to-sequence generation: exposure bias. It consists of feeding the
model a mix of the teacher forced embeddings and the model predictions from the
previous step in training time. The technique has been used for improving the
model performance with recurrent neural networks (RNN). In the Transformer
model, unlike the RNN, the generation of a new word attends to the full
sentence generated so far, not only to the last word, and it is not
straightforward to apply the scheduled sampling technique. We propose some
structural changes to allow scheduled sampling to be applied to Transformer
architecture, via a two-pass decoding strategy. Experiments on two language
pairs achieve performance close to a teacher-forcing baseline and show that
this technique is promising for further exploration.
</summary>
    <author>
      <name>Tsvetomila Mihaylova</name>
    </author>
    <author>
      <name>Andr√© F. T. Martins</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2019 Student Research Workshop</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.07651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.07651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.05963v2</id>
    <updated>2020-01-11T11:03:05Z</updated>
    <published>2019-06-14T00:00:29Z</published>
    <title>Image Captioning: Transforming Objects into Words</title>
    <summary>  Image captioning models typically follow an encoder-decoder architecture
which uses abstract image feature vectors as input to the encoder. One of the
most successful algorithms uses feature vectors extracted from the region
proposals obtained from an object detector. In this work we introduce the
Object Relation Transformer, that builds upon this approach by explicitly
incorporating information about the spatial relationship between input detected
objects through geometric attention. Quantitative and qualitative results
demonstrate the importance of such geometric attention for image captioning,
leading to improvements on all common captioning metrics on the MS-COCO
dataset.
</summary>
    <author>
      <name>Simao Herdade</name>
    </author>
    <author>
      <name>Armin Kappeler</name>
    </author>
    <author>
      <name>Kofi Boakye</name>
    </author>
    <author>
      <name>Joao Soares</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05963v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05963v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.05551v1</id>
    <updated>2019-06-13T08:55:06Z</updated>
    <published>2019-06-13T08:55:06Z</published>
    <title>Lattice Transformer for Speech Translation</title>
    <summary>  Recent advances in sequence modeling have highlighted the strengths of the
transformer architecture, especially in achieving state-of-the-art machine
translation results. However, depending on the up-stream systems, e.g., speech
recognition, or word segmentation, the input to translation system can vary
greatly. The goal of this work is to extend the attention mechanism of the
transformer to naturally consume the lattice in addition to the traditional
sequential input. We first propose a general lattice transformer for speech
translation where the input is the output of the automatic speech recognition
(ASR) which contains multiple paths and posterior scores. To leverage the extra
information from the lattice structure, we develop a novel controllable lattice
attention mechanism to obtain latent representations. On the LDC
Spanish-English speech translation corpus, our experiments show that lattice
transformer generalizes significantly better and outperforms both a transformer
baseline and a lattice LSTM. Additionally, we validate our approach on the WMT
2017 Chinese-English translation task with lattice inputs from different BPE
segmentations. In this task, we also observe the improvements over strong
baselines.
</summary>
    <author>
      <name>Pei Zhang</name>
    </author>
    <author>
      <name>Boxing Chen</name>
    </author>
    <author>
      <name>Niyu Ge</name>
    </author>
    <author>
      <name>Kai Fan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to ACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.05474v2</id>
    <updated>2019-06-18T13:49:35Z</updated>
    <published>2019-06-13T04:07:12Z</published>
    <title>Transfer Learning in Biomedical Natural Language Processing: An
  Evaluation of BERT and ELMo on Ten Benchmarking Datasets</title>
    <summary>  Inspired by the success of the General Language Understanding Evaluation
benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE)
benchmark to facilitate research in the development of pre-training language
representations in the biomedicine domain. The benchmark consists of five tasks
with ten datasets that cover both biomedical and clinical texts with different
dataset sizes and difficulties. We also evaluate several baselines based on
BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and
MIMIC-III clinical notes achieves the best results. We make the datasets,
pre-trained models, and codes publicly available at
https://github.com/ncbi-nlp/BLUE_Benchmark.
</summary>
    <author>
      <name>Yifan Peng</name>
    </author>
    <author>
      <name>Shankai Yan</name>
    </author>
    <author>
      <name>Zhiyong Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by BioNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05474v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05474v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.05317v2</id>
    <updated>2019-06-14T20:13:16Z</updated>
    <published>2019-06-12T18:11:20Z</published>
    <title>COMET: Commonsense Transformers for Automatic Knowledge Graph
  Construction</title>
    <summary>  We present the first comprehensive study on automatic knowledge base
construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et
al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional
KBs that store knowledge with canonical templates, commonsense KBs only store
loosely structured open-text descriptions of knowledge. We posit that an
important step toward automatic commonsense completion is the development of
generative models of commonsense knowledge, and propose COMmonsEnse
Transformers (COMET) that learn to generate rich and diverse commonsense
descriptions in natural language. Despite the challenges of commonsense
modeling, our investigation reveals promising results when implicit knowledge
from deep pre-trained language models is transferred to generate explicit
knowledge in commonsense knowledge graphs. Empirical results demonstrate that
COMET is able to generate novel knowledge that humans rate as high quality,
with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which
approaches human performance for these resources. Our findings suggest that
using generative commonsense models for automatic commonsense KB completion
could soon be a plausible alternative to extractive methods.
</summary>
    <author>
      <name>Antoine Bosselut</name>
    </author>
    <author>
      <name>Hannah Rashkin</name>
    </author>
    <author>
      <name>Maarten Sap</name>
    </author>
    <author>
      <name>Chaitanya Malaviya</name>
    </author>
    <author>
      <name>Asli Celikyilmaz</name>
    </author>
    <author>
      <name>Yejin Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05317v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05317v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.05714v1</id>
    <updated>2019-06-12T15:45:26Z</updated>
    <published>2019-06-12T15:45:26Z</published>
    <title>A Multiscale Visualization of Attention in the Transformer Model</title>
    <summary>  The Transformer is a sequence model that forgoes traditional recurrent
architectures in favor of a fully attention-based approach. Besides improving
performance, an advantage of using attention is that it can also help to
interpret a model by showing how the model assigns weight to different input
elements. However, the multi-layer, multi-head attention mechanism in the
Transformer model can be difficult to decipher. To make the model more
accessible, we introduce an open-source tool that visualizes attention at
multiple scales, each of which provides a unique perspective on the attention
mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three
example use cases: detecting model bias, locating relevant attention heads, and
linking neurons to model behavior.
</summary>
    <author>
      <name>Jesse Vig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACL 2019 (System Demonstrations). arXiv admin note:
  substantial text overlap with arXiv:1904.02679</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.04341v1</id>
    <updated>2019-06-11T01:31:41Z</updated>
    <published>2019-06-11T01:31:41Z</published>
    <title>What Does BERT Look At? An Analysis of BERT's Attention</title>
    <summary>  Large pre-trained neural networks such as BERT have had great recent success
in NLP, motivating a growing body of research investigating what aspects of
language they are able to learn from unlabeled data. Most recent analysis has
focused on model outputs (e.g., language model surprisal) or internal vector
representations (e.g., probing classifiers). Complementary to these works, we
propose methods for analyzing the attention mechanisms of pre-trained models
and apply them to BERT. BERT's attention heads exhibit patterns such as
attending to delimiter tokens, specific positional offsets, or broadly
attending over the whole sentence, with heads in the same layer often
exhibiting similar behaviors. We further show that certain attention heads
correspond well to linguistic notions of syntax and coreference. For example,
we find heads that attend to the direct objects of verbs, determiners of nouns,
objects of prepositions, and coreferent mentions with remarkably high accuracy.
Lastly, we propose an attention-based probing classifier and use it to further
demonstrate that substantial syntactic information is captured in BERT's
attention.
</summary>
    <author>
      <name>Kevin Clark</name>
    </author>
    <author>
      <name>Urvashi Khandelwal</name>
    </author>
    <author>
      <name>Omer Levy</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BlackBoxNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.04341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.04341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03695v1</id>
    <updated>2019-06-09T19:25:27Z</updated>
    <published>2019-06-09T19:25:27Z</published>
    <title>Gendered Pronoun Resolution using BERT and an extractive question
  answering formulation</title>
    <summary>  The resolution of ambiguous pronouns is a longstanding challenge in Natural
Language Understanding. Recent studies have suggested gender bias among
state-of-the-art coreference resolution systems. As an example, Google AI
Language team recently released a gender-balanced dataset and showed that
performance of these coreference resolvers is significantly limited on the
dataset. In this paper, we propose an extractive question answering (QA)
formulation of pronoun resolution task that overcomes this limitation and shows
much lower gender bias (0.99) on their dataset. This system uses fine-tuned
representations from the pre-trained BERT model and outperforms the existing
baseline by a significant margin (22.2% absolute improvement in F1 score)
without using any hand-engineered features. This QA framework is equally
performant even without the knowledge of the candidate antecedents of the
pronoun. An ensemble of QA and BERT-based multiple choice and sequence
classification models further improves the F1 (23.3% absolute improvement upon
the baseline). This ensemble model was submitted to the shared task for the 1st
ACL workshop on Gender Bias for Natural Language Processing. It ranked 9th on
the final official leaderboard. Source code is available at
https://github.com/rakeshchada/corefqa
</summary>
    <author>
      <name>Rakesh Chada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1st ACL workshop on Gender Bias for Natural Language Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.03695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03496v1</id>
    <updated>2019-06-08T18:19:50Z</updated>
    <published>2019-06-08T18:19:50Z</published>
    <title>Making Asynchronous Stochastic Gradient Descent Work for Transformers</title>
    <summary>  Asynchronous stochastic gradient descent (SGD) is attractive from a speed
perspective because workers do not wait for synchronization. However, the
Transformer model converges poorly with asynchronous SGD, resulting in
substantially lower quality compared to synchronous SGD. To investigate why
this is the case, we isolate differences between asynchronous and synchronous
methods to investigate batch size and staleness effects. We find that summing
several asynchronous updates, rather than applying them immediately, restores
convergence behavior. With this hybrid method, Transformer training for neural
machine translation task reaches a near-convergence level 1.36x faster in
single-node multi-GPU training with no impact on model quality.
</summary>
    <author>
      <name>Alham Fikri Aji</name>
    </author>
    <author>
      <name>Kenneth Heafield</name>
    </author>
    <link href="http://arxiv.org/abs/1906.03496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.04165v1</id>
    <updated>2019-06-07T19:50:30Z</updated>
    <published>2019-06-07T19:50:30Z</published>
    <title>Leveraging BERT for Extractive Text Summarization on Lectures</title>
    <summary>  In the last two decades, automatic extractive text summarization on lectures
has demonstrated to be a useful tool for collecting key phrases and sentences
that best represent the content. However, many current approaches utilize dated
approaches, producing sub-par outputs or requiring several hours of manual
tuning to produce meaningful results. Recently, new machine learning
architectures have provided mechanisms for extractive summarization through the
clustering of output embeddings from deep learning models. This paper reports
on the project called Lecture Summarization Service, a python based RESTful
service that utilizes the BERT model for text embeddings and KMeans clustering
to identify sentences closes to the centroid for summary selection. The purpose
of the service was to provide students a utility that could summarize lecture
content, based on their desired number of sentences. On top of the summary
work, the service also includes lecture and summary management, storing content
on the cloud which can be used for collaboration. While the results of
utilizing BERT for extractive summarization were promising, there were still
areas where the model struggled, providing feature research opportunities for
further improvement.
</summary>
    <author>
      <name>Derek Miller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages, First Version</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.04165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.04165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.04284v2</id>
    <updated>2019-06-18T19:42:31Z</updated>
    <published>2019-06-07T13:58:49Z</published>
    <title>Analyzing the Structure of Attention in a Transformer Language Model</title>
    <summary>  The Transformer is a fully attention-based alternative to recurrent networks
that has achieved state-of-the-art results across a range of NLP tasks. In this
paper, we analyze the structure of attention in a Transformer language model,
the GPT-2 small pretrained model. We visualize attention for individual
instances and analyze the interaction between attention and syntax over a large
corpus. We find that attention targets different parts of speech at different
layer depths within the model, and that attention aligns with dependency
relations most strongly in the middle layers. We also find that the deepest
layers of the model capture the most distant relationships. Finally, we extract
exemplar sentences that reveal highly specific patterns targeted by particular
attention heads.
</summary>
    <author>
      <name>Jesse Vig</name>
    </author>
    <author>
      <name>Yonatan Belinkov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACL BlackboxNLP workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.04284v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.04284v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02780v1</id>
    <updated>2019-06-06T19:16:16Z</updated>
    <published>2019-06-06T19:16:16Z</published>
    <title>Syntactically Supervised Transformers for Faster Neural Machine
  Translation</title>
    <summary>  Standard decoders for neural machine translation autoregressively generate a
single target token per time step, which slows inference especially for long
outputs. While architectural advances such as the Transformer fully parallelize
the decoder computations at training time, inference still proceeds
sequentially. Recent developments in non- and semi- autoregressive decoding
produce multiple tokens per time step independently of the others, which
improves inference speed but deteriorates translation quality. In this work, we
propose the syntactically supervised Transformer (SynST), which first
autoregressively predicts a chunked parse tree before generating all of the
target tokens in one shot conditioned on the predicted parse. A series of
controlled experiments demonstrates that SynST decodes sentences ~ 5x faster
than the baseline autoregressive Transformer while achieving higher BLEU scores
than most competing methods on En-De and En-Fr datasets.
</summary>
    <author>
      <name>Nader Akoury</name>
    </author>
    <author>
      <name>Kalpesh Krishna</name>
    </author>
    <author>
      <name>Mohit Iyyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, accepted to ACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.02780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02762v1</id>
    <updated>2019-06-06T18:10:08Z</updated>
    <published>2019-06-06T18:10:08Z</published>
    <title>Understanding and Improving Transformer From a Multi-Particle Dynamic
  System Point of View</title>
    <summary>  The Transformer architecture is widely used in natural language processing.
Despite its success, the design principle of the Transformer remains elusive.
In this paper, we provide a novel perspective towards understanding the
architecture: we show that the Transformer can be mathematically interpreted as
a numerical Ordinary Differential Equation (ODE) solver for a
convection-diffusion equation in a multi-particle dynamic system. In
particular, how words in a sentence are abstracted into contexts by passing
through the layers of the Transformer can be interpreted as approximating
multiple particles' movement in the space using the Lie-Trotter splitting
scheme and the Euler's method. Given this ODE's perspective, the rich
literature of numerical analysis can be brought to guide us in designing
effective structures beyond the Transformer. As an example, we propose to
replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting
scheme, a scheme that is more commonly used and with much lower local
truncation errors. The Strang-Marchuk splitting scheme suggests that the
self-attention and position-wise feed-forward network (FFN) sub-layers should
not be treated equally. Instead, in each layer, two position-wise FFN
sub-layers should be used, and the self-attention sub-layer is placed in
between. This leads to a brand new architecture. Such an FFN-attention-FFN
layer is "Macaron-like", and thus we call the network with this new
architecture the Macaron Net. Through extensive experiments, we show that the
Macaron Net is superior to the Transformer on both supervised and unsupervised
learning tasks. The reproducible codes and pretrained models can be found at
https://github.com/zhuohan123/macaron-net
</summary>
    <author>
      <name>Yiping Lu</name>
    </author>
    <author>
      <name>Zhuohan Li</name>
    </author>
    <author>
      <name>Di He</name>
    </author>
    <author>
      <name>Zhiqing Sun</name>
    </author>
    <author>
      <name>Bin Dong</name>
    </author>
    <author>
      <name>Tao Qin</name>
    </author>
    <author>
      <name>Liwei Wang</name>
    </author>
    <author>
      <name>Tie-Yan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1906.02762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02715v2</id>
    <updated>2019-10-28T17:53:14Z</updated>
    <published>2019-06-06T17:33:22Z</published>
    <title>Visualizing and Measuring the Geometry of BERT</title>
    <summary>  Transformer architectures show significant promise for natural language
processing. Given that a single pretrained model can be fine-tuned to perform
well on many different tasks, these networks appear to extract generally useful
linguistic features. A natural question is how such networks represent this
information internally. This paper describes qualitative and quantitative
investigations of one particularly effective model, BERT. At a high level,
linguistic features seem to be represented in separate semantic and syntactic
subspaces. We find evidence of a fine-grained geometric representation of word
senses. We also present empirical descriptions of syntactic representations in
both attention matrices and individual word embeddings, as well as a
mathematical argument to explain the geometry of these representations.
</summary>
    <author>
      <name>Andy Coenen</name>
    </author>
    <author>
      <name>Emily Reif</name>
    </author>
    <author>
      <name>Ann Yuan</name>
    </author>
    <author>
      <name>Been Kim</name>
    </author>
    <author>
      <name>Adam Pearce</name>
    </author>
    <author>
      <name>Fernanda Vi√©gas</name>
    </author>
    <author>
      <name>Martin Wattenberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.02715v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02715v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01958v1</id>
    <updated>2019-06-05T11:53:38Z</updated>
    <published>2019-06-05T11:53:38Z</published>
    <title>From Balustrades to Pierre Vinken: Looking for Syntax in Transformer
  Self-Attentions</title>
    <summary>  We inspect the multi-head self-attention in Transformer NMT encoders for
three source languages, looking for patterns that could have a syntactic
interpretation. In many of the attention heads, we frequently find sequences of
consecutive states attending to the same position, which resemble syntactic
phrases. We propose a transparent deterministic method of quantifying the
amount of syntactic information present in the self-attentions, based on
automatically building and evaluating phrase-structure trees from the
phrase-like sequences. We compare the resulting trees to existing constituency
treebanks, both manually and by computing precision and recall.
</summary>
    <author>
      <name>David Mareƒçek</name>
    </author>
    <author>
      <name>Rudolf Rosa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at BlackboxNLP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01787v1</id>
    <updated>2019-06-05T02:24:12Z</updated>
    <published>2019-06-05T02:24:12Z</published>
    <title>Learning Deep Transformer Models for Machine Translation</title>
    <summary>  Transformer is the state-of-the-art model in recent machine translation
evaluations. Two strands of research are promising to improve models of this
kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de
facto standard for the development of the Transformer system, and the other
uses deeper language representation but faces the difficulty arising from
learning deep networks. Here, we continue the line of research on the latter.
We claim that a truly deep Transformer model can surpass the Transformer-Big
counterpart by 1) proper use of layer normalization and 2) a novel way of
passing the combination of previous layers to the next. On WMT'16 English-
German, NIST OpenMT'12 Chinese-English and larger WMT'18 Chinese-English tasks,
our deep system (30/25-layer encoder) outperforms the shallow
Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As
another bonus, the deep model is 1.6X smaller in size and 3X faster in training
than Transformer-Big.
</summary>
    <author>
      <name>Qiang Wang</name>
    </author>
    <author>
      <name>Bei Li</name>
    </author>
    <author>
      <name>Tong Xiao</name>
    </author>
    <author>
      <name>Jingbo Zhu</name>
    </author>
    <author>
      <name>Changliang Li</name>
    </author>
    <author>
      <name>Derek F. Wong</name>
    </author>
    <author>
      <name>Lidia S. Chao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01733v1</id>
    <updated>2019-06-04T21:28:31Z</updated>
    <published>2019-06-04T21:28:31Z</published>
    <title>The Unreasonable Effectiveness of Transformer Language Models in
  Grammatical Error Correction</title>
    <summary>  Recent work on Grammatical Error Correction (GEC) has highlighted the
importance of language modeling in that it is certainly possible to achieve
good performance by comparing the probabilities of the proposed edits. At the
same time, advancements in language modeling have managed to generate
linguistic output, which is almost indistinguishable from that of
human-generated text. In this paper, we up the ante by exploring the potential
of more sophisticated language models in GEC and offer some key insights on
their strengths and weaknesses. We show that, in line with recent results in
other NLP tasks, Transformer architectures achieve consistently high
performance and provide a competitive baseline for future machine learning
models.
</summary>
    <author>
      <name>Dimitrios Alikaniotis</name>
    </author>
    <author>
      <name>Vipul Raheja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 tables, accepted at the 14th Workshop on Innovative Use of
  NLP for Building Educational Applications</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01698v1</id>
    <updated>2019-06-04T19:41:10Z</updated>
    <published>2019-06-04T19:41:10Z</published>
    <title>Open Sesame: Getting Inside BERT's Linguistic Knowledge</title>
    <summary>  How and to what extent does BERT encode syntactically-sensitive hierarchical
information or positionally-sensitive linear information? Recent work has shown
that contextual representations like BERT perform well on tasks that require
sensitivity to linguistic structure. We present here two studies which aim to
provide a better understanding of the nature of BERT's representations. The
first of these focuses on the identification of structurally-defined elements
using diagnostic classifiers, while the second explores BERT's representation
of subject-verb agreement and anaphor-antecedent dependencies through a
quantitative assessment of self-attention vectors. In both cases, we find that
BERT encodes positional information about word tokens well on its lower layers,
but switches to a hierarchically-oriented encoding on higher layers. We
conclude then that BERT's representations do indeed model linguistically
relevant aspects of hierarchical structure, though they do not appear to show
the sharp sensitivity to hierarchical structure that is found in human
processing of reflexive anaphora.
</summary>
    <author>
      <name>Yongjie Lin</name>
    </author>
    <author>
      <name>Yi Chern Tan</name>
    </author>
    <author>
      <name>Robert Frank</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proceedings of the 2019 ACL Workshop BlackboxNLP:
  Analyzing and Interpreting Neural Networks for NLP</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01502v1</id>
    <updated>2019-06-04T15:12:47Z</updated>
    <published>2019-06-04T15:12:47Z</published>
    <title>How multilingual is Multilingual BERT?</title>
    <summary>  In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et
al. (2018) as a single language model pre-trained from monolingual corpora in
104 languages, is surprisingly good at zero-shot cross-lingual model transfer,
in which task-specific annotations in one language are used to fine-tune the
model for evaluation in another language. To understand why, we present a large
number of probing experiments, showing that transfer is possible even to
languages in different scripts, that transfer works best between typologically
similar languages, that monolingual corpora can train models for
code-switching, and that the model can find translation pairs. From these
results, we can conclude that M-BERT does create multilingual representations,
but that these representations exhibit systematic deficiencies affecting
certain language pairs.
</summary>
    <author>
      <name>Telmo Pires</name>
    </author>
    <author>
      <name>Eva Schlinger</name>
    </author>
    <author>
      <name>Dan Garrette</name>
    </author>
    <link href="http://arxiv.org/abs/1906.01502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01282v1</id>
    <updated>2019-06-04T08:58:14Z</updated>
    <published>2019-06-04T08:58:14Z</published>
    <title>Lattice-Based Transformer Encoder for Neural Machine Translation</title>
    <summary>  Neural machine translation (NMT) takes deterministic sequences for source
representations. However, either word-level or subword-level segmentations have
multiple choices to split a source sequence with different word segmentors or
different subword vocabulary sizes. We hypothesize that the diversity in
segmentations may affect the NMT performance. To integrate different
segmentations with the state-of-the-art NMT model, Transformer, we propose
lattice-based encoders to explore effective word or subword representation in
an automatic way during training. We propose two methods: 1) lattice positional
encoding and 2) lattice-aware self-attention. These two methods can be used
together and show complementary to each other to further improve translation
performance. Experiment results show superiorities of lattice-based encoders in
word-level and subword-level representations over conventional Transformer
encoder.
</summary>
    <author>
      <name>Fengshun Xiao</name>
    </author>
    <author>
      <name>Jiangtong Li</name>
    </author>
    <author>
      <name>Hai Zhao</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Kehai Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01236v1</id>
    <updated>2019-06-04T07:10:16Z</updated>
    <published>2019-06-04T07:10:16Z</published>
    <title>RTHN: A RNN-Transformer Hierarchical Network for Emotion Cause
  Extraction</title>
    <summary>  The emotion cause extraction (ECE) task aims at discovering the potential
causes behind a certain emotion expression in a document. Techniques including
rule-based methods, traditional machine learning methods and deep neural
networks have been proposed to solve this task. However, most of the previous
work considered ECE as a set of independent clause classification problems and
ignored the relations between multiple clauses in a document. In this work, we
propose a joint emotion cause extraction framework, named RNN-Transformer
Hierarchical Network (RTHN), to encode and classify multiple clauses
synchronously. RTHN is composed of a lower word-level encoder based on RNNs to
encode multiple words in each clause, and an upper clause-level encoder based
on Transformer to learn the correlation between multiple clauses in a document.
We furthermore propose ways to encode the relative position and global
predication information into Transformer that can capture the causality between
clauses and make RTHN more efficient. We finally achieve the best performance
among 12 compared systems and improve the F1 score of the state-of-the-art from
72.69\% to 76.77\%.
</summary>
    <author>
      <name>Rui Xia</name>
    </author>
    <author>
      <name>Mengran Zhang</name>
    </author>
    <author>
      <name>Zixiang Ding</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IJCAI 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01038v1</id>
    <updated>2019-06-03T19:33:13Z</updated>
    <published>2019-06-03T19:33:13Z</published>
    <title>Transforming Complex Sentences into a Semantic Hierarchy</title>
    <summary>  We present an approach for recursively splitting and rephrasing complex
English sentences into a novel semantic hierarchy of simplified sentences, with
each of them presenting a more regular structure that may facilitate a wide
variety of artificial intelligence tasks, such as machine translation (MT) or
information extraction (IE). Using a set of hand-crafted transformation rules,
input sentences are recursively transformed into a two-layered hierarchical
representation in the form of core sentences and accompanying contexts that are
linked via rhetorical relations. In this way, the semantic relationship of the
decomposed constituents is preserved in the output, maintaining its
interpretability for downstream applications. Both a thorough manual analysis
and automatic evaluation across three datasets from two different domains
demonstrate that the proposed syntactic simplification approach outperforms the
state of the art in structural text simplification. Moreover, an extrinsic
evaluation shows that when applying our framework as a preprocessing step the
performance of state-of-the-art Open IE systems can be improved by up to 346%
in precision and 52% in recall. To enable reproducible research, all code is
provided online.
</summary>
    <author>
      <name>Christina Niklaus</name>
    </author>
    <author>
      <name>Matthias Cetto</name>
    </author>
    <author>
      <name>Andre Freitas</name>
    </author>
    <author>
      <name>Siegfried Handschuh</name>
    </author>
    <link href="http://arxiv.org/abs/1906.01038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01161v2</id>
    <updated>2019-06-13T11:26:56Z</updated>
    <published>2019-06-03T11:10:10Z</published>
    <title>Resolving Gendered Ambiguous Pronouns with BERT</title>
    <summary>  Pronoun resolution is part of coreference resolution, the task of pairing an
expression to its referring entity. This is an important task for natural
language understanding and a necessary component of machine translation
systems, chat bots and assistants. Neural machine learning systems perform far
from ideally in this task, reaching as low as 73% F1 scores on modern benchmark
datasets. Moreover, they tend to perform better for masculine pronouns than for
feminine ones. Thus, the problem is both challenging and important for NLP
researchers and practitioners. In this project, we describe our BERT-based
approach to solving the problem of gender-balanced pronoun resolution. We are
able to reach 92% F1 score and a much lower gender bias on the benchmark
dataset shared by Google AI Language team.
</summary>
    <author>
      <name>Matei Ionita</name>
    </author>
    <author>
      <name>Yury Kashnitsky</name>
    </author>
    <author>
      <name>Ken Krige</name>
    </author>
    <author>
      <name>Vladimir Larin</name>
    </author>
    <author>
      <name>Denis Logvinenko</name>
    </author>
    <author>
      <name>Atanas Atanasov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to 1st ACL Workshop on Gender Bias for Natural Language
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01161v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01161v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.00346v2</id>
    <updated>2019-11-27T02:36:05Z</updated>
    <published>2019-06-02T05:11:38Z</published>
    <title>Pre-training of Graph Augmented Transformers for Medication
  Recommendation</title>
    <summary>  Medication recommendation is an important healthcare application. It is
commonly formulated as a temporal prediction task. Hence, most existing works
only utilize longitudinal electronic health records (EHRs) from a small number
of patients with multiple visits ignoring a large number of patients with a
single visit (selection bias). Moreover, important hierarchical knowledge such
as diagnosis hierarchy is not leveraged in the representation learning process.
To address these challenges, we propose G-BERT, a new model to combine the
power of Graph Neural Networks (GNNs) and BERT (Bidirectional Encoder
Representations from Transformers) for medical code representation and
medication recommendation. We use GNNs to represent the internal hierarchical
structures of medical codes. Then we integrate the GNN representation into a
transformer-based visit encoder and pre-train it on EHR data from patients only
with a single visit. The pre-trained visit encoder and representation are then
fine-tuned for downstream predictive tasks on longitudinal EHRs from patients
with multiple visits. G-BERT is the first to bring the language model
pre-training schema into the healthcare domain and it achieved state-of-the-art
performance on the medication recommendation task.
</summary>
    <author>
      <name>Junyuan Shang</name>
    </author>
    <author>
      <name>Tengfei Ma</name>
    </author>
    <author>
      <name>Cao Xiao</name>
    </author>
    <author>
      <name>Jimeng Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI2019; fix some undefined problems; provide more intuitive
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.00346v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00346v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.00295v1</id>
    <updated>2019-06-01T21:29:20Z</updated>
    <published>2019-06-01T21:29:20Z</published>
    <title>Multimodal Transformer for Unaligned Multimodal Language Sequences</title>
    <summary>  Human language is often multimodal, which comprehends a mixture of natural
language, facial gestures, and acoustic behaviors. However, two major
challenges in modeling such multimodal human language time-series data exist:
1) inherent data non-alignment due to variable sampling rates for the sequences
from each modality; and 2) long-range dependencies between elements across
modalities. In this paper, we introduce the Multimodal Transformer (MulT) to
generically address the above issues in an end-to-end manner without explicitly
aligning the data. At the heart of our model is the directional pairwise
crossmodal attention, which attends to interactions between multimodal
sequences across distinct time steps and latently adapt streams from one
modality to another. Comprehensive experiments on both aligned and non-aligned
multimodal time-series show that our model outperforms state-of-the-art methods
by a large margin. In addition, empirical analysis suggests that correlated
crossmodal signals are able to be captured by the proposed crossmodal attention
mechanism in MulT.
</summary>
    <author>
      <name>Yao-Hung Hubert Tsai</name>
    </author>
    <author>
      <name>Shaojie Bai</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
    <author>
      <name>J. Zico Kolter</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <link href="http://arxiv.org/abs/1906.00295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.00138v1</id>
    <updated>2019-06-01T03:05:31Z</updated>
    <published>2019-06-01T03:05:31Z</published>
    <title>Efficient Adaptation of Pretrained Transformers for Abstractive
  Summarization</title>
    <summary>  Large-scale learning of transformer language models has yielded improvements
on a variety of natural language understanding tasks. Whether they can be
effectively adapted for summarization, however, has been less explored, as the
learned representations are less seamlessly integrated into existing neural
text production architectures. In this work, we propose two solutions for
efficiently adapting pretrained transformer language models as text
summarizers: source embeddings and domain-adaptive training. We test these
solutions on three abstractive summarization datasets, achieving new state of
the art performance on two of them. Finally, we show that these improvements
are achieved by producing more focused summaries with fewer superfluous and
that performance improvements are more pronounced on more abstractive datasets.
</summary>
    <author>
      <name>Andrew Hoang</name>
    </author>
    <author>
      <name>Antoine Bosselut</name>
    </author>
    <author>
      <name>Asli Celikyilmaz</name>
    </author>
    <author>
      <name>Yejin Choi</name>
    </author>
    <link href="http://arxiv.org/abs/1906.00138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.13164v1</id>
    <updated>2019-05-30T16:49:11Z</updated>
    <published>2019-05-30T16:49:11Z</published>
    <title>Hierarchical Transformers for Multi-Document Summarization</title>
    <summary>  In this paper, we develop a neural summarization model which can effectively
process multiple input documents and distill Transformer architecture with the
ability to encode documents in a hierarchical manner. We represent
cross-document relationships via an attention mechanism which allows to share
information as opposed to simply concatenating text spans and processing them
as a flat sequence. Our model learns latent dependencies among textual units,
but can also take advantage of explicit graph representations focusing on
similarity or discourse relations. Empirical results on the WikiSum dataset
demonstrate that the proposed architecture brings substantial improvements over
several strong baselines.
</summary>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Mirella Lapata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear at ACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.13164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.13164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.13068v2</id>
    <updated>2019-06-29T16:42:14Z</updated>
    <published>2019-05-30T14:17:39Z</published>
    <title>Unbabel's Submission to the WMT2019 APE Shared Task: BERT-based
  Encoder-Decoder for Automatic Post-Editing</title>
    <summary>  This paper describes Unbabel's submission to the WMT2019 APE Shared Task for
the English-German language pair. Following the recent rise of large, powerful,
pre-trained models, we adapt the BERT pretrained model to perform Automatic
Post-Editing in an encoder-decoder framework. Analogously to dual-encoder
architectures we develop a BERT-based encoder-decoder (BED) model in which a
single pretrained BERT encoder receives both the source src and machine
translation tgt strings. Furthermore, we explore a conservativeness factor to
constrain the APE system to perform fewer edits. As the official results show,
when trained on a weighted combination of in-domain and artificial training
data, our BED system with the conservativeness penalty improves significantly
the translations of a strong Neural Machine Translation system by $-0.78$ and
$+1.23$ in terms of TER and BLEU, respectively. Finally, our submission
achieves a new state-of-the-art, ex-aequo, in English-German APE of NMT.
</summary>
    <author>
      <name>Ant√≥nio V. Lopes</name>
    </author>
    <author>
      <name>M. Amin Farajian</name>
    </author>
    <author>
      <name>Gon√ßalo M. Correia</name>
    </author>
    <author>
      <name>Jonay Trenous</name>
    </author>
    <author>
      <name>Andr√© F. T. Martins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated sections 2.2 and 4</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.13068v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.13068v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.12848v1</id>
    <updated>2019-05-30T04:28:19Z</updated>
    <published>2019-05-30T04:28:19Z</published>
    <title>A Simple but Effective Method to Incorporate Multi-turn Context with
  BERT for Conversational Machine Comprehension</title>
    <summary>  Conversational machine comprehension (CMC) requires understanding the context
of multi-turn dialogue. Using BERT, a pre-training language model, has been
successful for single-turn machine comprehension, while modeling multiple turns
of question answering with BERT has not been established because BERT has a
limit on the number and the length of input sequences. In this paper, we
propose a simple but effective method with BERT for CMC. Our method uses BERT
to encode a paragraph independently conditioned with each question and each
answer in a multi-turn context. Then, the method predicts an answer on the
basis of the paragraph representations encoded with BERT. The experiments with
representative CMC datasets, QuAC and CoQA, show that our method outperformed
recently published methods (+0.8 F1 on QuAC and +2.1 F1 on CoQA). In addition,
we conducted a detailed analysis of the effects of the number and types of
dialogue history on the accuracy of CMC, and we found that the gold answer
history, which may not be given in an actual conversation, contributed to the
model performance most on both datasets.
</summary>
    <author>
      <name>Yasuhito Ohsugi</name>
    </author>
    <author>
      <name>Itsumi Saito</name>
    </author>
    <author>
      <name>Kyosuke Nishida</name>
    </author>
    <author>
      <name>Hisako Asano</name>
    </author>
    <author>
      <name>Junji Tomita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACL 2019 Workshop on NLP for Conversational AI
  (NLP4ConvAI)</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.12848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.12848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.11006v2</id>
    <updated>2019-10-28T07:52:19Z</updated>
    <published>2019-05-27T07:08:12Z</published>
    <title>Levenshtein Transformer</title>
    <summary>  Modern neural sequence generation models are built to either generate tokens
step-by-step from scratch or (iteratively) modify a sequence of tokens bounded
by a fixed length. In this work, we develop Levenshtein Transformer, a new
partially autoregressive model devised for more flexible and amenable sequence
generation. Unlike previous approaches, the atomic operations of our model are
insertion and deletion. The combination of them facilitates not only generation
but also sequence refinement allowing dynamic length changes. We also propose a
set of new training techniques dedicated at them, effectively exploiting one as
the other's learning signal thanks to their complementary nature. Experiments
applying the proposed model achieve comparable performance but much-improved
efficiency on both generation (e.g. machine translation, text summarization)
and refinement tasks (e.g. automatic post-editing). We further confirm the
flexibility of our model by showing a Levenshtein Transformer trained by
machine translation can straightforwardly be used for automatic post-editing.
</summary>
    <author>
      <name>Jiatao Gu</name>
    </author>
    <author>
      <name>Changhan Wang</name>
    </author>
    <author>
      <name>Jake Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages (6 pages appendix). Camera ready, accepted by NeurIPS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.11006v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.11006v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.08836v1</id>
    <updated>2019-05-21T19:13:16Z</updated>
    <published>2019-05-21T19:13:16Z</published>
    <title>Sample Efficient Text Summarization Using a Single Pre-Trained
  Transformer</title>
    <summary>  Language model (LM) pre-training has resulted in impressive performance and
sample efficiency on a variety of language understanding tasks. However, it
remains unclear how to best use pre-trained LMs for generation tasks such as
abstractive summarization, particularly to enhance sample efficiency. In these
sequence-to-sequence settings, prior work has experimented with loading
pre-trained weights into the encoder and/or decoder networks, but used
non-pre-trained encoder-decoder attention weights. We instead use a pre-trained
decoder-only network, where the same Transformer LM both encodes the source and
generates the summary. This ensures that all parameters in the network,
including those governing attention over source states, have been pre-trained
before the fine-tuning step. Experiments on the CNN/Daily Mail dataset show
that our pre-trained Transformer LM substantially improves over pre-trained
Transformer encoder-decoder networks in limited-data settings. For instance, it
achieves 13.1 ROUGE-2 using only 1% of the training data (~3000 examples),
while pre-trained encoder-decoder models score 2.3 ROUGE-2.
</summary>
    <author>
      <name>Urvashi Khandelwal</name>
    </author>
    <author>
      <name>Kevin Clark</name>
    </author>
    <author>
      <name>Dan Jurafsky</name>
    </author>
    <author>
      <name>Lukasz Kaiser</name>
    </author>
    <link href="http://arxiv.org/abs/1905.08836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.08454v1</id>
    <updated>2019-05-21T06:12:47Z</updated>
    <published>2019-05-21T06:12:47Z</published>
    <title>A Seq-to-Seq Transformer Premised Temporal Convolutional Network for
  Chinese Word Segmentation</title>
    <summary>  The prevalent approaches of Chinese word segmentation task almost rely on the
Bi-LSTM neural network. However, the methods based the Bi-LSTM have some
inherent drawbacks: hard to parallel computing, little efficient in applying
the Dropout method to inhibit the Overfitting and little efficient in capturing
the character information at the more distant site of a long sentence for the
word segmentation task. In this work, we propose a sequence-to-sequence
transformer model for Chinese word segmentation, which is premised a type of
convolutional neural network named temporal convolutional network. The model
uses the temporal convolutional network to construct an encoder, and uses one
layer of fully-connected neural network to build a decoder, and applies the
Dropout method to inhibit the Overfitting, and captures the character
information at the distant site of a sentence by adding the layers of the
encoder, and binds Conditional Random Fields model to train parameters, and
uses the Viterbi algorithm to infer the final result of the Chinese word
segmentation. The experiments on traditional Chinese corpora and simplified
Chinese corpora show that the performance of Chinese word segmentation of the
model is equivalent to the performance of the methods based the Bi-LSTM, and
the model has a tremendous growth in parallel computing than the models based
the Bi-LSTM.
</summary>
    <author>
      <name>Wei Jiang</name>
    </author>
    <author>
      <name>Yan Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1905.08454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.07504v2</id>
    <updated>2019-05-21T02:37:11Z</updated>
    <published>2019-05-17T23:52:08Z</published>
    <title>Story Ending Prediction by Transferable BERT</title>
    <summary>  Recent advances, such as GPT and BERT, have shown success in incorporating a
pre-trained transformer language model and fine-tuning operation to improve
downstream NLP systems. However, this framework still has some fundamental
problems in effectively incorporating supervised knowledge from other related
tasks. In this study, we investigate a transferable BERT (TransBERT) training
framework, which can transfer not only general language knowledge from
large-scale unlabeled data but also specific kinds of knowledge from various
semantically related supervised tasks, for a target task. Particularly, we
propose utilizing three kinds of transfer tasks, including natural language
inference, sentiment classification, and next action prediction, to further
train BERT based on a pre-trained model. This enables the model to get a better
initialization for the target task. We take story ending prediction as the
target task to conduct experiments. The final result, an accuracy of 91.8%,
dramatically outperforms previous state-of-the-art baseline methods. Several
comparative experiments give some helpful suggestions on how to select transfer
tasks. Error analysis shows what are the strength and weakness of BERT-based
models for story ending prediction.
</summary>
    <author>
      <name>Zhongyang Li</name>
    </author>
    <author>
      <name>Xiao Ding</name>
    </author>
    <author>
      <name>Ting Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted and to appear in IJCAI 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.07504v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07504v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.07213v1</id>
    <updated>2019-05-17T11:39:21Z</updated>
    <published>2019-05-17T11:39:21Z</published>
    <title>Adaptation of Deep Bidirectional Multilingual Transformers for Russian
  Language</title>
    <summary>  The paper introduces methods of adaptation of multilingual masked language
models for a specific language. Pre-trained bidirectional language models show
state-of-the-art performance on a wide range of tasks including reading
comprehension, natural language inference, and sentiment analysis. At the
moment there are two alternative approaches to train such models: monolingual
and multilingual. While language specific models show superior performance,
multilingual models allow to perform a transfer from one language to another
and solve tasks for different languages simultaneously. This work shows that
transfer learning from a multilingual model to monolingual model results in
significant growth of performance on such tasks as reading comprehension,
paraphrase detection, and sentiment analysis. Furthermore, multilingual
initialization of monolingual model substantially reduces training time.
Pre-trained models for the Russian language are open sourced.
</summary>
    <author>
      <name>Yuri Kuratov</name>
    </author>
    <author>
      <name>Mikhail Arkhipov</name>
    </author>
    <link href="http://arxiv.org/abs/1905.07213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.06638v1</id>
    <updated>2019-05-16T10:21:51Z</updated>
    <published>2019-05-16T10:21:51Z</published>
    <title>Latent Universal Task-Specific BERT</title>
    <summary>  This paper describes a language representation model which combines the
Bidirectional Encoder Representations from Transformers (BERT) learning
mechanism described in Devlin et al. (2018) with a generalization of the
Universal Transformer model described in Dehghani et al. (2018). We further
improve this model by adding a latent variable that represents the persona and
topics of interests of the writer for each training example. We also describe a
simple method to improve the usefulness of our language representation for
solving problems in a specific domain at the expense of its ability to
generalize to other fields. Finally, we release a pre-trained language
representation model for social texts that was trained on 100 million tweets.
</summary>
    <author>
      <name>Alon Rozental</name>
    </author>
    <author>
      <name>Zohar Kelrich</name>
    </author>
    <author>
      <name>Daniel Fleischer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.06638v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06638v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.06566v1</id>
    <updated>2019-05-16T07:20:21Z</updated>
    <published>2019-05-16T07:20:21Z</published>
    <title>HIBERT: Document Level Pre-training of Hierarchical Bidirectional
  Transformers for Document Summarization</title>
    <summary>  Neural extractive summarization models usually employ a hierarchical encoder
for document encoding and they are trained using sentence-level labels, which
are created heuristically using rule-based methods. Training the hierarchical
encoder with these \emph{inaccurate} labels is challenging. Inspired by the
recent work on pre-training transformer sentence encoders
\cite{devlin:2018:arxiv}, we propose {\sc Hibert} (as shorthand for {\bf
HI}erachical {\bf B}idirectional {\bf E}ncoder {\bf R}epresentations from {\bf
T}ransformers) for document encoding and a method to pre-train it using
unlabeled data. We apply the pre-trained {\sc Hibert} to our summarization
model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on
the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times
dataset. We also achieve the state-of-the-art performance on these two
datasets.
</summary>
    <author>
      <name>Xingxing Zhang</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <author>
      <name>Ming Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in ACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.06566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.05950v2</id>
    <updated>2019-08-09T15:51:47Z</updated>
    <published>2019-05-15T05:47:23Z</published>
    <title>BERT Rediscovers the Classical NLP Pipeline</title>
    <summary>  Pre-trained text encoders have rapidly advanced the state of the art on many
NLP tasks. We focus on one such model, BERT, and aim to quantify where
linguistic information is captured within the network. We find that the model
represents the steps of the traditional NLP pipeline in an interpretable and
localizable way, and that the regions responsible for each step appear in the
expected sequence: POS tagging, parsing, NER, semantic roles, then coreference.
Qualitative analysis reveals that the model can and often does adjust this
pipeline dynamically, revising lower-level decisions on the basis of
disambiguating information from higher-level representations.
</summary>
    <author>
      <name>Ian Tenney</name>
    </author>
    <author>
      <name>Dipanjan Das</name>
    </author>
    <author>
      <name>Ellie Pavlick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at ACL 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics (2019) 4593-4601</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.05950v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.05950v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.05621v3</id>
    <updated>2019-08-20T12:49:12Z</updated>
    <published>2019-05-14T14:03:21Z</published>
    <title>Style Transformer: Unpaired Text Style Transfer without Disentangled
  Latent Representation</title>
    <summary>  Disentangling the content and style in the latent space is prevalent in
unpaired text style transfer. However, two major issues exist in most of the
current neural models. 1) It is difficult to completely strip the style
information from the semantics for a sentence. 2) The recurrent neural network
(RNN) based encoder and decoder, mediated by the latent representation, cannot
well deal with the issue of the long-term dependency, resulting in poor
preservation of non-stylistic semantic content. In this paper, we propose the
Style Transformer, which makes no assumption about the latent representation of
source sentence and equips the power of attention mechanism in Transformer to
achieve better style transfer and better content preservation.
</summary>
    <author>
      <name>Ning Dai</name>
    </author>
    <author>
      <name>Jianze Liang</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.05621v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.05621v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.05583v3</id>
    <updated>2020-02-05T12:00:32Z</updated>
    <published>2019-05-14T13:17:26Z</published>
    <title>How to Fine-Tune BERT for Text Classification?</title>
    <summary>  Language model pre-training has proven to be useful in learning universal
language representations. As a state-of-the-art language model pre-training
model, BERT (Bidirectional Encoder Representations from Transformers) has
achieved amazing results in many language understanding tasks. In this paper,
we conduct exhaustive experiments to investigate different fine-tuning methods
of BERT on text classification task and provide a general solution for BERT
fine-tuning. Finally, the proposed solution obtains new state-of-the-art
results on eight widely-studied text classification datasets.
</summary>
    <author>
      <name>Chi Sun</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Yige Xu</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1905.05583v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.05583v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02124v2</id>
    <updated>2019-07-01T01:48:38Z</updated>
    <published>2019-05-14T10:33:08Z</published>
    <title>PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT
  Model</title>
    <summary>  In this work we focus on fine-tuning a pre-trained BERT model and applying it
to patent classification. When applied to large datasets of over two millions
patents, our approach outperforms the state of the art by an approach using CNN
with word embeddings. In addition, we focus on patent claims without other
parts in patent documents. Our contributions include: (1) a new
state-of-the-art method based on pre-trained BERT model and fine-tuning for
patent classification, (2) a large dataset USPTO-3M at the CPC subclass level
with SQL statements that can be used by future researchers, (3) showing that
patent claims alone are sufficient for classification task, in contrast to
conventional wisdom.
</summary>
    <author>
      <name>Jieh-Sheng Lee</name>
    </author>
    <author>
      <name>Jieh Hsiang</name>
    </author>
    <link href="http://arxiv.org/abs/1906.02124v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02124v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.05615v1</id>
    <updated>2019-05-13T03:18:38Z</updated>
    <published>2019-05-13T03:18:38Z</published>
    <title>Transfer Learning for Scientific Data Chain Extraction in Small Chemical
  Corpus with BERT-CRF Model</title>
    <summary>  Computational chemistry develops fast in recent years due to the rapid growth
and breakthroughs in AI. Thanks for the progress in natural language
processing, researchers can extract more fine-grained knowledge in publications
to stimulate the development in computational chemistry. While the works and
corpora in chemical entity extraction have been restricted in the biomedicine
or life science field instead of the chemistry field, we build a new corpus in
chemical bond field annotated for 7 types of entities: compound, solvent,
method, bond, reaction, pKa and pKa value. This paper presents a novel BERT-CRF
model to build scientific chemical data chains by extracting 7 chemical
entities and relations from publications. And we propose a joint model to
extract the entities and relations simultaneously. Experimental results on our
Chemical Special Corpus demonstrate that we achieve state-of-art and
competitive NER performance.
</summary>
    <author>
      <name>Na Pang</name>
    </author>
    <author>
      <name>Li Qian</name>
    </author>
    <author>
      <name>Weimin Lyu</name>
    </author>
    <author>
      <name>Jin-Dong Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1905.05615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.05615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.04226v2</id>
    <updated>2019-07-11T15:45:32Z</updated>
    <published>2019-05-10T15:50:00Z</published>
    <title>Language Modeling with Deep Transformers</title>
    <summary>  We explore deep autoregressive Transformer models in language modeling for
speech recognition. We focus on two aspects. First, we revisit Transformer
model configurations specifically for language modeling. We show that well
configured Transformer models outperform our baseline models based on the
shallow stack of LSTM recurrent neural network layers. We carry out experiments
on the open-source LibriSpeech 960hr task, for both 200K vocabulary word-level
and 10K byte-pair encoding subword-level language modeling. We apply our
word-level models to conventional hybrid speech recognition by lattice
rescoring, and the subword-level models to attention based encoder-decoder
models by shallow fusion. Second, we show that deep Transformer language models
do not require positional encoding. The positional encoding is an essential
augmentation for the self-attention mechanism which is invariant to sequence
ordering. However, in autoregressive setup, as is the case for language
modeling, the amount of information increases along the position dimension,
which is a positional signal by its own. The analysis of attention weights
shows that deep autoregressive self-attention models can automatically make use
of such positional information. We find that removing the positional encoding
even slightly improves the performance of these models.
</summary>
    <author>
      <name>Kazuki Irie</name>
    </author>
    <author>
      <name>Albert Zeyer</name>
    </author>
    <author>
      <name>Ralf Schl√ºter</name>
    </author>
    <author>
      <name>Hermann Ney</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21437/Interspeech.2019-2225</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21437/Interspeech.2019-2225" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the proceedings of INTERSPEECH 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.04226v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.04226v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.02851v2</id>
    <updated>2019-05-24T00:14:18Z</updated>
    <published>2019-05-08T00:33:37Z</published>
    <title>FAQ Retrieval using Query-Question Similarity and BERT-Based
  Query-Answer Relevance</title>
    <summary>  Frequently Asked Question (FAQ) retrieval is an important task where the
objective is to retrieve an appropriate Question-Answer (QA) pair from a
database based on a user's query. We propose a FAQ retrieval system that
considers the similarity between a user's query and a question as well as the
relevance between the query and an answer. Although a common approach to FAQ
retrieval is to construct labeled data for training, it takes annotation costs.
Therefore, we use a traditional unsupervised information retrieval system to
calculate the similarity between the query and question. On the other hand, the
relevance between the query and answer can be learned by using QA pairs in a
FAQ database. The recently-proposed BERT model is used for the relevance
calculation. Since the number of QA pairs in FAQ page is not enough to train a
model, we cope with this issue by leveraging FAQ sets that are similar to the
one in question. We evaluate our approach on two datasets. The first one is
localgovFAQ, a dataset we construct in a Japanese administrative municipality
domain. The second is StackExchange dataset, which is the public dataset in
English. We demonstrate that our proposed method outperforms baseline methods
on these datasets.
</summary>
    <author>
      <name>Wataru Sakata</name>
    </author>
    <author>
      <name>Tomohide Shibata</name>
    </author>
    <author>
      <name>Ribeka Tanaka</name>
    </author>
    <author>
      <name>Sadao Kurohashi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in SIGIR 2019 (short paper), camera ready, 4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.02851v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02851v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01780v2</id>
    <updated>2019-06-11T14:46:27Z</updated>
    <published>2019-05-06T01:16:33Z</published>
    <title>Anonymized BERT: An Augmentation Approach to the Gendered Pronoun
  Resolution Challenge</title>
    <summary>  We present our 7th place solution to the Gendered Pronoun Resolution
challenge, which uses BERT without fine-tuning and a novel augmentation
strategy designed for contextual embedding token-level tasks. Our method
anonymizes the referent by replacing candidate names with a set of common
placeholder names. Besides the usual benefits of effectively increasing
training data size, this approach diversifies idiosyncratic information
embedded in names. Using same set of common first names can also help the model
recognize names better, shorten token length, and remove gender and regional
biases associated with names. The system scored 0.1947 log loss in stage 2,
where the augmentation contributed to an improvements of 0.04. Post-competition
analysis shows that, when using different embedding layers, the system scores
0.1799 which would be third place.
</summary>
    <author>
      <name>Bo Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; accepted by 1st ACL Workshop on Gender Bias for NLP at ACL
  2019; code is at https://github.com/boliu61/gendered-pronoun-resolution</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.01780v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01780v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01758v1</id>
    <updated>2019-05-05T22:16:08Z</updated>
    <published>2019-05-05T22:16:08Z</published>
    <title>Investigating the Successes and Failures of BERT for Passage Re-Ranking</title>
    <summary>  The bidirectional encoder representations from transformers (BERT) model has
recently advanced the state-of-the-art in passage re-ranking. In this paper, we
analyze the results produced by a fine-tuned BERT model to better understand
the reasons behind such substantial improvements. To this aim, we focus on the
MS MARCO passage re-ranking dataset and provide potential reasons for the
successes and failures of BERT for retrieval. In more detail, we empirically
study a set of hypotheses and provide additional analysis to explain the
successful performance of BERT.
</summary>
    <author>
      <name>Harshith Padigela</name>
    </author>
    <author>
      <name>Hamed Zamani</name>
    </author>
    <author>
      <name>W. Bruce Croft</name>
    </author>
    <link href="http://arxiv.org/abs/1905.01758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.11660v2</id>
    <updated>2020-03-02T23:25:25Z</updated>
    <published>2019-04-26T03:00:19Z</published>
    <title>Transformers with convolutional context for ASR</title>
    <summary>  The recent success of transformer networks for neural machine translation and
other NLP tasks has led to a surge in research work trying to apply it for
speech recognition. Recent efforts studied key research questions around ways
of combining positional embedding with speech features, and stability of
optimization for large scale learning of transformer networks. In this paper,
we propose replacing the sinusoidal positional embedding for transformers with
convolutionally learned input representations. These contextual representations
provide subsequent transformer blocks with relative positional information
needed for discovering long-range relationships between local concepts. The
proposed system has favorable optimization characteristics where our reported
results are produced with fixed learning rate of 1.0 and no warmup steps. The
proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech
``test clean'' and ``test other'' subsets when no extra LM text is provided.
</summary>
    <author>
      <name>Abdelrahman Mohamed</name>
    </author>
    <author>
      <name>Dmytro Okhonko</name>
    </author>
    <author>
      <name>Luke Zettlemoyer</name>
    </author>
    <link href="http://arxiv.org/abs/1904.11660v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.11660v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.10610v1</id>
    <updated>2019-04-24T02:26:48Z</updated>
    <published>2019-04-24T02:26:48Z</published>
    <title>Condition-Transforming Variational AutoEncoder for Conversation Response
  Generation</title>
    <summary>  This paper proposes a new model, called condition-transforming variational
autoencoder (CTVAE), to improve the performance of conversation response
generation using conditional variational autoencoders (CVAEs). In conventional
CVAEs , the prior distribution of latent variable z follows a multivariate
Gaussian distribution with mean and variance modulated by the input conditions.
Previous work found that this distribution tends to become condition
independent in practical application. In our proposed CTVAE model, the latent
variable z is sampled by performing a non-lineartransformation on the
combination of the input conditions and the samples from a
condition-independent prior distribution N (0; I). In our objective
evaluations, the CTVAE model outperforms the CVAE model on fluency metrics and
surpasses a sequence-to-sequence (Seq2Seq) model on diversity metrics. In
subjective preference tests, our proposed CTVAE model performs significantly
better than CVAE and Seq2Seq models on generating fluency, informative and
topic relevant responses.
</summary>
    <author>
      <name>Yu-Ping Ruan</name>
    </author>
    <author>
      <name>Zhen-Hua Ling</name>
    </author>
    <author>
      <name>Quan Liu</name>
    </author>
    <author>
      <name>Zhigang Chen</name>
    </author>
    <author>
      <name>Nitin Indurkhya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2019, oral</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.10610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.10610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01969v3</id>
    <updated>2020-02-12T20:07:00Z</updated>
    <published>2019-04-22T02:18:00Z</published>
    <title>Poly-encoders: Transformer Architectures and Pre-training Strategies for
  Fast and Accurate Multi-sentence Scoring</title>
    <summary>  The use of deep pre-trained bidirectional transformers has led to remarkable
progress in a number of applications (Devlin et al., 2018). For tasks that make
pairwise comparisons between sequences, matching a given input with a
corresponding label, two approaches are common: Cross-encoders performing full
self-attention over the pair and Bi-encoders encoding the pair separately. The
former often performs better, but is too slow for practical use. In this work,
we develop a new transformer architecture, the Poly-encoder, that learns global
rather than token level self-attention features. We perform a detailed
comparison of all three approaches, including what pre-training and fine-tuning
strategies work best. We show our models achieve state-of-the-art results on
three existing tasks; that Poly-encoders are faster than Cross-encoders and
more accurate than Bi-encoders; and that the best results are obtained by
pre-training on large datasets similar to the downstream tasks.
</summary>
    <author>
      <name>Samuel Humeau</name>
    </author>
    <author>
      <name>Kurt Shuster</name>
    </author>
    <author>
      <name>Marie-Anne Lachaux</name>
    </author>
    <author>
      <name>Jason Weston</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.01969v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01969v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.09675v3</id>
    <updated>2020-02-24T18:59:28Z</updated>
    <published>2019-04-21T23:08:53Z</published>
    <title>BERTScore: Evaluating Text Generation with BERT</title>
    <summary>  We propose BERTScore, an automatic evaluation metric for text generation.
Analogously to common metrics, BERTScore computes a similarity score for each
token in the candidate sentence with each token in the reference sentence.
However, instead of exact matches, we compute token similarity using contextual
embeddings. We evaluate using the outputs of 363 machine translation and image
captioning systems. BERTScore correlates better with human judgments and
provides stronger model selection performance than existing metrics. Finally,
we use an adversarial paraphrase detection task to show that BERTScore is more
robust to challenging examples when compared to existing metrics.
</summary>
    <author>
      <name>Tianyi Zhang</name>
    </author>
    <author>
      <name>Varsha Kishore</name>
    </author>
    <author>
      <name>Felix Wu</name>
    </author>
    <author>
      <name>Kilian Q. Weinberger</name>
    </author>
    <author>
      <name>Yoav Artzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code available at https://github.com/Tiiiger/bert_score; To appear in
  ICLR2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.09675v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09675v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.09408v2</id>
    <updated>2019-10-17T04:25:15Z</updated>
    <published>2019-04-20T06:43:14Z</published>
    <title>Language Models with Transformers</title>
    <summary>  The Transformer architecture is superior to RNN-based models in computational
efficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer
models on various NLP tasks using pre-trained language models on large-scale
corpora. Surprisingly, these Transformer architectures are suboptimal for
language model itself. Neither self-attention nor the positional encoding in
the Transformer is able to efficiently incorporate the word-level sequential
context crucial to language modeling.
  In this paper, we explore effective Transformer architectures for language
model, including adding additional LSTM layers to better capture the sequential
context while still keeping the computation efficient. We propose Coordinate
Architecture Search (CAS) to find an effective architecture through iterative
refinement of the model. Experimental results on the PTB, WikiText-2, and
WikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all
problems, i.e. on average an improvement of 12.0 perplexity units compared to
state-of-the-art LSTMs. The source code is publicly available.
</summary>
    <author>
      <name>Chenguang Wang</name>
    </author>
    <author>
      <name>Mu Li</name>
    </author>
    <author>
      <name>Alexander J. Smola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 tables, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.09408v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09408v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.09077v2</id>
    <updated>2019-10-03T17:22:55Z</updated>
    <published>2019-04-19T04:45:44Z</published>
    <title>Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</title>
    <summary>  Pretrained contextual representation models (Peters et al., 2018; Devlin et
al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new
release of BERT (Devlin, 2018) includes a model simultaneously pretrained on
104 languages with impressive performance for zero-shot cross-lingual transfer
on a natural language inference task. This paper explores the broader
cross-lingual potential of mBERT (multilingual) as a zero shot language
transfer model on 5 NLP tasks covering a total of 39 languages from various
language families: NLI, document classification, NER, POS tagging, and
dependency parsing. We compare mBERT with the best-published methods for
zero-shot cross-lingual transfer and find mBERT competitive on each task.
Additionally, we investigate the most effective strategy for utilizing mBERT in
this manner, determine to what extent mBERT generalizes away from language
specific features, and measure factors that influence cross-lingual transfer.
</summary>
    <author>
      <name>Shijie Wu</name>
    </author>
    <author>
      <name>Mark Dredze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2019 Camera Ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.09077v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09077v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08398v3</id>
    <updated>2019-08-22T05:09:47Z</updated>
    <published>2019-04-17T17:55:18Z</published>
    <title>DocBERT: BERT for Document Classification</title>
    <summary>  We present, to our knowledge, the first application of BERT to document
classification. A few characteristics of the task might lead one to think that
BERT is not the most appropriate model: syntactic structures matter less for
content categories, documents can often be longer than typical BERT input, and
documents often have multiple labels. Nevertheless, we show that a
straightforward classification model using BERT is able to achieve the state of
the art across four popular datasets. To address the computational expense
associated with BERT inference, we distill knowledge from BERT-large to small
bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x
fewer parameters. The primary contribution of our paper is improved baselines
that can provide the foundation for future work.
</summary>
    <author>
      <name>Ashutosh Adhikari</name>
    </author>
    <author>
      <name>Achyudh Ram</name>
    </author>
    <author>
      <name>Raphael Tang</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1904.08398v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08398v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08850v1</id>
    <updated>2019-04-17T14:21:43Z</updated>
    <published>2019-04-17T14:21:43Z</published>
    <title>True Parallel Graph Transformations: an Algebraic Approach Based on Weak
  Spans</title>
    <summary>  We address the problem of defining graph transformations by the simultaneous
application of direct transformations even when these cannot be applied
independently of each other. An algebraic approach is adopted, with production
rules of the form $L\xleftarrow{l}K \xleftarrow{i} I \xrightarrow{r} R$, called
weak spans. A parallel coherent transformation is introduced and shown to be a
conservative extension of the interleaving semantics of parallel independent
direct transformations. A categorical construction of finitely attributed
structures is proposed, in which parallel coherent transformations can be built
in a natural way. These notions are introduced and illustrated on detailed
examples.
</summary>
    <author>
      <name>Thierry Boy de la Tour</name>
    </author>
    <author>
      <name>Rachid Echahed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.08850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.07531v4</id>
    <updated>2019-04-26T12:44:38Z</updated>
    <published>2019-04-16T08:30:31Z</published>
    <title>Understanding the Behaviors of BERT in Ranking</title>
    <summary>  This paper studies the performances and behaviors of BERT in ranking tasks.
We explore several different ways to leverage the pre-trained BERT and
fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web
Track ad hoc document ranking. Experimental results on MS MARCO demonstrate the
strong effectiveness of BERT in question-answering focused passage ranking
tasks, as well as the fact that BERT is a strong interaction-based seq2seq
matching model. Experimental results on TREC show the gaps between the BERT
pre-trained on surrounding contexts and the needs of ad hoc document ranking.
Analyses illustrate how BERT allocates its attentions between query-document
tokens in its Transformer layers, how it prefers semantic matches between
paraphrase tokens, and how that differs with the soft match patterns learned by
a click-trained neural ranker.
</summary>
    <author>
      <name>Yifan Qiao</name>
    </author>
    <author>
      <name>Chenyan Xiong</name>
    </author>
    <author>
      <name>Zhenghao Liu</name>
    </author>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">There is an error in Table 1 and we will update them to correct
  results. Please refer to MS MARCO Leaderboard for the actually evaluation
  results</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.07531v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07531v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.06652v1</id>
    <updated>2019-04-14T08:17:06Z</updated>
    <published>2019-04-14T08:17:06Z</published>
    <title>Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering</title>
    <summary>  Recently, a simple combination of passage retrieval using off-the-shelf IR
techniques and a BERT reader was found to be very effective for question
answering directly on Wikipedia, yielding a large improvement over the previous
state of the art on a standard benchmark dataset. In this paper, we present a
data augmentation technique using distant supervision that exploits positive as
well as negative examples. We apply a stage-wise approach to fine tuning BERT
on multiple datasets, starting with data that is "furthest" from the test data
and ending with the "closest". Experimental results show large gains in
effectiveness over previous approaches on English QA datasets, and we establish
new baselines on two recent Chinese QA datasets.
</summary>
    <author>
      <name>Wei Yang</name>
    </author>
    <author>
      <name>Yuqing Xie</name>
    </author>
    <author>
      <name>Luchen Tan</name>
    </author>
    <author>
      <name>Kun Xiong</name>
    </author>
    <author>
      <name>Ming Li</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1904.06652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.06652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05255v1</id>
    <updated>2019-04-10T15:52:13Z</updated>
    <published>2019-04-10T15:52:13Z</published>
    <title>Simple BERT Models for Relation Extraction and Semantic Role Labeling</title>
    <summary>  We present simple BERT-based models for relation extraction and semantic role
labeling. In recent years, state-of-the-art performance has been achieved using
neural models by incorporating lexical and syntactic features such as
part-of-speech tags and dependency trees. In this paper, extensive experiments
on datasets for these two tasks show that without using any external features,
a simple BERT-based model can achieve state-of-the-art performance. To our
knowledge, we are the first to successfully apply BERT in this manner. Our
models provide strong baselines for future research.
</summary>
    <author>
      <name>Peng Shi</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03450v1</id>
    <updated>2019-04-06T14:02:13Z</updated>
    <published>2019-04-06T14:02:13Z</published>
    <title>UM-IU@LING at SemEval-2019 Task 6: Identifying Offensive Tweets Using
  BERT and SVMs</title>
    <summary>  This paper describes the UM-IU@LING's system for the SemEval 2019 Task 6:
OffensEval. We take a mixed approach to identify and categorize hate speech in
social media. In subtask A, we fine-tuned a BERT based classifier to detect
abusive content in tweets, achieving a macro F1 score of 0.8136 on the test
data, thus reaching the 3rd rank out of 103 submissions. In subtasks B and C,
we used a linear SVM with selected character n-gram features. For subtask C,
our system could identify the target of abuse with a macro F1 score of 0.5243,
ranking it 27th out of 65 submissions.
</summary>
    <author>
      <name>Jian Zhu</name>
    </author>
    <author>
      <name>Zuoyu Tian</name>
    </author>
    <author>
      <name>Sandra K√ºbler</name>
    </author>
    <link href="http://arxiv.org/abs/1904.03450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03339v1</id>
    <updated>2019-04-06T02:24:30Z</updated>
    <published>2019-04-06T02:24:30Z</published>
    <title>ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for
  out-of-domain samples</title>
    <summary>  This paper describes our system, Joint Encoders for Stable Suggestion
Inference (JESSI), for the SemEval 2019 Task 9: Suggestion Mining from Online
Reviews and Forums. JESSI is a combination of two sentence encoders: (a) one
using multiple pre-trained word embeddings learned from log-bilinear regression
(GloVe) and translation (CoVe) models, and (b) one on top of word encodings
from a pre-trained deep bidirectional transformer (BERT). We include a domain
adversarial training module when training for out-of-domain samples. Our
experiments show that while BERT performs exceptionally well for in-domain
samples, several runs of the model show that it is unstable for out-of-domain
samples. The problem is mitigated tremendously by (1) combining BERT with a
non-BERT encoder, and (2) using an RNN-based classifier on top of BERT. Our
final models obtained second place with 77.78\% F-Score on Subtask A (i.e.
in-domain) and achieved an F-Score of 79.59\% on Subtask B (i.e.
out-of-domain), even without using any additional external data.
</summary>
    <author>
      <name>Cheoneum Park</name>
    </author>
    <author>
      <name>Juae Kim</name>
    </author>
    <author>
      <name>Hyeon-gu Lee</name>
    </author>
    <author>
      <name>Reinald Kim Amplayo</name>
    </author>
    <author>
      <name>Harksoo Kim</name>
    </author>
    <author>
      <name>Jungyun Seo</name>
    </author>
    <author>
      <name>Changki Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SemEval 2019 Task 9</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.03339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03323v3</id>
    <updated>2019-06-20T20:41:58Z</updated>
    <published>2019-04-06T00:34:39Z</published>
    <title>Publicly Available Clinical BERT Embeddings</title>
    <summary>  Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT
(Devlin et al., 2018) have dramatically improved performance for many natural
language processing (NLP) tasks in recent months. However, these models have
been minimally explored on specialty corpora, such as clinical text; moreover,
in the clinical domain, no publicly-available pre-trained BERT models yet
exist. In this work, we address this need by exploring and releasing BERT
models for clinical text: one for generic clinical text and another for
discharge summaries specifically. We demonstrate that using a domain-specific
model yields performance improvements on three common clinical NLP tasks as
compared to nonspecific embeddings. These domain-specific models are not as
performant on two clinical de-identification tasks, and argue that this is a
natural consequence of the differences between de-identified source text and
synthetically non de-identified task text.
</summary>
    <author>
      <name>Emily Alsentzer</name>
    </author>
    <author>
      <name>John R. Murphy</name>
    </author>
    <author>
      <name>Willie Boag</name>
    </author>
    <author>
      <name>Wei-Hung Weng</name>
    </author>
    <author>
      <name>Di Jin</name>
    </author>
    <author>
      <name>Tristan Naumann</name>
    </author>
    <author>
      <name>Matthew B. A. McDermott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Clinical Natural Language Processing (ClinicalNLP) Workshop at NAACL
  2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.03323v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03323v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03092v1</id>
    <updated>2019-04-05T14:40:22Z</updated>
    <published>2019-04-05T14:40:22Z</published>
    <title>Modeling Recurrence for Transformer</title>
    <summary>  Recently, the Transformer model that is based solely on attention mechanisms,
has advanced the state-of-the-art on various machine translation tasks.
However, recent studies reveal that the lack of recurrence hinders its further
improvement of translation capacity. In response to this problem, we propose to
directly model recurrence for Transformer with an additional recurrence
encoder. In addition to the standard recurrent neural network, we introduce a
novel attentive recurrent network to leverage the strengths of both attention
and recurrent networks. Experimental results on the widely-used WMT14
English-German and WMT17 Chinese-English translation tasks demonstrate the
effectiveness of the proposed approach. Our studies also reveal that the
proposed model benefits from a short-cut that bridges the source and target
sequences with a single recurrent layer, which outperforms its deep
counterpart.
</summary>
    <author>
      <name>Jie Hao</name>
    </author>
    <author>
      <name>Xing Wang</name>
    </author>
    <author>
      <name>Baosong Yang</name>
    </author>
    <author>
      <name>Longyue Wang</name>
    </author>
    <author>
      <name>Jinfeng Zhang</name>
    </author>
    <author>
      <name>Zhaopeng Tu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NAACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.03092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.02342v2</id>
    <updated>2019-05-18T01:07:52Z</updated>
    <published>2019-04-04T04:33:15Z</published>
    <title>Text Generation from Knowledge Graphs with Graph Transformers</title>
    <summary>  Generating texts which express complex ideas spanning multiple sentences
requires a structured representation of their content (document plan), but
these representations are prohibitively expensive to manually produce. In this
work, we address the problem of generating coherent multi-sentence texts from
the output of an information extraction system, and in particular a knowledge
graph. Graphical knowledge representations are ubiquitous in computing, but
pose a significant challenge for text generation techniques due to their
non-hierarchical nature, collapsing of long-distance dependencies, and
structural variety. We introduce a novel graph transforming encoder which can
leverage the relational structure of such knowledge graphs without imposing
linearization or hierarchical constraints. Incorporated into an encoder-decoder
setup, we provide an end-to-end trainable system for graph-to-text generation
that we apply to the domain of scientific text. Automatic and human evaluations
show that our technique produces more informative texts which exhibit better
document structure than competitive encoder-decoder methods.
</summary>
    <author>
      <name>Rik Koncel-Kedziorski</name>
    </author>
    <author>
      <name>Dhanush Bekal</name>
    </author>
    <author>
      <name>Yi Luan</name>
    </author>
    <author>
      <name>Mirella Lapata</name>
    </author>
    <author>
      <name>Hannaneh Hajishirzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a long paper in NAACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.02342v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.02342v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.02232v2</id>
    <updated>2019-05-04T02:44:35Z</updated>
    <published>2019-04-03T20:29:10Z</published>
    <title>BERT Post-Training for Review Reading Comprehension and Aspect-based
  Sentiment Analysis</title>
    <summary>  Question-answering plays an important role in e-commerce as it allows
potential customers to actively seek crucial information about products or
services to help their purchase decision making. Inspired by the recent success
of machine reading comprehension (MRC) on formal documents, this paper explores
the potential of turning customer reviews into a large source of knowledge that
can be exploited to answer user questions.~We call this problem Review Reading
Comprehension (RRC). To the best of our knowledge, no existing work has been
done on RRC. In this work, we first build an RRC dataset called ReviewRC based
on a popular benchmark for aspect-based sentiment analysis. Since ReviewRC has
limited training examples for RRC (and also for aspect-based sentiment
analysis), we then explore a novel post-training approach on the popular
language model BERT to enhance the performance of fine-tuning of BERT for RRC.
To show the generality of the approach, the proposed post-training is also
applied to some other review-based tasks such as aspect extraction and aspect
sentiment classification in aspect-based sentiment analysis. Experimental
results demonstrate that the proposed post-training is highly effective. The
datasets and code are available at https://www.cs.uic.edu/~hxu/.
</summary>
    <author>
      <name>Hu Xu</name>
    </author>
    <author>
      <name>Bing Liu</name>
    </author>
    <author>
      <name>Lei Shu</name>
    </author>
    <author>
      <name>Philip S. Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by NAACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.02232v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.02232v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.00962v5</id>
    <updated>2020-01-03T06:53:00Z</updated>
    <published>2019-04-01T16:53:35Z</published>
    <title>Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</title>
    <summary>  Training large deep neural networks on massive datasets is computationally
very challenging. There has been recent surge in interest in using large batch
stochastic optimization methods to tackle this issue. The most prominent
algorithm in this line of research is LARS, which by employing layerwise
adaptive learning rates trains ResNet on ImageNet in a few minutes. However,
LARS performs poorly for attention models like BERT, indicating that its
performance gains are not consistent across tasks. In this paper, we first
study a principled layerwise adaptation strategy to accelerate training of deep
neural networks using large mini-batches. Using this strategy, we develop a new
layerwise adaptive large batch optimization technique called LAMB; we then
provide convergence analysis of LAMB as well as LARS, showing convergence to a
stationary point in general nonconvex settings. Our empirical results
demonstrate the superior performance of LAMB across various tasks such as BERT
and ResNet-50 training with very little hyperparameter tuning. In particular,
for BERT training, our optimizer enables use of very large batch sizes of 32868
without any degradation of performance. By increasing the batch size to the
memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to
just 76 minutes (Table 1). The LAMB implementation is available at
https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py
</summary>
    <author>
      <name>Yang You</name>
    </author>
    <author>
      <name>Jing Li</name>
    </author>
    <author>
      <name>Sashank Reddi</name>
    </author>
    <author>
      <name>Jonathan Hseu</name>
    </author>
    <author>
      <name>Sanjiv Kumar</name>
    </author>
    <author>
      <name>Srinadh Bhojanapalli</name>
    </author>
    <author>
      <name>Xiaodan Song</name>
    </author>
    <author>
      <name>James Demmel</name>
    </author>
    <author>
      <name>Kurt Keutzer</name>
    </author>
    <author>
      <name>Cho-Jui Hsieh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.00962v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.00962v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.00132v2</id>
    <updated>2019-05-31T20:43:22Z</updated>
    <published>2019-03-30T01:51:24Z</published>
    <title>ANA at SemEval-2019 Task 3: Contextual Emotion detection in
  Conversations through hierarchical LSTMs and BERT</title>
    <summary>  This paper describes the system submitted by ANA Team for the SemEval-2019
Task 3: EmoContext. We propose a novel Hierarchical LSTMs for Contextual
Emotion Detection (HRLCE) model. It classifies the emotion of an utterance
given its conversational context. The results show that, in this task, our
HRCLE outperforms the most recent state-of-the-art text classification
framework: BERT. We combine the results generated by BERT and HRCLE to achieve
an overall score of 0.7709 which ranked 5th on the final leader board of the
competition among 165 Teams.
</summary>
    <author>
      <name>Chenyang Huang</name>
    </author>
    <author>
      <name>Amine Trabelsi</name>
    </author>
    <author>
      <name>Osmar R. Za√Øane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the SemEval-2019 International Workshop on Semantic
  Evaluation</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.00132v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.00132v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.12392v2</id>
    <updated>2019-04-07T23:37:21Z</updated>
    <published>2019-03-29T08:36:06Z</published>
    <title>Training a Neural Speech Waveform Model using Spectral Losses of
  Short-Time Fourier Transform and Continuous Wavelet Transform</title>
    <summary>  Recently, we proposed short-time Fourier transform (STFT)-based loss
functions for training a neural speech waveform model. In this paper, we
generalize the above framework and propose a training scheme for such models
based on spectral amplitude and phase losses obtained by either STFT or
continuous wavelet transform (CWT), or both of them. Since CWT is capable of
having time and frequency resolutions different from those of STFT and is cable
of considering those closer to human auditory scales, the proposed loss
functions could provide complementary information on speech signals.
Experimental results showed that it is possible to train a high-quality model
by using the proposed CWT spectral loss and is as good as one using STFT-based
loss.
</summary>
    <author>
      <name>Shinji Takaki</name>
    </author>
    <author>
      <name>Hirokazu Kameoka</name>
    </author>
    <author>
      <name>Junichi Yamagishi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Interspeech 2019, Graz, Austria</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.12392v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.12392v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.12136v1</id>
    <updated>2019-03-28T17:23:50Z</updated>
    <published>2019-03-28T17:23:50Z</published>
    <title>Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</title>
    <summary>  In the natural language processing literature, neural networks are becoming
increasingly deeper and complex. The recent poster child of this trend is the
deep language representation model, which includes BERT, ELMo, and GPT. These
developments have led to the conviction that previous-generation, shallower
neural networks for language understanding are obsolete. In this paper,
however, we demonstrate that rudimentary, lightweight neural networks can still
be made competitive without architecture changes, external training data, or
additional input features. We propose to distill knowledge from BERT, a
state-of-the-art language representation model, into a single-layer BiLSTM, as
well as its siamese counterpart for sentence-pair tasks. Across multiple
datasets in paraphrasing, natural language inference, and sentiment
classification, we achieve comparable results with ELMo, while using roughly
100 times fewer parameters and 15 times less inference time.
</summary>
    <author>
      <name>Raphael Tang</name>
    </author>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Linqing Liu</name>
    </author>
    <author>
      <name>Lili Mou</name>
    </author>
    <author>
      <name>Olga Vechtomova</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures; first three authors contributed equally</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.12136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.12136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.10972v1</id>
    <updated>2019-03-26T15:58:33Z</updated>
    <published>2019-03-26T15:58:33Z</published>
    <title>Simple Applications of BERT for Ad Hoc Document Retrieval</title>
    <summary>  Following recent successes in applying BERT to question answering, we explore
simple applications to ad hoc document retrieval. This required confronting the
challenge posed by documents that are typically longer than the length of input
BERT was designed to handle. We address this issue by applying inference on
sentences individually, and then aggregating sentence scores to produce
document scores. Experiments on TREC microblog and newswire test collections
show that our approach is simple yet effective, as we report the highest
average precision on these datasets by neural approaches that we are aware of.
</summary>
    <author>
      <name>Wei Yang</name>
    </author>
    <author>
      <name>Haotian Zhang</name>
    </author>
    <author>
      <name>Jimmy Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1903.10972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.10318v2</id>
    <updated>2019-09-05T15:35:21Z</updated>
    <published>2019-03-25T13:42:45Z</published>
    <title>Fine-tune BERT for Extractive Summarization</title>
    <summary>  BERT, a pre-trained Transformer model, has achieved ground-breaking
performance on multiple NLP tasks. In this paper, we describe BERTSUM, a simple
variant of BERT, for extractive summarization. Our system is the state of the
art on the CNN/Dailymail dataset, outperforming the previous best-performed
system by 1.65 on ROUGE-L. The codes to reproduce our results are available at
https://github.com/nlpyang/BertSum
</summary>
    <author>
      <name>Yang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">fix figure 1</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.10318v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10318v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.10118v1</id>
    <updated>2019-03-25T03:40:15Z</updated>
    <published>2019-03-25T03:40:15Z</published>
    <title>End-to-End Learning Using Cycle Consistency for Image-to-Caption
  Transformations</title>
    <summary>  So far, research to generate captions from images has been carried out from
the viewpoint that a caption holds sufficient information for an image. If it
is possible to generate an image that is close to the input image from a
generated caption, i.e., if it is possible to generate a natural language
caption containing sufficient information to reproduce the image, then the
caption is considered to be faithful to the image. To make such regeneration
possible, learning using the cycle-consistency loss is effective. In this
study, we propose a method of generating captions by learning end-to-end mutual
transformations between images and texts. To evaluate our method, we perform
comparative experiments with and without the cycle consistency. The results are
evaluated by an automatic evaluation and crowdsourcing, demonstrating that our
proposed method is effective.
</summary>
    <author>
      <name>Keisuke Hagiwara</name>
    </author>
    <author>
      <name>Yusuke Mukuta</name>
    </author>
    <author>
      <name>Tatsuya Harada</name>
    </author>
    <link href="http://arxiv.org/abs/1903.10118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.09588v1</id>
    <updated>2019-03-22T16:29:18Z</updated>
    <published>2019-03-22T16:29:18Z</published>
    <title>Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing
  Auxiliary Sentence</title>
    <summary>  Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained
opinion polarity towards a specific aspect, is a challenging subtask of
sentiment analysis (SA). In this paper, we construct an auxiliary sentence from
the aspect and convert ABSA to a sentence-pair classification task, such as
question answering (QA) and natural language inference (NLI). We fine-tune the
pre-trained model from BERT and achieve new state-of-the-art results on
SentiHood and SemEval-2014 Task 4 datasets.
</summary>
    <author>
      <name>Chi Sun</name>
    </author>
    <author>
      <name>Luyao Huang</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to NAACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.09588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.09588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.08953v1</id>
    <updated>2019-03-21T12:39:02Z</updated>
    <published>2019-03-21T12:39:02Z</published>
    <title>Learning Multi-Level Information for Dialogue Response Selection by
  Highway Recurrent Transformer</title>
    <summary>  With the increasing research interest in dialogue response generation, there
is an emerging branch formulating this task as selecting next sentences, where
given the partial dialogue contexts, the goal is to determine the most probable
next sentence. Following the recent success of the Transformer model, this
paper proposes (1) a new variant of attention mechanism based on multi-head
attention, called highway attention, and (2) a recurrent model based on
transformer and the proposed highway attention, so-called Highway Recurrent
Transformer. Experiments on the response selection task in the seventh Dialog
System Technology Challenge (DSTC7) show the capability of the proposed model
of modeling both utterance-level and dialogue-level information; the
effectiveness of each module is further analyzed as well.
</summary>
    <author>
      <name>Ting-Rui Chiang</name>
    </author>
    <author>
      <name>Chao-Wei Huang</name>
    </author>
    <author>
      <name>Shang-Yu Su</name>
    </author>
    <author>
      <name>Yun-Nung Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1903.08953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.08953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.07402v1</id>
    <updated>2019-03-18T12:54:22Z</updated>
    <published>2019-03-18T12:54:22Z</published>
    <title>Neutron: An Implementation of the Transformer Translation Model and its
  Variants</title>
    <summary>  The Transformer translation model is easier to parallelize and provides
better performance comparing with recurrent seq2seq models, which makes it
popular among industry and research community. We implement Neutron in this
work, including the Transformer model and several variants from most recent
researches. It is easier to modify and provides comparable performance with
interesting features while keep readability.
</summary>
    <author>
      <name>Hongfei Xu</name>
    </author>
    <author>
      <name>Qiuhui Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1903.07402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.06464v1</id>
    <updated>2019-03-15T11:13:22Z</updated>
    <published>2019-03-15T11:13:22Z</published>
    <title>A Context-Aware Citation Recommendation Model with BERT and Graph
  Convolutional Networks</title>
    <summary>  With the tremendous growth in the number of scientific papers being
published, searching for references while writing a scientific paper is a
time-consuming process. A technique that could add a reference citation at the
appropriate place in a sentence will be beneficial. In this perspective,
context-aware citation recommendation has been researched upon for around two
decades. Many researchers have utilized the text data called the context
sentence, which surrounds the citation tag, and the metadata of the target
paper to find the appropriate cited research. However, the lack of
well-organized benchmarking datasets and no model that can attain high
performance has made the research difficult.
  In this paper, we propose a deep learning based model and well-organized
dataset for context-aware paper citation recommendation. Our model comprises a
document encoder and a context encoder, which uses Graph Convolutional Networks
(GCN) layer and Bidirectional Encoder Representations from Transformers (BERT),
which is a pre-trained model of textual data. By modifying the related PeerRead
dataset, we propose a new dataset called FullTextPeerRead containing context
sentences to cited references and paper metadata. To the best of our knowledge,
This dataset is the first well-organized dataset for context-aware paper
recommendation. The results indicate that the proposed model with the proposed
datasets can attain state-of-the-art performance and achieve a more than 28%
improvement in mean average precision (MAP) and recall@k.
</summary>
    <author>
      <name>Chanwoo Jeong</name>
    </author>
    <author>
      <name>Sion Jang</name>
    </author>
    <author>
      <name>Hyuna Shin</name>
    </author>
    <author>
      <name>Eunjeong Park</name>
    </author>
    <author>
      <name>Sungchul Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.06464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.06464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.05823v4</id>
    <updated>2019-11-22T00:54:27Z</updated>
    <published>2019-03-14T05:53:22Z</published>
    <title>Deep Patent Landscaping Model Using Transformer and Graph Embedding</title>
    <summary>  Patent landscaping is a method used for searching related patents during a
research and development (R&amp;D) project. To avoid the risk of patent
infringement and to follow current trends in technology, patent landscaping is
a crucial task required during the early stages of an R&amp;D project. As the
process of patent landscaping requires advanced resources and can be tedious,
the demand for automated patent landscaping has been gradually increasing.
However, a shortage of well-defined benchmark datasets and comparable models
makes it difficult to find related research studies. In this paper, we propose
an automated patent landscaping model based on deep learning. To analyze the
text of patents, the proposed model uses a modified transformer structure. To
analyze the metadata of patents, we propose a graph embedding method that uses
a diffusion graph called Diff2Vec. Furthermore, we introduce four benchmark
datasets for comparing related research studies in patent landscaping. The
datasets are produced by querying Google BigQuery, based on a search formula
from a Korean patent attorney. The obtained results indicate that the proposed
model and datasets can attain state-of-the-art performance, as compared with
current patent landscaping models.
</summary>
    <author>
      <name>Seokkyu Choi</name>
    </author>
    <author>
      <name>Hyeonju Lee</name>
    </author>
    <author>
      <name>Eunjeong Lucy Park</name>
    </author>
    <author>
      <name>Sungchul Choi</name>
    </author>
    <link href="http://arxiv.org/abs/1903.05823v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.05823v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.10909v1</id>
    <updated>2019-02-28T05:54:16Z</updated>
    <published>2019-02-28T05:54:16Z</published>
    <title>BERT for Joint Intent Classification and Slot Filling</title>
    <summary>  Intent classification and slot filling are two essential tasks for natural
language understanding. They often suffer from small-scale human-labeled
training data, resulting in poor generalization capability, especially for rare
words. Recently a new language representation model, BERT (Bidirectional
Encoder Representations from Transformers), facilitates pre-training deep
bidirectional representations on large-scale unlabeled corpora, and has created
state-of-the-art models for a wide variety of natural language processing tasks
after simple fine-tuning. However, there has not been much effort on exploring
BERT for natural language understanding. In this work, we propose a joint
intent classification and slot filling model based on BERT. Experimental
results demonstrate that our proposed model achieves significant improvement on
intent classification accuracy, slot filling F1, and sentence-level semantic
frame accuracy on several public benchmark datasets, compared to the
attention-based recurrent neural network models and slot-gated models.
</summary>
    <author>
      <name>Qian Chen</name>
    </author>
    <author>
      <name>Zhu Zhuo</name>
    </author>
    <author>
      <name>Wen Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.10909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.10909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.10126v2</id>
    <updated>2019-03-21T08:43:35Z</updated>
    <published>2019-02-25T19:53:01Z</published>
    <title>BUT-FIT at SemEval-2019 Task 7: Determining the Rumour Stance with
  Pre-Trained Deep Bidirectional Transformers</title>
    <summary>  This paper describes our system submitted to SemEval 2019 Task 7: RumourEval
2019: Determining Rumour Veracity and Support for Rumours, Subtask A (Gorrell
et al., 2019). The challenge focused on classifying whether posts from Twitter
and Reddit support, deny, query, or comment a hidden rumour, truthfulness of
which is the topic of an underlying discussion thread. We formulate the problem
as a stance classification, determining the rumour stance of a post with
respect to the previous thread post and the source thread post. The recent BERT
architecture was employed to build an end-to-end system which has reached the
F1 score of 61.67% on the provided test data. It finished at the 2nd place in
the competition, without any hand-crafted features, only 0.2% behind the
winner.
</summary>
    <author>
      <name>Martin Fajcik</name>
    </author>
    <author>
      <name>Luk√°≈° Burget</name>
    </author>
    <author>
      <name>Pavel Smrz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to NAACL SemEval workshop. Work in
  progress</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 13th International Workshop on Semantic
  Evaluation 13 (2019) 1097-1104</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.10126v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.10126v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.09381v3</id>
    <updated>2019-04-08T08:47:04Z</updated>
    <published>2019-02-25T15:49:10Z</published>
    <title>EAT2seq: A generic framework for controlled sentence transformation
  without task-specific training</title>
    <summary>  We present EAT2seq: a novel method to architect automatic linguistic
transformations for a number of tasks, including controlled grammatical or
lexical changes, style transfer, text generation, and machine translation. Our
approach consists in creating an abstract representation of a sentence's
meaning and grammar, which we use as input to an encoder-decoder network
trained to reproduce the original sentence. Manipulating the abstract
representation allows the transformation of sentences according to
user-provided parameters, both grammatically and lexically, in any combination.
The same architecture can further be used for controlled text generation, and
has additional promise for machine translation. This strategy holds the promise
of enabling many tasks that were hitherto outside the scope of NLP techniques
for want of sufficient training data. We provide empirical evidence for the
effectiveness of our approach by reproducing and transforming English
sentences, and evaluating the results both manually and automatically. A single
model trained on monolingual data is used for all tasks without any
task-specific training. For a model trained on 8.5 million sentences, we report
a BLEU score of 74.45 for reproduction, and scores between 55.29 and 81.82 for
back-and-forth grammatical transformations across 14 category pairs.
</summary>
    <author>
      <name>Tommi Gr√∂ndahl</name>
    </author>
    <author>
      <name>N. Asokan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.09381v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.09381v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.09113v2</id>
    <updated>2019-02-28T05:55:04Z</updated>
    <published>2019-02-25T07:07:38Z</published>
    <title>Star-Transformer</title>
    <summary>  Although Transformer has achieved great successes on many NLP tasks, its
heavy structure with fully-connected attention connections leads to
dependencies on large training data. In this paper, we present
Star-Transformer, a lightweight alternative by careful sparsification. To
reduce model complexity, we replace the fully-connected structure with a
star-shaped topology, in which every two non-adjacent nodes are connected
through a shared relay node. Thus, complexity is reduced from quadratic to
linear, while preserving capacity to capture both local composition and
long-range dependency. The experiments on four tasks (22 datasets) show that
Star-Transformer achieved significant improvements against the standard
Transformer for the modestly sized datasets.
</summary>
    <author>
      <name>Qipeng Guo</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Pengfei Liu</name>
    </author>
    <author>
      <name>Yunfan Shao</name>
    </author>
    <author>
      <name>Xiangyang Xue</name>
    </author>
    <author>
      <name>Zheng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by NAACL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.09113v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.09113v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.04094v2</id>
    <updated>2019-04-09T18:01:28Z</updated>
    <published>2019-02-11T19:02:27Z</published>
    <title>BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field
  Language Model</title>
    <summary>  We show that BERT (Devlin et al., 2018) is a Markov random field language
model. This formulation gives way to a natural procedure to sample sentences
from BERT. We generate from BERT and find that it can produce high-quality,
fluent generations. Compared to the generations of a traditional left-to-right
language model, BERT generates sentences that are more diverse but of slightly
worse quality.
</summary>
    <author>
      <name>Alex Wang</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeuralGen 2019;
  https://colab.research.google.com/drive/1MxKZGtQ9SSBjTK5ArsZ5LKhkztzg52RV</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.04094v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04094v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.03249v1</id>
    <updated>2019-02-08T19:00:04Z</updated>
    <published>2019-02-08T19:00:04Z</published>
    <title>Insertion Transformer: Flexible Sequence Generation via Insertion
  Operations</title>
    <summary>  We present the Insertion Transformer, an iterative, partially autoregressive
model for sequence generation based on insertion operations. Unlike typical
autoregressive models which rely on a fixed, often left-to-right ordering of
the output, our approach accommodates arbitrary orderings by allowing for
tokens to be inserted anywhere in the sequence during decoding. This
flexibility confers a number of advantages: for instance, not only can our
model be trained to follow specific orderings such as left-to-right generation
or a binary tree traversal, but it can also be trained to maximize entropy over
all valid insertions for robustness. In addition, our model seamlessly
accommodates both fully autoregressive generation (one insertion at a time) and
partially autoregressive generation (simultaneous insertions at multiple
locations). We validate our approach by analyzing its performance on the WMT
2014 English-German machine translation task under various settings for
training and decoding. We find that the Insertion Transformer outperforms many
prior non-autoregressive approaches to translation at comparable or better
levels of parallelism, and successfully recovers the performance of the
original Transformer while requiring only logarithmically many iterations
during decoding.
</summary>
    <author>
      <name>Mitchell Stern</name>
    </author>
    <author>
      <name>William Chan</name>
    </author>
    <author>
      <name>Jamie Kiros</name>
    </author>
    <author>
      <name>Jakob Uszkoreit</name>
    </author>
    <link href="http://arxiv.org/abs/1902.03249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.02671v2</id>
    <updated>2019-05-15T11:13:54Z</updated>
    <published>2019-02-07T15:05:46Z</published>
    <title>BERT and PALs: Projected Attention Layers for Efficient Adaptation in
  Multi-Task Learning</title>
    <summary>  Multi-task learning shares information between related tasks, sometimes
reducing the number of parameters required. State-of-the-art results across
multiple natural language understanding tasks in the GLUE benchmark have
previously used transfer from a single large task: unsupervised pre-training
with BERT, where a separate BERT model was fine-tuned for each task. We explore
multi-task approaches that share a single BERT model with a small number of
additional task-specific parameters. Using new adaptation modules, PALs or
`projected attention layers', we match the performance of separately fine-tuned
models on the GLUE benchmark with roughly 7 times fewer parameters, and obtain
state-of-the-art results on the Recognizing Textual Entailment dataset.
</summary>
    <author>
      <name>Asa Cooper Stickland</name>
    </author>
    <author>
      <name>Iain Murray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ICML 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.02671v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02671v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.02113v1</id>
    <updated>2019-02-06T11:15:08Z</updated>
    <published>2019-02-06T11:15:08Z</published>
    <title>Latent Space Cartography: Generalised Metric-Inspired Measures and
  Measure-Based Transformations for Generative Models</title>
    <summary>  Deep generative models are universal tools for learning data distributions on
high dimensional data spaces via a mapping to lower dimensional latent spaces.
We provide a study of latent space geometries and extend and build upon
previous results on Riemannian metrics. We show how a class of heuristic
measures gives more flexibility in finding meaningful, problem-specific
distances, and how it can be applied to diverse generator types such as
autoregressive generators commonly used in e.g. language and other sequence
modeling. We further demonstrate how a diffusion-inspired transformation
previously studied in cartography can be used to smooth out latent spaces,
stretching them according to a chosen measure. In addition to providing more
meaningful distances directly in latent space, this also provides a unique tool
for novel kinds of data visualizations. We believe that the proposed methods
can be a valuable tool for studying the structure of latent spaces and learned
data distributions of generative models.
</summary>
    <author>
      <name>Max F. Frenzel</name>
    </author>
    <author>
      <name>Bogdan Teleaga</name>
    </author>
    <author>
      <name>Asahi Ushio</name>
    </author>
    <link href="http://arxiv.org/abs/1902.02113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.01030v2</id>
    <updated>2019-06-03T14:43:32Z</updated>
    <published>2019-02-04T04:42:08Z</published>
    <title>Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers</title>
    <summary>  Most approaches to extraction multiple relations from a paragraph require
multiple passes over the paragraph. In practice, multiple passes are
computationally expensive and this makes difficult to scale to longer
paragraphs and larger text corpora. In this work, we focus on the task of
multiple relation extraction by encoding the paragraph only once (one-pass). We
build our solution on the pre-trained self-attentive (Transformer) models,
where we first add a structured prediction layer to handle extraction between
multiple entity pairs, then enhance the paragraph embedding to capture multiple
relational information associated with each entity with an entity-aware
attention technique. We show that our approach is not only scalable but can
also perform state-of-the-art on the standard benchmark ACE 2005.
</summary>
    <author>
      <name>Haoyu Wang</name>
    </author>
    <author>
      <name>Ming Tan</name>
    </author>
    <author>
      <name>Mo Yu</name>
    </author>
    <author>
      <name>Shiyu Chang</name>
    </author>
    <author>
      <name>Dakuo Wang</name>
    </author>
    <author>
      <name>Kun Xu</name>
    </author>
    <author>
      <name>Xiaoxiao Guo</name>
    </author>
    <author>
      <name>Saloni Potdar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.01030v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01030v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.11467v1</id>
    <updated>2019-01-31T16:51:49Z</updated>
    <published>2019-01-31T16:51:49Z</published>
    <title>Towards Controlled Transformation of Sentiment in Sentences</title>
    <summary>  An obstacle to the development of many natural language processing products
is the vast amount of training examples necessary to get satisfactory results.
The generation of these examples is often a tedious and time-consuming task.
This paper this paper proposes a method to transform the sentiment of sentences
in order to limit the work necessary to generate more training data. This means
that one sentence can be transformed to an opposite sentiment sentence and
should reduce by half the work required in the generation of text. The proposed
pipeline consists of a sentiment classifier with an attention mechanism to
highlight the short phrases that determine the sentiment of a sentence. Then,
these phrases are changed to phrases of the opposite sentiment using a baseline
model and an autoencoder approach. Experiments are run on both the separate
parts of the pipeline as well as on the end-to-end model. The sentiment
classifier is tested on its accuracy and is found to perform adequately. The
autoencoder is tested on how well it is able to change the sentiment of an
encoded phrase and it was found that such a task is possible. We use human
evaluation to judge the performance of the full (end-to-end) pipeline and that
reveals that a model using word vectors outperforms the encoder model.
Numerical evaluation shows that a success rate of 54.7% is achieved on the
sentiment change.
</summary>
    <author>
      <name>Wouter Leeftink</name>
    </author>
    <author>
      <name>Gerasimos Spanakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICAART 2019, 8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.11467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.11467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.11117v4</id>
    <updated>2019-05-17T19:47:49Z</updated>
    <published>2019-01-30T22:03:01Z</published>
    <title>The Evolved Transformer</title>
    <summary>  Recent works have highlighted the strength of the Transformer architecture on
sequence tasks while, at the same time, neural architecture search (NAS) has
begun to outperform human-designed models. Our goal is to apply NAS to search
for a better alternative to the Transformer. We first construct a large search
space inspired by the recent advances in feed-forward sequence models and then
run evolutionary architecture search with warm starting by seeding our initial
population with the Transformer. To directly search on the computationally
expensive WMT 2014 English-German translation task, we develop the Progressive
Dynamic Hurdles method, which allows us to dynamically allocate more resources
to more promising candidate models. The architecture found in our experiments
-- the Evolved Transformer -- demonstrates consistent improvement over the
Transformer on four well-established language tasks: WMT 2014 English-German,
WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size,
the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8
on WMT'14 English-German; at smaller sizes, it achieves the same quality as the
original "big" Transformer with 37.6% less parameters and outperforms the
Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.
</summary>
    <author>
      <name>David R. So</name>
    </author>
    <author>
      <name>Chen Liang</name>
    </author>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML version with SOTA results</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.11117v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.11117v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.08634v3</id>
    <updated>2019-12-09T17:32:42Z</updated>
    <published>2019-01-24T20:22:14Z</published>
    <title>A BERT Baseline for the Natural Questions</title>
    <summary>  This technical note describes a new baseline for the Natural Questions. Our
model is based on BERT and reduces the gap between the model F1 scores reported
in the original dataset paper and the human upper bound by 30% and 50% relative
for the long and short answer tasks respectively. This baseline has been
submitted to the official NQ leaderboard at
ai.google.com/research/NaturalQuestions. Code, preprocessed data and pretrained
model are available at
https://github.com/google-research/language/tree/master/language/question_answering/bert_joint.
</summary>
    <author>
      <name>Chris Alberti</name>
    </author>
    <author>
      <name>Kenton Lee</name>
    </author>
    <author>
      <name>Michael Collins</name>
    </author>
    <link href="http://arxiv.org/abs/1901.08634v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.08634v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05287v1</id>
    <updated>2019-01-16T14:01:15Z</updated>
    <published>2019-01-16T14:01:15Z</published>
    <title>Assessing BERT's Syntactic Abilities</title>
    <summary>  I assess the extent to which the recently introduced BERT model captures
English syntactic phenomena, using (1) naturally-occurring subject-verb
agreement stimuli; (2) "coloreless green ideas" subject-verb agreement stimuli,
in which content words in natural sentences are randomly replaced with words
sharing the same part-of-speech and inflection; and (3) manually crafted
stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT
model performs remarkably well on all cases.
</summary>
    <author>
      <name>Yoav Goldberg</name>
    </author>
    <link href="http://arxiv.org/abs/1901.05287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.04085v4</id>
    <updated>2019-02-18T22:04:21Z</updated>
    <published>2019-01-13T23:27:58Z</published>
    <title>Passage Re-ranking with BERT</title>
    <summary>  Recently, neural models pretrained on a language modeling task, such as ELMo
(Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et
al., 2018), have achieved impressive results on various natural language
processing tasks such as question-answering and natural language inference. In
this paper, we describe a simple re-implementation of BERT for query-based
passage re-ranking. Our system is the state of the art on the TREC-CAR dataset
and the top entry in the leaderboard of the MS MARCO passage retrieval task,
outperforming the previous state of the art by 27% (relative) in MRR@10. The
code to reproduce our results is available at
https://github.com/nyu-dl/dl4marco-bert
</summary>
    <author>
      <name>Rodrigo Nogueira</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <link href="http://arxiv.org/abs/1901.04085v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.04085v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.02534v1</id>
    <updated>2019-01-08T21:57:30Z</updated>
    <published>2019-01-08T21:57:30Z</published>
    <title>Team Papelo: Transformer Networks at FEVER</title>
    <summary>  We develop a system for the FEVER fact extraction and verification challenge
that uses a high precision entailment classifier based on transformer networks
pretrained with language modeling, to classify a broad set of potential
evidence. The precision of the entailment classifier allows us to enhance
recall by considering every statement from several articles to decide upon each
claim. We include not only the articles best matching the claim text by TFIDF
score, but read additional articles whose titles match named entities and
capitalized expressions occurring in the claim text. The entailment module
evaluates potential evidence one statement at a time, together with the title
of the page the evidence came from (providing a hint about possible pronoun
antecedents). In preliminary evaluation, the system achieves .5736 FEVER score,
.6108 label accuracy, and .6485 evidence F1 on the FEVER shared task test set.
</summary>
    <author>
      <name>Christopher Malon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared at EMNLP 2018 First Workshop on Fact Extraction and
  Verification (FEVER)</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.02534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.02534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.06705v1</id>
    <updated>2018-12-17T11:26:42Z</updated>
    <published>2018-12-17T11:26:42Z</published>
    <title>Conditional BERT Contextual Augmentation</title>
    <summary>  We propose a novel data augmentation method for labeled sentences called
conditional BERT contextual augmentation. Data augmentation methods are often
applied to prevent overfitting and improve generalization of deep neural
network models. Recently proposed contextual augmentation augments labeled
sentences by randomly replacing words with more varied substitutions predicted
by language model. BERT demonstrates that a deep bidirectional language model
is more powerful than either an unidirectional language model or the shallow
concatenation of a forward and backward model. We retrofit BERT to conditional
BERT by introducing a new conditional masked language model\footnote{The term
"conditional masked language model" appeared once in original BERT paper, which
indicates context-conditional, is equivalent to term "masked language model".
In our paper, "conditional masked language model" indicates we apply extra
label-conditional constraint to the "masked language model".} task. The well
trained conditional BERT can be applied to enhance contextual augmentation.
Experiments on six various different text classification tasks show that our
method can be easily applied to both convolutional or recurrent neural networks
classifier to obtain obvious improvement.
</summary>
    <author>
      <name>Xing Wu</name>
    </author>
    <author>
      <name>Shangwen Lv</name>
    </author>
    <author>
      <name>Liangjun Zang</name>
    </author>
    <author>
      <name>Jizhong Han</name>
    </author>
    <author>
      <name>Songlin Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.06705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.06705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.02825v5</id>
    <updated>2019-09-14T20:03:50Z</updated>
    <published>2018-12-05T03:05:08Z</published>
    <title>Attending to Mathematical Language with Transformers</title>
    <summary>  Mathematical expressions were generated, evaluated and used to train neural
network models based on the transformer architecture. The expressions and their
targets were analyzed as a character-level sequence transduction task in which
the encoder and decoder are built on attention mechanisms. Three models were
trained to understand and evaluate symbolic variables and expressions in
mathematics: (1) the self-attentive and feed-forward transformer without
recurrence or convolution, (2) the universal transformer with recurrence, and
(3) the adaptive universal transformer with recurrence and adaptive computation
time. The models respectively achieved test accuracies as high as 76.1%, 78.8%
and 84.9% in evaluating the expressions to match the target values. For the
cases inferred incorrectly, the results differed from the targets by only one
or two characters. The models notably learned to add, subtract and multiply
both positive and negative decimal numbers of variable digits assigned to
symbolic variables.
</summary>
    <author>
      <name>Artit Wangperawong</name>
    </author>
    <link href="http://arxiv.org/abs/1812.02825v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.02825v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.01431v1</id>
    <updated>2018-11-30T19:12:04Z</updated>
    <published>2018-11-30T19:12:04Z</published>
    <title>Modeling natural language emergence with integral transform theory and
  reinforcement learning</title>
    <summary>  Zipf's law predicts a power-law relationship between word rank and frequency
in language communication systems and has been widely reported in a variety of
natural language processing applications. However, the emergence of natural
language is often modeled as a function of bias between speaker and listener
interests, which lacks a direct way of relating information-theoretic bias to
Zipfian rank. A function of bias also serves as an unintuitive interpretation
of the communicative effort exchanged between a speaker and a listener. We
counter these shortcomings by proposing a novel integral transform and kernel
for mapping communicative bias functions to corresponding word frequency-rank
representations at any arbitrary phase transition point, resulting in a direct
way to link communicative effort (modeled by speaker/listener bias) to specific
vocabulary used (represented by word rank). We demonstrate the practical
utility of our integral transform by showing how a change from bias to rank
results in greater accuracy and performance at an image classification task for
assigning word labels to images randomly subsampled from CIFAR10. We model this
task as a reinforcement learning game between a speaker and listener and
compare the relative impact of bias and Zipfian word rank on communicative
performance (and accuracy) between the two agents.
</summary>
    <author>
      <name>Bohdan Khomtchouk</name>
    </author>
    <author>
      <name>Shyam Sudhakaran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, 2 tables. arXiv admin note: text overlap with
  arXiv:1603.03153</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.01431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.01431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.04716v1</id>
    <updated>2018-11-12T13:33:35Z</updated>
    <published>2018-11-12T13:33:35Z</published>
    <title>Input Combination Strategies for Multi-Source Transformer Decoder</title>
    <summary>  In multi-source sequence-to-sequence tasks, the attention mechanism can be
modeled in several ways. This topic has been thoroughly studied on recurrent
architectures. In this paper, we extend the previous work to the
encoder-decoder attention in the Transformer architecture. We propose four
different input combination strategies for the encoder-decoder attention:
serial, parallel, flat, and hierarchical. We evaluate our methods on tasks of
multimodal translation and translation with multiple source languages. The
experiments show that the models are able to use multiple sources and improve
over single source baselines.
</summary>
    <author>
      <name>Jind≈ôich Libovick√Ω</name>
    </author>
    <author>
      <name>Jind≈ôich Helcl</name>
    </author>
    <author>
      <name>David Mareƒçek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at WMT18</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.04716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.12730v2</id>
    <updated>2018-12-01T15:36:52Z</updated>
    <published>2018-10-29T15:20:32Z</published>
    <title>Audiovisual speaker conversion: jointly and simultaneously transforming
  facial expression and acoustic characteristics</title>
    <summary>  An audiovisual speaker conversion method is presented for simultaneously
transforming the facial expressions and voice of a source speaker into those of
a target speaker. Transforming the facial and acoustic features together makes
it possible for the converted voice and facial expressions to be highly
correlated and for the generated target speaker to appear and sound natural. It
uses three neural networks: a conversion network that fuses and transforms the
facial and acoustic features, a waveform generation network that produces the
waveform from both the converted facial and acoustic features, and an image
reconstruction network that outputs an RGB facial image also based on both the
converted features. The results of experiments using an emotional audiovisual
database showed that the proposed method achieved significantly higher
naturalness compared with one that separately transformed acoustic and facial
features.
</summary>
    <author>
      <name>Fuming Fang</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <author>
      <name>Junichi Yamagishi</name>
    </author>
    <author>
      <name>Isao Echizen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.12730v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12730v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.11193v1</id>
    <updated>2018-10-26T05:44:01Z</updated>
    <published>2018-10-26T05:44:01Z</published>
    <title>Integrating Transformer and Paraphrase Rules for Sentence Simplification</title>
    <summary>  Sentence simplification aims to reduce the complexity of a sentence while
retaining its original meaning. Current models for sentence simplification
adopted ideas from ma- chine translation studies and implicitly learned
simplification mapping rules from normal- simple sentence pairs. In this paper,
we explore a novel model based on a multi-layer and multi-head attention
architecture and we pro- pose two innovative approaches to integrate the Simple
PPDB (A Paraphrase Database for Simplification), an external paraphrase
knowledge base for simplification that covers a wide range of real-world
simplification rules. The experiments show that the integration provides two
major benefits: (1) the integrated model outperforms multiple state- of-the-art
baseline models for sentence simplification in the literature (2) through
analysis of the rule utilization, the model seeks to select more accurate
simplification rules. The code and models used in the paper are available at
https://github.com/ Sanqiang/text_simplification.
</summary>
    <author>
      <name>Sanqiang Zhao</name>
    </author>
    <author>
      <name>Rui Meng</name>
    </author>
    <author>
      <name>Daqing He</name>
    </author>
    <author>
      <name>Saptono Andi</name>
    </author>
    <author>
      <name>Parmanto Bambang</name>
    </author>
    <link href="http://arxiv.org/abs/1810.11193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.11193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.10437v3</id>
    <updated>2019-09-05T06:02:03Z</updated>
    <published>2018-10-24T15:07:19Z</published>
    <title>Variational Semi-supervised Aspect-term Sentiment Analysis via
  Transformer</title>
    <summary>  Aspect-term sentiment analysis (ATSA) is a longstanding challenge in natural
language understanding. It requires fine-grained semantical reasoning about a
target entity appeared in the text. As manual annotation over the aspects is
laborious and time-consuming, the amount of labeled data is limited for
supervised learning. This paper proposes a semi-supervised method for the ATSA
problem by using the Variational Autoencoder based on Transformer (VAET), which
models the latent distribution via variational inference. By disentangling the
latent representation into the aspect-specific sentiment and the lexical
context, our method induces the underlying sentiment prediction for the
unlabeled data, which then benefits the ATSA classifier. Our method is
classifier agnostic, i.e., the classifier is an independent module and various
advanced supervised models can be integrated. Experimental results are obtained
on the SemEval 2014 task 4 and show that our method is effective with four
classical classifiers. The proposed method outperforms two general
semisupervised methods and achieves state-of-the-art performance.
</summary>
    <author>
      <name>Xingyi Cheng</name>
    </author>
    <author>
      <name>Weidi Xu</name>
    </author>
    <author>
      <name>Taifeng Wang</name>
    </author>
    <author>
      <name>Wei Chu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CoNLL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.10437v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.10437v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04805v2</id>
    <updated>2019-05-24T20:37:26Z</updated>
    <published>2018-10-11T00:50:01Z</published>
    <title>BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding</title>
    <summary>  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
</summary>
    <author>
      <name>Jacob Devlin</name>
    </author>
    <author>
      <name>Ming-Wei Chang</name>
    </author>
    <author>
      <name>Kenton Lee</name>
    </author>
    <author>
      <name>Kristina Toutanova</name>
    </author>
    <link href="http://arxiv.org/abs/1810.04805v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04805v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.03581v1</id>
    <updated>2018-10-08T17:09:10Z</updated>
    <published>2018-10-08T17:09:10Z</published>
    <title>Improving the Transformer Translation Model with Document-Level Context</title>
    <summary>  Although the Transformer translation model (Vaswani et al., 2017) has
achieved state-of-the-art performance in a variety of translation tasks, how to
use document-level context to deal with discourse phenomena problematic for
Transformer still remains a challenge. In this work, we extend the Transformer
model with a new context encoder to represent document-level context, which is
then incorporated into the original encoder and decoder. As large-scale
document-level parallel corpora are usually not available, we introduce a
two-step training method to take full advantage of abundant sentence-level
parallel corpora and limited document-level parallel corpora. Experiments on
the NIST Chinese-English datasets and the IWSLT French-English datasets show
that our approach improves over Transformer significantly.
</summary>
    <author>
      <name>Jiacheng Zhang</name>
    </author>
    <author>
      <name>Huanbo Luan</name>
    </author>
    <author>
      <name>Maosong Sun</name>
    </author>
    <author>
      <name>FeiFei Zhai</name>
    </author>
    <author>
      <name>Jingfang Xu</name>
    </author>
    <author>
      <name>Min Zhang</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.03581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.03581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.08895v3</id>
    <updated>2019-01-30T12:40:57Z</updated>
    <published>2018-09-19T07:41:17Z</published>
    <title>Neural Speech Synthesis with Transformer Network</title>
    <summary>  Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2)
are proposed and achieve state-of-the-art performance, they still suffer from
two problems: 1) low efficiency during training and inference; 2) hard to model
long dependency using current recurrent neural networks (RNNs). Inspired by the
success of Transformer network in neural machine translation (NMT), in this
paper, we introduce and adapt the multi-head attention mechanism to replace the
RNN structures and also the original attention mechanism in Tacotron2. With the
help of multi-head self-attention, the hidden states in the encoder and decoder
are constructed in parallel, which improves the training efficiency. Meanwhile,
any two inputs at different times are connected directly by self-attention
mechanism, which solves the long range dependency problem effectively. Using
phoneme sequences as input, our Transformer TTS network generates mel
spectrograms, followed by a WaveNet vocoder to output the final audio results.
Experiments are conducted to test the efficiency and performance of our new
network. For the efficiency, our Transformer TTS network can speed up the
training about 4.25 times faster compared with Tacotron2. For the performance,
rigorous human tests show that our proposed model achieves state-of-the-art
performance (outperforms Tacotron2 with a gap of 0.048) and is very close to
human quality (4.39 vs 4.44 in MOS).
</summary>
    <author>
      <name>Naihan Li</name>
    </author>
    <author>
      <name>Shujie Liu</name>
    </author>
    <author>
      <name>Yanqing Liu</name>
    </author>
    <author>
      <name>Sheng Zhao</name>
    </author>
    <author>
      <name>Ming Liu</name>
    </author>
    <author>
      <name>Ming Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1809.08895v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08895v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.02922v2</id>
    <updated>2018-09-11T00:38:25Z</updated>
    <published>2018-09-09T05:03:34Z</published>
    <title>Transforming Question Answering Datasets Into Natural Language Inference
  Datasets</title>
    <summary>  Existing datasets for natural language inference (NLI) have propelled
research on language understanding. We propose a new method for automatically
deriving NLI datasets from the growing abundance of large-scale question
answering datasets. Our approach hinges on learning a sentence transformation
model which converts question-answer pairs into their declarative forms.
Despite being primarily trained on a single QA dataset, we show that it can be
successfully applied to a variety of other QA resources. Using this system, we
automatically derive a new freely available dataset of over 500k NLI examples
(QA-NLI), and show that it exhibits a wide range of inference phenomena rarely
seen in previous NLI datasets.
</summary>
    <author>
      <name>Dorottya Demszky</name>
    </author>
    <author>
      <name>Kelvin Guu</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.02922v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.02922v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00188v1</id>
    <updated>2018-09-01T14:10:50Z</updated>
    <published>2018-09-01T14:10:50Z</published>
    <title>MS-UEdin Submission to the WMT2018 APE Shared Task: Dual-Source
  Transformer for Automatic Post-Editing</title>
    <summary>  This paper describes the Microsoft and University of Edinburgh submission to
the Automatic Post-editing shared task at WMT2018. Based on training data and
systems from the WMT2017 shared task, we re-implement our own models from the
last shared task and introduce improvements based on extensive parameter
sharing. Next we experiment with our implementation of dual-source transformer
models and data selection for the IT domain. Our submissions decisively wins
the SMT post-editing sub-task establishing the new state-of-the-art and is a
very close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task. Based on
the rather weak results in the NMT sub-task, we hypothesize that
neural-on-neural APE might not be actually useful.
</summary>
    <author>
      <name>Marcin Junczys-Dowmunt</name>
    </author>
    <author>
      <name>Roman Grundkiewicz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Winning submissions for WMT2018 APE shared task</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10583v2</id>
    <updated>2018-09-13T02:45:27Z</updated>
    <published>2018-08-31T03:11:08Z</published>
    <title>AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale</title>
    <summary>  AISHELL-1 is by far the largest open-source speech corpus available for
Mandarin speech recognition research. It was released with a baseline system
containing solid training and testing pipelines for Mandarin ASR. In AISHELL-2,
1000 hours of clean read-speech data from iOS is published, which is free for
academic usage. On top of AISHELL-2 corpus, an improved recipe is developed and
released, containing key components for industrial applications, such as
Chinese word segmentation, flexible vocabulary expension and phone set
transformation etc. Pipelines support various state-of-the-art techniques, such
as time-delayed neural networks and Lattic-Free MMI objective funciton. In
addition, we also release dev and test data from other channels(Android and
Mic). For research community, we hope that AISHELL-2 corpus can be a solid
resource for topics like transfer learning and robust ASR. For industry, we
hope AISHELL-2 recipe can be a helpful reference for building meaningful
industrial systems and products.
</summary>
    <author>
      <name>Jiayu Du</name>
    </author>
    <author>
      <name>Xingyu Na</name>
    </author>
    <author>
      <name>Xuechen Liu</name>
    </author>
    <author>
      <name>Hui Bu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10583v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10583v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01591v1</id>
    <updated>2018-08-05T09:50:47Z</updated>
    <published>2018-08-05T09:50:47Z</published>
    <title>LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse
  Semantic Accumulation and Example to Pattern Transformation</title>
    <summary>  Recurrent neural networks (RNNs) are temporal networks and cumulative in
nature that have shown promising results in various natural language processing
tasks. Despite their success, it still remains a challenge to understand their
hidden behavior. In this work, we analyze and interpret the cumulative nature
of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation
(LISA) for explaining decisions and detecting the most likely (i.e., saliency)
patterns that the network relies on while decision making. We demonstrate (1)
LISA: "How an RNN accumulates or builds semantics during its sequential
processing for a given text example and expected response" (2) Example2pattern:
"How the saliency patterns look like for each category in the data according to
the network in decision making". We analyse the sensitiveness of RNNs about
different inputs to check the increase or decrease in prediction scores and
further extract the saliency patterns learned by the network. We employ two
relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to
explain RNN predictions via the LISA and example2pattern.
</summary>
    <author>
      <name>Pankaj Gupta</name>
    </author>
    <author>
      <name>Hinrich Sch√ºtze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2018 Conference on Empirical Methods in Natural Language Processing
  (EMNLP2018) workshop on Analyzing and Interpreting Neural Networks for NLP
  (BlackBoxNLP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.01591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11605v1</id>
    <updated>2018-07-30T23:13:55Z</updated>
    <published>2018-07-30T23:13:55Z</published>
    <title>Doubly Attentive Transformer Machine Translation</title>
    <summary>  In this paper a doubly attentive transformer machine translation model
(DATNMT) is presented in which a doubly-attentive transformer decoder normally
joins spatial visual features obtained via pretrained convolutional neural
networks, conquering any gap between image captioning and translation. In this
framework, the transformer decoder figures out how to take care of
source-language words and parts of an image freely by methods for two separate
attention components in an Enhanced Multi-Head Attention Layer of doubly
attentive transformer, as it generates words in the target language. We find
that the proposed model can effectively exploit not just the scarce multimodal
machine translation data, but also large general-domain text-only machine
translation corpora, or image-text image captioning corpora. The experimental
results show that the proposed doubly-attentive transformer-decoder performs
better than a single-decoder transformer model, and gives the state-of-the-art
results in the English-German multimodal machine translation task.
</summary>
    <author>
      <name>Hasan Sait Arslan</name>
    </author>
    <author>
      <name>Mark Fishel</name>
    </author>
    <author>
      <name>Gholamreza Anbarjafari</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04175v1</id>
    <updated>2018-07-11T14:51:35Z</updated>
    <published>2018-07-11T14:51:35Z</published>
    <title>Cross-lingual Word Analogies using Linear Transformations between
  Semantic Spaces</title>
    <summary>  We generalize the word analogy task across languages, to provide a new
intrinsic evaluation method for cross-lingual semantic spaces. We experiment
with six languages within different language families, including English,
German, Spanish, Italian, Czech, and Croatian. State-of-the-art monolingual
semantic spaces are transformed into a shared space using dictionaries of word
translations. We compare several linear transformations and rank them for
experiments with monolingual (no transformation), bilingual (one semantic space
is transformed to another), and multilingual (all semantic spaces are
transformed onto English space) versions of semantic spaces. We show that
tested linear transformations preserve relationships between words (word
analogies) and lead to impressive results. We achieve average accuracy of
51.1%, 43.1%, and 38.2% for monolingual, bilingual, and multilingual semantic
spaces, respectively.
</summary>
    <author>
      <name>Tom√°≈° Brychc√≠n</name>
    </author>
    <author>
      <name>Stephen Eugene Taylor</name>
    </author>
    <author>
      <name>Luk√°≈° Svoboda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages. arXiv admin note: text overlap with arXiv:1807.04172</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04172v1</id>
    <updated>2018-07-11T14:48:02Z</updated>
    <published>2018-07-11T14:48:02Z</published>
    <title>Linear Transformations for Cross-lingual Semantic Textual Similarity</title>
    <summary>  Cross-lingual semantic textual similarity systems estimate the degree of the
meaning similarity between two sentences, each in a different language.
State-of-the-art algorithms usually employ machine translation and combine vast
amount of features, making the approach strongly supervised, resource rich, and
difficult to use for poorly-resourced languages.
  In this paper, we study linear transformations, which project monolingual
semantic spaces into a shared space using bilingual dictionaries. We propose a
novel transformation, which builds on the best ideas from prior works. We
experiment with unsupervised techniques for sentence similarity based only on
semantic spaces and we show they can be significantly improved by the word
weighting. Our transformation outperforms other methods and together with word
weighting leads to very promising results on several datasets in different
languages.
</summary>
    <author>
      <name>Tom√°≈° Brychc√≠n</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03819v3</id>
    <updated>2019-03-05T16:46:19Z</updated>
    <published>2018-07-10T18:39:15Z</published>
    <title>Universal Transformers</title>
    <summary>  Recurrent neural networks (RNNs) sequentially process data by updating their
state with each new data point, and have long been the de facto choice for
sequence modeling tasks. However, their inherently sequential computation makes
them slow to train. Feed-forward and convolutional architectures have recently
been shown to achieve superior results on some sequence modeling tasks such as
machine translation, with the added advantage that they concurrently process
all inputs in the sequence, leading to easy parallelization and faster training
times. Despite these successes, however, popular feed-forward sequence models
like the Transformer fail to generalize in many simple tasks that recurrent
models handle with ease, e.g. copying strings or even simple logical inference
when the string or formula lengths exceed those observed at training time. We
propose the Universal Transformer (UT), a parallel-in-time self-attentive
recurrent sequence model which can be cast as a generalization of the
Transformer model and which addresses these issues. UTs combine the
parallelizability and global receptive field of feed-forward sequence models
like the Transformer with the recurrent inductive bias of RNNs. We also add a
dynamic per-position halting mechanism and find that it improves accuracy on
several tasks. In contrast to the standard Transformer, under certain
assumptions, UTs can be shown to be Turing-complete. Our experiments show that
UTs outperform standard Transformers on a wide range of algorithmic and
language understanding tasks, including the challenging LAMBADA language
modeling task where UTs achieve a new state of the art, and machine translation
where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De
dataset.
</summary>
    <author>
      <name>Mostafa Dehghani</name>
    </author>
    <author>
      <name>Stephan Gouws</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Jakob Uszkoreit</name>
    </author>
    <author>
      <name>≈Åukasz Kaiser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at ICLR2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03819v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03819v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08748v2</id>
    <updated>2018-12-07T07:09:14Z</updated>
    <published>2018-06-22T16:19:46Z</published>
    <title>Persistent Hidden States and Nonlinear Transformation for Long
  Short-Term Memory</title>
    <summary>  Recurrent neural networks (RNNs) have been drawing much attention with great
success in many applications like speech recognition and neural machine
translation. Long short-term memory (LSTM) is one of the most popular RNN units
in deep learning applications. LSTM transforms the input and the previous
hidden states to the next states with the affine transformation, multiplication
operations and a nonlinear activation function, which makes a good data
representation for a given task. The affine transformation includes rotation
and reflection, which change the semantic or syntactic information of
dimensions in the hidden states. However, considering that a model interprets
the output sequence of LSTM over the whole input sequence, the dimensions of
the states need to keep the same type of semantic or syntactic information
regardless of the location in the sequence. In this paper, we propose a simple
variant of the LSTM unit, persistent recurrent unit (PRU), where each dimension
of hidden states keeps persistent information across time, so that the space
keeps the same meaning over the whole sequence. In addition, to improve the
nonlinear transformation power, we add a feedforward layer in the PRU
structure. In the experiment, we evaluate our proposed methods with three
different tasks, and the results confirm that our methods have better
performance than the conventional LSTM.
</summary>
    <author>
      <name>Heeyoul Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08748v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08748v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06957v2</id>
    <updated>2018-06-20T19:16:46Z</updated>
    <published>2018-06-18T21:18:18Z</published>
    <title>A Comparison of Transformer and Recurrent Neural Networks on
  Multilingual Neural Machine Translation</title>
    <summary>  Recently, neural machine translation (NMT) has been extended to
multilinguality, that is to handle more than one translation direction with a
single system. Multilingual NMT showed competitive performance against pure
bilingual systems. Notably, in low-resource settings, it proved to work
effectively and efficiently, thanks to shared representation space that is
forced across languages and induces a sort of transfer-learning. Furthermore,
multilingual NMT enables so-called zero-shot inference across language pairs
never seen at training time. Despite the increasing interest in this framework,
an in-depth analysis of what a multilingual NMT model is capable of and what it
is not is still missing. Motivated by this, our work (i) provides a
quantitative and comparative analysis of the translations produced by
bilingual, multilingual and zero-shot systems; (ii) investigates the
translation quality of two of the currently dominant neural architectures in
MT, which are the Recurrent and the Transformer ones; and (iii) quantitatively
explores how the closeness between languages influences the zero-shot
translation. Our analysis leverages multiple professional post-edits of
automatic translations by several different systems and focuses both on
automatic standard metrics (BLEU and TER) and on widely used error categories,
which are lexical, morphology, and word order errors.
</summary>
    <author>
      <name>Surafel M. Lakew</name>
    </author>
    <author>
      <name>Mauro Cettolo</name>
    </author>
    <author>
      <name>Marcello Federico</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, to appear on the 27th International Conference on
  Computational Linguistics (COLING 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06957v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06957v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05059v2</id>
    <updated>2018-06-14T00:48:39Z</updated>
    <published>2018-06-12T05:13:04Z</published>
    <title>Multilingual End-to-End Speech Recognition with A Single Transformer on
  Low-Resource Languages</title>
    <summary>  Sequence-to-sequence attention-based models integrate an acoustic,
pronunciation and language model into a single neural network, which make them
very suitable for multilingual automatic speech recognition (ASR). In this
paper, we are concerned with multilingual speech recognition on low-resource
languages by a single Transformer, one of sequence-to-sequence attention-based
models. Sub-words are employed as the multilingual modeling unit without using
any pronunciation lexicon. First, we show that a single multilingual ASR
Transformer performs well on low-resource languages despite of some language
confusion. We then look at incorporating language information into the model by
inserting the language symbol at the beginning or at the end of the original
sub-words sequence under the condition of language information being known
during training. Experiments on CALLHOME datasets demonstrate that the
multilingual ASR Transformer with the language symbol at the end performs
better and can obtain relatively 10.5\% average word error rate (WER) reduction
compared to SHL-MLSTM with residual learning. We go on to show that, assuming
the language information being known during training and testing, about
relatively 12.4\% average WER reduction can be observed compared to SHL-MLSTM
with residual learning through giving the language symbol as the sentence start
token.
</summary>
    <author>
      <name>Shiyu Zhou</name>
    </author>
    <author>
      <name>Shuang Xu</name>
    </author>
    <author>
      <name>Bo Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1805.06239</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05059v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05059v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08329v2</id>
    <updated>2018-09-04T18:16:40Z</updated>
    <published>2018-05-22T00:16:39Z</published>
    <title>Guided Feature Transformation (GFT): A Neural Language Grounding Module
  for Embodied Agents</title>
    <summary>  Recently there has been a rising interest in training agents, embodied in
virtual environments, to perform language-directed tasks by deep reinforcement
learning. In this paper, we propose a simple but effective neural language
grounding module for embodied agents that can be trained end to end from
scratch taking raw pixels, unstructured linguistic commands, and sparse rewards
as the inputs. We model the language grounding process as a language-guided
transformation of visual features, where latent sentence embeddings are used as
the transformation matrices. In several language-directed navigation tasks that
feature challenging partial observability and require simple reasoning, our
module significantly outperforms the state of the art. We also release
XWorld3D, an easy-to-customize 3D environment that can potentially be modified
to evaluate a variety of embodied agents.
</summary>
    <author>
      <name>Haonan Yu</name>
    </author>
    <author>
      <name>Xiaochen Lian</name>
    </author>
    <author>
      <name>Haichao Zhang</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CoRL 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08329v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08329v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06239v2</id>
    <updated>2018-05-18T12:46:54Z</updated>
    <published>2018-05-16T10:43:47Z</published>
    <title>A Comparison of Modeling Units in Sequence-to-Sequence Speech
  Recognition with the Transformer on Mandarin Chinese</title>
    <summary>  The choice of modeling units is critical to automatic speech recognition
(ASR) tasks. Conventional ASR systems typically choose context-dependent states
(CD-states) or context-dependent phonemes (CD-phonemes) as their modeling
units. However, it has been challenged by sequence-to-sequence attention-based
models, which integrate an acoustic, pronunciation and language model into a
single neural network. On English ASR tasks, previous attempts have already
shown that the modeling unit of graphemes can outperform that of phonemes by
sequence-to-sequence attention-based model.
  In this paper, we are concerned with modeling units on Mandarin Chinese ASR
tasks using sequence-to-sequence attention-based models with the Transformer.
Five modeling units are explored including context-independent phonemes
(CI-phonemes), syllables, words, sub-words and characters. Experiments on HKUST
datasets demonstrate that the lexicon free modeling units can outperform
lexicon related modeling units in terms of character error rate (CER). Among
five modeling units, character based model performs best and establishes a new
state-of-the-art CER of $26.64\%$ on HKUST datasets without a hand-designed
lexicon and an extra language model integration, which corresponds to a $4.8\%$
relative improvement over the existing best CER of $28.0\%$ by the joint
CTC-attention based encoder-decoder network.
</summary>
    <author>
      <name>Shiyu Zhou</name>
    </author>
    <author>
      <name>Linhao Dong</name>
    </author>
    <author>
      <name>Shuang Xu</name>
    </author>
    <author>
      <name>Bo Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1804.10752</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06239v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06239v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01086v1</id>
    <updated>2018-05-03T02:16:27Z</updated>
    <published>2018-05-03T02:16:27Z</published>
    <title>Transformation Networks for Target-Oriented Sentiment Classification</title>
    <summary>  Target-oriented sentiment classification aims at classifying sentiment
polarities over individual opinion targets in a sentence. RNN with attention
seems a good fit for the characteristics of this task, and indeed it achieves
the state-of-the-art performance. After re-examining the drawbacks of attention
mechanism and the obstacles that block CNN to perform well in this
classification task, we propose a new model to overcome these issues. Instead
of attention, our model employs a CNN layer to extract salient features from
the transformed word representations originated from a bi-directional RNN
layer. Between the two layers, we propose a component to generate
target-specific representations of words in the sentence, meanwhile incorporate
a mechanism for preserving the original contextual information from the RNN
layer. Experiments show that our model achieves a new state-of-the-art
performance on a few benchmarks.
</summary>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Lidong Bing</name>
    </author>
    <author>
      <name>Wai Lam</name>
    </author>
    <author>
      <name>Bei Shi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.01086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00760v1</id>
    <updated>2018-05-02T12:14:11Z</updated>
    <published>2018-05-02T12:14:11Z</published>
    <title>Aspect Term Extraction with History Attention and Selective
  Transformation</title>
    <summary>  Aspect Term Extraction (ATE), a key sub-task in Aspect-Based Sentiment
Analysis, aims to extract explicit aspect expressions from online user reviews.
We present a new framework for tackling ATE. It can exploit two useful clues,
namely opinion summary and aspect detection history. Opinion summary is
distilled from the whole input sentence, conditioned on each current token for
aspect prediction, and thus the tailor-made summary can help aspect prediction
on this token. Another clue is the information of aspect detection history, and
it is distilled from the previous aspect predictions so as to leverage the
coordinate structure and tagging schema constraints to upgrade the aspect
prediction. Experimental results over four benchmark datasets clearly
demonstrate that our framework can outperform all state-of-the-art methods.
</summary>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Lidong Bing</name>
    </author>
    <author>
      <name>Piji Li</name>
    </author>
    <author>
      <name>Wai Lam</name>
    </author>
    <author>
      <name>Zhimou Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00631v3</id>
    <updated>2018-05-07T03:45:42Z</updated>
    <published>2018-05-02T05:25:43Z</published>
    <title>Accelerating Neural Transformer via an Average Attention Network</title>
    <summary>  With parallelizable attention networks, the neural Transformer is very fast
to train. However, due to the auto-regressive architecture and self-attention
in the decoder, the decoding procedure becomes slow. To alleviate this issue,
we propose an average attention network as an alternative to the self-attention
network in the decoder of the neural Transformer. The average attention network
consists of two layers, with an average layer that models dependencies on
previous positions and a gating layer that is stacked over the average layer to
enhance the expressiveness of the proposed attention network. We apply this
network on the decoder part of the neural Transformer to replace the original
target-side self-attention model. With masking tricks and dynamic programming,
our model enables the neural Transformer to decode sentences over four times
faster than its original version with almost no loss in training time and
translation performance. We conduct a series of experiments on WMT17
translation tasks, where on 6 different language pairs, we obtain robust and
consistent speed-ups in decoding.
</summary>
    <author>
      <name>Biao Zhang</name>
    </author>
    <author>
      <name>Deyi Xiong</name>
    </author>
    <author>
      <name>Jinsong Su</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2018, long paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00631v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00631v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10752v2</id>
    <updated>2018-06-04T07:46:02Z</updated>
    <published>2018-04-28T06:54:11Z</published>
    <title>Syllable-Based Sequence-to-Sequence Speech Recognition with the
  Transformer in Mandarin Chinese</title>
    <summary>  Sequence-to-sequence attention-based models have recently shown very
promising results on automatic speech recognition (ASR) tasks, which integrate
an acoustic, pronunciation and language model into a single neural network. In
these models, the Transformer, a new sequence-to-sequence attention-based model
relying entirely on self-attention without using RNNs or convolutions, achieves
a new single-model state-of-the-art BLEU on neural machine translation (NMT)
tasks. Since the outstanding performance of the Transformer, we extend it to
speech and concentrate on it as the basic architecture of sequence-to-sequence
attention-based model on Mandarin Chinese ASR tasks. Furthermore, we
investigate a comparison between syllable based model and context-independent
phoneme (CI-phoneme) based model with the Transformer in Mandarin Chinese.
Additionally, a greedy cascading decoder with the Transformer is proposed for
mapping CI-phoneme sequences and syllable sequences into word sequences.
Experiments on HKUST datasets demonstrate that syllable based model with the
Transformer performs better than CI-phoneme based counterpart, and achieves a
character error rate (CER) of \emph{$28.77\%$}, which is competitive to the
state-of-the-art CER of $28.0\%$ by the joint CTC-attention based
encoder-decoder network.
</summary>
    <author>
      <name>Shiyu Zhou</name>
    </author>
    <author>
      <name>Linhao Dong</name>
    </author>
    <author>
      <name>Shuang Xu</name>
    </author>
    <author>
      <name>Bo Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by INTERSPEECH2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.10752v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10752v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00247v2</id>
    <updated>2018-05-02T15:27:25Z</updated>
    <published>2018-04-01T01:59:52Z</published>
    <title>Training Tips for the Transformer Model</title>
    <summary>  This article describes our experiments in neural machine translation using
the recent Tensor2Tensor framework and the Transformer sequence-to-sequence
model (Vaswani et al., 2017). We examine some of the critical parameters that
affect the final translation quality, memory usage, training stability and
training time, concluding each experiment with a set of recommendations for
fellow researchers. In addition to confirming the general mantra "more data and
larger models", we address scaling to multiple GPUs and provide practical tips
for improved training regarding batch size, learning rate, warmup steps,
maximum sentence length and checkpoint averaging. We hope that our observations
will allow others to get better results given their particular hardware and
data constraints.
</summary>
    <author>
      <name>Martin Popel</name>
    </author>
    <author>
      <name>Ond≈ôej Bojar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2478/pralin-2018-0002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2478/pralin-2018-0002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the version published in PBML
  (https://ufal.mff.cuni.cz/pbml/110/art-popel-bojar.pdf)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The Prague Bulletin of Mathematical Linguistics 110, April 2018,
  pp. 43-70</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1804.00247v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00247v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00047v2</id>
    <updated>2018-06-07T06:37:44Z</updated>
    <published>2018-03-30T20:17:31Z</published>
    <title>Conditional End-to-End Audio Transforms</title>
    <summary>  We present an end-to-end method for transforming audio from one style to
another. For the case of speech, by conditioning on speaker identities, we can
train a single model to transform words spoken by multiple people into multiple
target voices. For the case of music, we can specify musical instruments and
achieve the same result. Architecturally, our method is a fully-differentiable
sequence-to-sequence model based on convolutional and hierarchical recurrent
neural networks. It is designed to capture long-term acoustic dependencies,
requires minimal post-processing, and produces realistic audio transforms.
Ablation studies confirm that our model can separate speaker and instrument
properties from acoustic content at different receptive fields. Empirically,
our method achieves competitive performance on community-standard datasets.
</summary>
    <author>
      <name>Albert Haque</name>
    </author>
    <author>
      <name>Michelle Guo</name>
    </author>
    <author>
      <name>Prateek Verma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.00047v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00047v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02132v1</id>
    <updated>2017-11-06T19:35:00Z</updated>
    <published>2017-11-06T19:35:00Z</published>
    <title>Weighted Transformer Network for Machine Translation</title>
    <summary>  State-of-the-art results on neural machine translation often use attentional
sequence-to-sequence models with some form of convolution or recursion. Vaswani
et al. (2017) propose a new architecture that avoids recurrence and convolution
completely. Instead, it uses only self-attention and feed-forward layers. While
the proposed architecture achieves state-of-the-art results on several machine
translation tasks, it requires a large number of parameters and training
iterations to converge. We propose Weighted Transformer, a Transformer with
modified attention layers, that not only outperforms the baseline network in
BLEU score but also converges 15-40% faster. Specifically, we replace the
multi-head attention by multiple self-attention branches that the model learns
to combine during the training process. Our model improves the state-of-the-art
performance by 0.5 BLEU points on the WMT 2014 English-to-German translation
task and by 0.4 on the English-to-French translation task.
</summary>
    <author>
      <name>Karim Ahmed</name>
    </author>
    <author>
      <name>Nitish Shirish Keskar</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <link href="http://arxiv.org/abs/1711.02132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09137v1</id>
    <updated>2017-10-25T09:45:58Z</updated>
    <published>2017-10-25T09:45:58Z</published>
    <title>Linking Tweets with Monolingual and Cross-Lingual News using Transformed
  Word Embeddings</title>
    <summary>  Social media platforms have grown into an important medium to spread
information about an event published by the traditional media, such as news
articles. Grouping such diverse sources of information that discuss the same
topic in varied perspectives provide new insights. But the gap in word usage
between informal social media content such as tweets and diligently written
content (e.g. news articles) make such assembling difficult. In this paper, we
propose a transformation framework to bridge the word usage gap between tweets
and online news articles across languages by leveraging their word embeddings.
Using our framework, word embeddings extracted from tweets and news articles
are aligned closer to each other across languages, thus facilitating the
identification of similarity between news articles and tweets. Experimental
results show a notable improvement over baselines for monolingual tweets and
news articles comparison, while new findings are reported for cross-lingual
comparison.
</summary>
    <author>
      <name>Aditya Mogadala</name>
    </author>
    <author>
      <name>Dominik Jung</name>
    </author>
    <author>
      <name>Achim Rettinger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at CICLing 2017 (18th International Conference on
  Intelligent Text Processing and Computational Linguistics). To appear in
  International Journal of Computational Linguistics and Applications (IJLCA)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.09137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03203v2</id>
    <updated>2017-10-10T00:51:23Z</updated>
    <published>2017-10-09T17:30:12Z</published>
    <title>Deep Learning Paradigm with Transformed Monolingual Word Embeddings for
  Multilingual Sentiment Analysis</title>
    <summary>  The surge of social media use brings huge demand of multilingual sentiment
analysis (MSA) for unveiling cultural difference. So far, traditional methods
resorted to machine translation---translating texts in other languages to
English, and then adopt the methods once worked in English. However, this
paradigm is conditioned by the quality of machine translation. In this paper,
we propose a new deep learning paradigm to assimilate the differences between
languages for MSA. We first pre-train monolingual word embeddings separately,
then map word embeddings in different spaces into a shared embedding space, and
then finally train a parameter-sharing deep neural network for MSA. The
experimental results show that our paradigm is effective. Especially, our CNN
model outperforms a state-of-the-art baseline by around 2.1% in terms of
classification accuracy.
</summary>
    <author>
      <name>Yujie Lu</name>
    </author>
    <author>
      <name>Tatsunori Mori</name>
    </author>
    <link href="http://arxiv.org/abs/1710.03203v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03203v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03487v2</id>
    <updated>2018-06-25T15:27:02Z</updated>
    <published>2017-05-06T18:35:35Z</published>
    <title>A neural network system for transformation of regional cuisine style</title>
    <summary>  We propose a novel system which can transform a recipe into any selected
regional style (e.g., Japanese, Mediterranean, or Italian). This system has two
characteristics. First the system can identify the degree of regional cuisine
style mixture of any selected recipe and visualize such regional cuisine style
mixtures using barycentric Newton diagrams. Second, the system can suggest
ingredient substitutions through an extended word2vec model, such that a recipe
becomes more authentic for any selected regional cuisine style. Drawing on a
large number of recipes from Yummly, an example shows how the proposed system
can transform a traditional Japanese recipe, Sukiyaki, into French style.
</summary>
    <author>
      <name>Masahiro Kazama</name>
    </author>
    <author>
      <name>Minami Sugimoto</name>
    </author>
    <author>
      <name>Chizuru Hosokawa</name>
    </author>
    <author>
      <name>Keisuke Matsushima</name>
    </author>
    <author>
      <name>Lav R. Varshney</name>
    </author>
    <author>
      <name>Yoshiki Ishikawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/fict.2018.00014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/fict.2018.00014" rel="related"/>
    <link href="http://arxiv.org/abs/1705.03487v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03487v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04222v2</id>
    <updated>2017-09-22T16:41:54Z</updated>
    <published>2017-04-13T17:41:11Z</published>
    <title>Learning Latent Representations for Speech Generation and Transformation</title>
    <summary>  An ability to model a generative process and learn a latent representation
for speech in an unsupervised fashion will be crucial to process vast
quantities of unlabelled speech data. Recently, deep probabilistic generative
models such as Variational Autoencoders (VAEs) have achieved tremendous success
in modeling natural images. In this paper, we apply a convolutional VAE to
model the generative process of natural speech. We derive latent space
arithmetic operations to disentangle learned latent representations. We
demonstrate the capability of our model to modify the phonetic content or the
speaker identity for speech segments using the derived operations, without the
need for parallel supervisory data.
</summary>
    <author>
      <name>Wei-Ning Hsu</name>
    </author>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>James Glass</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Interspeech 2017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech 2017, pp 1273-1277</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.04222v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04222v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02298v2</id>
    <updated>2017-06-30T15:14:22Z</updated>
    <published>2017-04-07T17:13:03Z</published>
    <title>TransNets: Learning to Transform for Recommendation</title>
    <summary>  Recently, deep learning methods have been shown to improve the performance of
recommender systems over traditional methods, especially when review text is
available. For example, a recent model, DeepCoNN, uses neural nets to learn one
latent representation for the text of all reviews written by a target user, and
a second latent representation for the text of all reviews for a target item,
and then combines these latent representations to obtain state-of-the-art
performance on recommendation tasks. We show that (unsurprisingly) much of the
predictive value of review text comes from reviews of the target user for the
target item. We then introduce a way in which this information can be used in
recommendation, even when the target user's review for the target item is not
available. Our model, called TransNets, extends the DeepCoNN model by
introducing an additional latent layer representing the target user-target item
pair. We then regularize this layer, at training time, to be similar to another
latent representation of the target user's review of the target item. We show
that TransNets and extensions of it improve substantially over the previous
state-of-the-art.
</summary>
    <author>
      <name>Rose Catherine</name>
    </author>
    <author>
      <name>William Cohen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3109859.3109878</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3109859.3109878" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in the 11th ACM Conference on Recommender
  Systems (RecSys 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.02298v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02298v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03859v1</id>
    <updated>2017-02-13T16:31:06Z</updated>
    <published>2017-02-13T16:31:06Z</published>
    <title>Offline bilingual word vectors, orthogonal transformations and the
  inverted softmax</title>
    <summary>  Usually bilingual word vectors are trained "online". Mikolov et al. showed
they can also be found "offline", whereby two pre-trained embeddings are
aligned with a linear transformation, using dictionaries compiled from expert
knowledge. In this work, we prove that the linear transformation between two
spaces should be orthogonal. This transformation can be obtained using the
singular value decomposition. We introduce a novel "inverted softmax" for
identifying translation pairs, with which we improve the precision @1 of
Mikolov's original mapping from 34% to 43%, when translating a test set
composed of both common and rare English words into Italian. Orthogonal
transformations are more robust to noise, enabling us to learn the
transformation without expert bilingual signal by constructing a
"pseudo-dictionary" from the identical character strings which appear in both
languages, achieving 40% precision on the same test set. Finally, we extend our
method to retrieve the true translations of English sentences from a corpus of
200k Italian sentences with a precision @1 of 68%.
</summary>
    <author>
      <name>Samuel L. Smith</name>
    </author>
    <author>
      <name>David H. P. Turban</name>
    </author>
    <author>
      <name>Steven Hamblin</name>
    </author>
    <author>
      <name>Nils Y. Hammerla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to conference track at ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.03859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04949v1</id>
    <updated>2016-12-15T07:19:46Z</updated>
    <published>2016-12-15T07:19:46Z</published>
    <title>Recurrent Image Captioner: Describing Images with Spatial-Invariant
  Transformation and Attention Filtering</title>
    <summary>  Along with the prosperity of recurrent neural network in modelling sequential
data and the power of attention mechanism in automatically identify salient
information, image captioning, a.k.a., image description, has been remarkably
advanced in recent years. Nonetheless, most existing paradigms may suffer from
the deficiency of invariance to images with different scaling, rotation, etc.;
and effective integration of standalone attention to form a holistic end-to-end
system. In this paper, we propose a novel image captioning architecture, termed
Recurrent Image Captioner (\textbf{RIC}), which allows visual encoder and
language decoder to coherently cooperate in a recurrent manner. Specifically,
we first equip CNN-based visual encoder with a differentiable layer to enable
spatially invariant transformation of visual signals. Moreover, we deploy an
attention filter module (differentiable) between encoder and decoder to
dynamically determine salient visual parts. We also employ bidirectional LSTM
to preprocess sentences for generating better textual representations. Besides,
we propose to exploit variational inference to optimize the whole architecture.
Extensive experimental results on three benchmark datasets (i.e., Flickr8k,
Flickr30k and MS COCO) demonstrate the superiority of our proposed architecture
as compared to most of the state-of-the-art methods.
</summary>
    <author>
      <name>Hao Liu</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Fumin Shen</name>
    </author>
    <author>
      <name>Lixin Duan</name>
    </author>
    <author>
      <name>Heng Tao Shen</name>
    </author>
    <link href="http://arxiv.org/abs/1612.04949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08865v3</id>
    <updated>2018-04-19T17:30:56Z</updated>
    <published>2016-03-29T17:58:46Z</published>
    <title>Compilation as a Typed EDSL-to-EDSL Transformation</title>
    <summary>  This article is about an implementation and compilation technique that is
used in RAW-Feldspar which is a complete rewrite of the Feldspar embedded
domain-specific language (EDSL) (Axelsson et al. 2010). Feldspar is high-level
functional language that generates efficient C code to run on embedded targets.
The gist of the technique presented in this post is the following: rather
writing a back end that converts pure Feldspar expressions directly to C, we
translate them to a low-level monadic EDSL. From the low-level EDSL, C code is
then generated. This approach has several advantages:
  1. The translation is simpler to write than a complete C back end.
  2. The translation is between two typed EDSLs, which rules out many potential
errors.
  3. The low-level EDSL is reusable and can be shared between several
high-level EDSLs.
  Although the article contains a lot of code, most of it is in fact reusable.
As mentioned in Discussion, we can write the same implementation in less than
50 lines of code using generic libraries that we have developed to support
Feldspar.
</summary>
    <author>
      <name>Emil Axelsson</name>
    </author>
    <link href="http://arxiv.org/abs/1603.08865v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08865v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07572v2</id>
    <updated>2016-05-08T08:50:11Z</updated>
    <published>2016-02-24T16:06:25Z</published>
    <title>Ultradense Word Embeddings by Orthogonal Transformation</title>
    <summary>  Embeddings are generic representations that are useful for many NLP tasks. In
this paper, we introduce DENSIFIER, a method that learns an orthogonal
transformation of the embedding space that focuses the information relevant for
a task in an ultradense subspace of a dimensionality that is smaller by a
factor of 100 than the original space. We show that ultradense embeddings
generated by DENSIFIER reach state of the art on a lexicon creation task in
which words are annotated with three types of lexical information - sentiment,
concreteness and frequency. On the SemEval2015 10B sentiment analysis task we
show that no information is lost when the ultradense subspace is used, but
training is an order of magnitude more efficient due to the compactness of the
ultradense space.
</summary>
    <author>
      <name>Sascha Rothe</name>
    </author>
    <author>
      <name>Sebastian Ebert</name>
    </author>
    <author>
      <name>Hinrich Sch√ºtze</name>
    </author>
    <link href="http://arxiv.org/abs/1602.07572v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07572v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.01259v2</id>
    <updated>2016-05-30T14:45:27Z</updated>
    <published>2015-11-04T09:41:31Z</published>
    <title>Transforming Wikipedia into an Ontology-based Information Retrieval
  Search Engine for Local Experts using a Third-Party Taxonomy</title>
    <summary>  Wikipedia is widely used for finding general information about a wide variety
of topics. Its vocation is not to provide local information. For example, it
provides plot, cast, and production information about a given movie, but not
showing times in your local movie theatre. Here we describe how we can connect
local information to Wikipedia, without altering its content. The case study we
present involves finding local scientific experts. Using a third-party
taxonomy, independent from Wikipedia's category hierarchy, we index information
connected to our local experts, present in their activity reports, and we
re-index Wikipedia content using the same taxonomy. The connections between
Wikipedia pages and local expert reports are stored in a relational database,
accessible through as public SPARQL endpoint. A Wikipedia gadget (or plugin)
activated by the interested user, accesses the endpoint as each Wikipedia page
is accessed. An additional tab on the Wikipedia page allows the user to open up
a list of teams of local experts associated with the subject matter in the
Wikipedia page. The technique, though presented here as a way to identify local
experts, is generic, in that any third party taxonomy, can be used in this to
connect Wikipedia to any non-Wikipedia data source.
</summary>
    <author>
      <name>Gregory Grefenstette</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TAO</arxiv:affiliation>
    </author>
    <author>
      <name>Karima Rafes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TAO, LRI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Joint Second Workshop on Language and Ontology \&amp; Terminology and
  Knowledge Structures (LangOnto2 + TermiKS) LO2TKS, May 2016, Portoroz,
  Slovenia. 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.01259v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.01259v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4021v5</id>
    <updated>2015-12-19T11:06:15Z</updated>
    <published>2014-12-12T15:26:43Z</published>
    <title>A Robust Transformation-Based Learning Approach Using Ripple Down Rules
  for Part-of-Speech Tagging</title>
    <summary>  In this paper, we propose a new approach to construct a system of
transformation rules for the Part-of-Speech (POS) tagging task. Our approach is
based on an incremental knowledge acquisition method where rules are stored in
an exception structure and new rules are only added to correct the errors of
existing rules; thus allowing systematic control of the interaction between the
rules. Experimental results on 13 languages show that our approach is fast in
terms of training time and tagging speed. Furthermore, our approach obtains
very competitive accuracy in comparison to state-of-the-art POS and
morphological taggers.
</summary>
    <author>
      <name>Dat Quoc Nguyen</name>
    </author>
    <author>
      <name>Dai Quoc Nguyen</name>
    </author>
    <author>
      <name>Dang Duc Pham</name>
    </author>
    <author>
      <name>Son Bao Pham</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/AIC-150698</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/AIC-150698" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 1: 13 pages. Version 2: Submitted to AI Communications - the
  European Journal on Artificial Intelligence. Version 3: Resubmitted after
  major revisions. Version 4: Resubmitted after minor revisions. Version 5: to
  appear in AI Communications (accepted for publication on 3/12/2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.4021v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4021v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4614v1</id>
    <updated>2014-09-26T05:09:40Z</updated>
    <published>2014-09-26T05:09:40Z</published>
    <title>Using graph transformation algorithms to generate natural language
  equivalents of icons expressing medical concepts</title>
    <summary>  A graphical language addresses the need to communicate medical information in
a synthetic way. Medical concepts are expressed by icons conveying fast visual
information about patients' current state or about the known effects of drugs.
In order to increase the visual language's acceptance and usability, a natural
language generation interface is currently developed. In this context, this
paper describes the use of an informatics method ---graph transformation--- to
prepare data consisting of concepts in an OWL-DL ontology for use in a natural
language generation component. The OWL concept may be considered as a
star-shaped graph with a central node. The method transforms it into a graph
representing the deep semantic structure of a natural language phrase. This
work may be of future use in other contexts where ontology concepts have to be
mapped to half-formalized natural language expressions.
</summary>
    <author>
      <name>Pascal Vaillant</name>
    </author>
    <author>
      <name>Jean-Baptiste Lamy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the TSD 2014 conference: Text, Speech and Dialogue, 17th
  international conference. Brno, Czech Republic, September 8-12, 2014. 10
  pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.4614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.0396v1</id>
    <updated>2011-12-02T07:15:49Z</updated>
    <published>2011-12-02T07:15:49Z</published>
    <title>Grammatical Relations of Myanmar Sentences Augmented by
  Transformation-Based Learning of Function Tagging</title>
    <summary>  In this paper we describe function tagging using Transformation Based
Learning (TBL) for Myanmar that is a method of extensions to the previous
statistics-based function tagger. Contextual and lexical rules (developed using
TBL) were critical in achieving good results. First, we describe a method for
expressing lexical relations in function tagging that statistical function
tagging are currently unable to express. Function tagging is the preprocessing
step to show grammatical relations of the sentences. Then we use the context
free grammar technique to clarify the grammatical relations in Myanmar
sentences or to output the parse trees. The grammatical relations are the
functional structure of a language. They rely very much on the function tag of
the tokens. We augment the grammatical relations of Myanmar sentences with
transformation-based learning of function tagging.
</summary>
    <author>
      <name>Win Win Thant</name>
    </author>
    <author>
      <name>Tin Myat Htwe</name>
    </author>
    <author>
      <name>Ni Lar Thein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 15 figures, 11 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.0396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.0396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.1966v1</id>
    <updated>2011-08-09T16:03:24Z</updated>
    <published>2011-08-09T16:03:24Z</published>
    <title>A Concise Query Language with Search and Transform Operations for
  Corpora with Multiple Levels of Annotation</title>
    <summary>  The usefulness of annotated corpora is greatly increased if there is an
associated tool that can allow various kinds of operations to be performed in a
simple way. Different kinds of annotation frameworks and many query languages
for them have been proposed, including some to deal with multiple layers of
annotation. We present here an easy to learn query language for a particular
kind of annotation framework based on 'threaded trees', which are somewhere
between the complete order of a tree and the anarchy of a graph. Through
'typed' threads, they can allow multiple levels of annotation in the same
document. Our language has a simple, intuitive and concise syntax and high
expressive power. It allows not only to search for complicated patterns with
short queries but also allows data manipulation and specification of arbitrary
return values. Many of the commonly used tasks that otherwise require writing
programs, can be performed with one or more queries. We compare the language
with some others and try to evaluate it.
</summary>
    <author>
      <name>Anil Kumar Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.1966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.1966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.5168v1</id>
    <updated>2009-03-30T09:45:20Z</updated>
    <published>2009-03-30T09:45:20Z</published>
    <title>Mathematical Model for Transformation of Sentences from Active Voice to
  Passive Voice</title>
    <summary>  Formal work in linguistics has both produced and used important mathematical
tools. Motivated by a survey of models for context and word meaning, syntactic
categories, phrase structure rules and trees, an attempt is being made in the
present paper to present a mathematical model for structuring of sentences from
active voice to passive voice, which is is the form of a transitive verb whose
grammatical subject serves as the patient, receiving the action of the verb.
  For this purpose we have parsed all sentences of a corpus and have generated
Boolean groups for each of them. It has been observed that when we take
constituents of the sentences as subgroups, the sequences of phrases form
permutation roups. Application of isomorphism property yields permutation
mapping between the important subgroups. It has resulted in a model for
transformation of sentences from active voice to passive voice. A computer
program has been written to enable the software developers to evolve grammar
software for sentence transformations.
</summary>
    <author>
      <name>Rakesh Pandey</name>
    </author>
    <author>
      <name>H. S. Dhami</name>
    </author>
    <link href="http://arxiv.org/abs/0903.5168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.5168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0112005v1</id>
    <updated>2001-12-05T05:56:13Z</updated>
    <published>2001-12-05T05:56:13Z</published>
    <title>Universal Model for Paraphrasing -- Using Transformation Based on a
  Defined Criteria --</title>
    <summary>  This paper describes a universal model for paraphrasing that transforms
according to defined criteria. We showed that by using different criteria we
could construct different kinds of paraphrasing systems including one for
answering questions, one for compressing sentences, one for polishing up, and
one for transforming written language to spoken language.
</summary>
    <author>
      <name>Masaki Murata</name>
    </author>
    <author>
      <name>Hitoshi Isahara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. Computation and Language</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">NLPRS'2001, Workshop on Automatic Paraphrasing: Theories and
  Applications</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0112005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0112005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0107021v1</id>
    <updated>2001-07-17T15:43:03Z</updated>
    <published>2001-07-17T15:43:03Z</published>
    <title>Multidimensional Transformation-Based Learning</title>
    <summary>  This paper presents a novel method that allows a machine learning algorithm
following the transformation-based learning paradigm \cite{brill95:tagging} to
be applied to multiple classification tasks by training jointly and
simultaneously on all fields. The motivation for constructing such a system
stems from the observation that many tasks in natural language processing are
naturally composed of multiple subtasks which need to be resolved
simultaneously; also tasks usually learned in isolation can possibly benefit
from being learned in a joint framework, as the signals for the extra tasks
usually constitute inductive bias.
  The proposed algorithm is evaluated in two experiments: in one, the system is
used to jointly predict the part-of-speech and text chunks/baseNP chunks of an
English corpus; and in the second it is used to learn the joint prediction of
word segment boundaries and part-of-speech tagging for Chinese. The results
show that the simultaneous learning of multiple tasks does achieve an
improvement in each task upon training the same tasks sequentially. The
part-of-speech tagging result of 96.63% is state-of-the-art for individual
systems on the particular train/test split.
</summary>
    <author>
      <name>Radu Florian</name>
    </author>
    <author>
      <name>Grace Ngai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, presented at CONLL 2001</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 5th Computational Natural Language Learning
  Workshop (CoNNL-2001), pages 1-8, Toulouse, France</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0107021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0107021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0107020v1</id>
    <updated>2001-07-17T15:26:13Z</updated>
    <published>2001-07-17T15:26:13Z</published>
    <title>Transformation-Based Learning in the Fast Lane</title>
    <summary>  Transformation-based learning has been successfully employed to solve many
natural language processing problems. It achieves state-of-the-art performance
on many natural language processing tasks and does not overtrain easily.
However, it does have a serious drawback: the training time is often
intorelably long, especially on the large corpora which are often used in NLP.
In this paper, we present a novel and realistic method for speeding up the
training time of a transformation-based learner without sacrificing
performance. The paper compares and contrasts the training time needed and
performance achieved by our modified learner with two other systems: a standard
transformation-based learner, and the ICA system \cite{hepple00:tbl}. The
results of these experiments show that our system is able to achieve a
significant improvement in training time while still achieving the same
performance as a standard transformation-based learner. This is a valuable
contribution to systems and algorithms which utilize transformation-based
learning at any part of the execution.
</summary>
    <author>
      <name>Grace Ngai</name>
    </author>
    <author>
      <name>Radu Florian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, presented at NAACL 2001</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Second Conference of the North American Chapter
  of the Association for Computational Linguistics, pages 40-47, Pittsburgh,
  PA, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0107020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0107020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0104020v1</id>
    <updated>2001-04-27T23:16:21Z</updated>
    <published>2001-04-27T23:16:21Z</published>
    <title>Coaxing Confidences from an Old Friend: Probabilistic Classifications
  from Transformation Rule Lists</title>
    <summary>  Transformation-based learning has been successfully employed to solve many
natural language processing problems. It has many positive features, but one
drawback is that it does not provide estimates of class membership
probabilities.
  In this paper, we present a novel method for obtaining class membership
probabilities from a transformation-based rule list classifier. Three
experiments are presented which measure the modeling accuracy and cross-entropy
of the probabilistic classifier on unseen data and the degree to which the
output probabilities from the classifier can be used to estimate confidences in
its classification decisions.
  The results of these experiments show that, for the task of text chunking,
the estimates produced by this technique are more informative than those
generated by a state-of-the-art decision tree.
</summary>
    <author>
      <name>Radu Florian</name>
    </author>
    <author>
      <name>John C. Henderson</name>
    </author>
    <author>
      <name>Grace Ngai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, presented at EMNLP 2000</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Fifth Conference on Empirical Methods in
  Natural Language Processing, pages 26-34, Hong Kong (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0104020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0104020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0008021v1</id>
    <updated>2000-08-22T15:16:22Z</updated>
    <published>2000-08-22T15:16:22Z</published>
    <title>Compact non-left-recursive grammars using the selective left-corner
  transform and factoring</title>
    <summary>  The left-corner transform removes left-recursion from (probabilistic)
context-free grammars and unification grammars, permitting simple top-down
parsing techniques to be used. Unfortunately the grammars produced by the
standard left-corner transform are usually much larger than the original. The
selective left-corner transform described in this paper produces a transformed
grammar which simulates left-corner recognition of a user-specified set of the
original productions, and top-down recognition of the others. Combined with two
factorizations, it produces non-left-recursive grammars that are not much
larger than the original.
</summary>
    <author>
      <name>Mark Johnson</name>
    </author>
    <author>
      <name>Brian Roark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 tables, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 18th International Conference on Computational
  Linguistics (COLING), 2000, pages 355-361</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0008021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0008021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9906015v1</id>
    <updated>1999-06-14T22:06:24Z</updated>
    <published>1999-06-14T22:06:24Z</published>
    <title>Learning Transformation Rules to Find Grammatical Relations</title>
    <summary>  Grammatical relationships are an important level of natural language
processing. We present a trainable approach to find these relationships through
transformation sequences and error-driven learning. Our approach finds
grammatical relationships between core syntax groups and bypasses much of the
parsing phase. On our training and test set, our procedure achieves 63.6%
recall and 77.3% precision (f-score = 69.8).
</summary>
    <author>
      <name>Lisa Ferro</name>
    </author>
    <author>
      <name>Marc Vilain</name>
    </author>
    <author>
      <name>Alexander Yeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages. Uses latex-acl.sty and named.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational Natural Language Learning (CoNLL-99), pages 43-52,
  June, 1999. Bergen, Norway</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9906015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9906015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cmp-lg/9806007v1</id>
    <updated>1998-06-09T23:10:27Z</updated>
    <published>1998-06-09T23:10:27Z</published>
    <title>An Investigation of Transformation-Based Learning in Discourse</title>
    <summary>  This paper presents results from the first attempt to apply
Transformation-Based Learning to a discourse-level Natural Language Processing
task. To address two limitations of the standard algorithm, we developed a
Monte Carlo version of Transformation-Based Learning to make the method
tractable for a wider range of problems without degradation in accuracy, and we
devised a committee method for assigning confidence measures to tags produced
by Transformation-Based Learning. The paper describes these advances, presents
experimental evidence that Transformation-Based Learning is as effective as
alternative approaches (such as Decision Trees and N-Grams) for a discourse
task called Dialogue Act Tagging, and argues that Transformation-Based Learning
has desirable features that make it particularly appealing for the Dialogue Act
Tagging task.
</summary>
    <author>
      <name>Ken Samuel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer and Information Sciences, University of Delaware</arxiv:affiliation>
    </author>
    <author>
      <name>Sandra Carberry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer and Information Sciences, University of Delaware</arxiv:affiliation>
    </author>
    <author>
      <name>K. Vijay-Shanker</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer and Information Sciences, University of Delaware</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 Postscript figure, uses ml98.sty</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning: Proceedings of the 15th International Conference</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cmp-lg/9806007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cmp-lg/9806007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cmp-lg/9806006v1</id>
    <updated>1998-06-08T16:06:59Z</updated>
    <published>1998-06-08T16:06:59Z</published>
    <title>Dialogue Act Tagging with Transformation-Based Learning</title>
    <summary>  For the task of recognizing dialogue acts, we are applying the
Transformation-Based Learning (TBL) machine learning algorithm. To circumvent a
sparse data problem, we extract values of well-motivated features of
utterances, such as speaker direction, punctuation marks, and a new feature,
called dialogue act cues, which we find to be more effective than cue phrases
and word n-grams in practice. We present strategies for constructing a set of
dialogue act cues automatically by minimizing the entropy of the distribution
of dialogue acts in a training corpus, filtering out irrelevant dialogue act
cues, and clustering semantically-related words. In addition, to address
limitations of TBL, we introduce a Monte Carlo strategy for training
efficiently and a committee method for computing confidence measures. These
ideas are combined in our working implementation, which labels held-out data as
accurately as any other reported system for the dialogue act tagging task.
</summary>
    <author>
      <name>Ken Samuel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer and Information Sciences, University of Delaware</arxiv:affiliation>
    </author>
    <author>
      <name>Sandra Carberry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer and Information Sciences, University of Delaware</arxiv:affiliation>
    </author>
    <author>
      <name>K. Vijay-Shanker</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer and Information Sciences, University of Delaware</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, no Postscript figures, uses colacl.sty and acl.bst</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 17th International Conference on Computational
  Linguistics (COLING-ACL '98)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cmp-lg/9806006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cmp-lg/9806006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cmp-lg/9806003v1</id>
    <updated>1998-06-03T16:47:37Z</updated>
    <published>1998-06-03T16:47:37Z</published>
    <title>Lazy Transformation-Based Learning</title>
    <summary>  We introduce a significant improvement for a relatively new machine learning
method called Transformation-Based Learning. By applying a Monte Carlo strategy
to randomly sample from the space of rules, rather than exhaustively analyzing
all possible rules, we drastically reduce the memory and time costs of the
algorithm, without compromising accuracy on unseen data. This enables
Transformation- Based Learning to apply to a wider range of domains, as it can
effectively consider a larger number of different features and feature
interactions in the data. In addition, the Monte Carlo improvement decreases
the labor demands on the human developer, who no longer needs to develop a
minimal set of rule templates to maintain tractability.
</summary>
    <author>
      <name>Ken Samuel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Science, University of Delaware</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 Postscript figures, uses aaai.sty and aaai.bst</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 11th International Florida Artificial
  Intelligence Research Symposium Conference</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cmp-lg/9806003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cmp-lg/9806003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cmp-lg/9806002v1</id>
    <updated>1998-06-02T14:28:17Z</updated>
    <published>1998-06-02T14:28:17Z</published>
    <title>Computing Dialogue Acts from Features with Transformation-Based Learning</title>
    <summary>  To interpret natural language at the discourse level, it is very useful to
accurately recognize dialogue acts, such as SUGGEST, in identifying speaker
intentions. Our research explores the utility of a machine learning method
called Transformation-Based Learning (TBL) in computing dialogue acts, because
TBL has a number of advantages over alternative approaches for this
application. We have identified some extensions to TBL that are necessary in
order to address the limitations of the original algorithm and the particular
demands of discourse processing. We use a Monte Carlo strategy to increase the
applicability of the TBL method, and we select features of utterances that can
be used as input to improve the performance of TBL. Our system is currently
being tested on the VerbMobil corpora of spoken dialogues, producing promising
preliminary results.
</summary>
    <author>
      <name>Ken Samuel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Science, University of Delaware</arxiv:affiliation>
    </author>
    <author>
      <name>Sandra Carberry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Science, University of Delaware</arxiv:affiliation>
    </author>
    <author>
      <name>K. Vijay-Shanker</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Science, University of Delaware</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 Postscript figure, uses aaai.sty and aaai.bst</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applying Machine Learning to Discourse Processing: Papers from the
  1998 AAAI Spring Symposium</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cmp-lg/9806002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cmp-lg/9806002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cmp-lg/9605023v1</id>
    <updated>1996-05-14T12:23:19Z</updated>
    <published>1996-05-14T12:23:19Z</published>
    <title>A Simple Transformation for Offline-Parsable Grammars and its
  Termination Properties</title>
    <summary>  We present, in easily reproducible terms, a simple transformation for
offline-parsable grammars which results in a provably terminating parsing
program directly top-down interpretable in Prolog. The transformation consists
in two steps: (1) removal of empty-productions, followed by: (2) left-recursion
elimination. It is related both to left-corner parsing (where the grammar is
compiled, rather than interpreted through a parsing program, and with the
advantage of guaranteed termination in the presence of empty productions) and
to the Generalized Greibach Normal Form for DCGs (with the advantage of
implementation simplicity).
</summary>
    <author>
      <name>Marc Dymetman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Rank Xerox Research Centre, Grenoble</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Latex. 5 pages. Appeared in Coling-94 Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/cmp-lg/9605023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cmp-lg/9605023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cmp-lg/9505040v1</id>
    <updated>1995-05-23T18:19:39Z</updated>
    <published>1995-05-23T18:19:39Z</published>
    <title>Text Chunking using Transformation-Based Learning</title>
    <summary>  Eric Brill introduced transformation-based learning and showed that it can do
part-of-speech tagging with fairly high accuracy. The same method can be
applied at a higher level of textual interpretation for locating chunks in the
tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is
convenient to view chunking as a tagging problem by encoding the chunk
structure in new tags attached to each word. In automatic tests using
Treebank-derived data, this technique achieved recall and precision rates of
roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that
partition the sentence. Some interesting adaptations to the
transformation-based learning approach are also suggested by this application.
</summary>
    <author>
      <name>Lance A. Ramshaw</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Bowdoin College</arxiv:affiliation>
    </author>
    <author>
      <name>Mitchell P. Marcus</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Pennsylvania</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, LaTeX2e, 1 included figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACL Third Workshop on Very Large Corpora, June 1995, pp. 82-94</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cmp-lg/9505040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cmp-lg/9505040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cmp-lg/9406011v1</id>
    <updated>1994-06-03T18:46:15Z</updated>
    <published>1994-06-03T18:46:15Z</published>
    <title>Exploring the Statistical Derivation of Transformational Rule Sequences
  for Part-of-Speech Tagging</title>
    <summary>  Eric Brill has recently proposed a simple and powerful corpus-based language
modeling approach that can be applied to various tasks including part-of-speech
tagging and building phrase structure trees. The method learns a series of
symbolic transformational rules, which can then be applied in sequence to a
test corpus to produce predictions. The learning process only requires counting
matches for a given set of rule templates, allowing the method to survey a very
large space of possible contextual factors. This paper analyses Brill's
approach as an interesting variation on existing decision tree methods, based
on experiments involving part-of-speech tagging for both English and ancient
Greek corpora. In particular, the analysis throws light on why the new
mechanism seems surprisingly resistant to overtraining. A fast, incremental
implementation and a mechanism for recording the dependencies that underlie the
resulting rule sequence are also described.
</summary>
    <author>
      <name>Lance A. Ramshaw</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univ. of Pennsylvania and Bowdoin College</arxiv:affiliation>
    </author>
    <author>
      <name>Mitchell P. Marcus</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univ. of Pennsylvania</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, in proceedings of the ACL Balancing Act workshop</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACL Balancing Act Workshop proceedings, July 94, pp. 86-95</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cmp-lg/9406011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cmp-lg/9406011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cmp-lg/9406010v1</id>
    <updated>1994-06-02T20:34:18Z</updated>
    <published>1994-06-02T20:34:18Z</published>
    <title>Some Advances in Transformation-Based Part of Speech Tagging</title>
    <summary>  Most recent research in trainable part of speech taggers has explored
stochastic tagging. While these taggers obtain high accuracy, linguistic
information is captured indirectly, typically in tens of thousands of lexical
and contextual probabilities. In [Brill92], a trainable rule-based tagger was
described that obtained performance comparable to that of stochastic taggers,
but captured relevant linguistic information in a small number of simple
non-stochastic rules. In this paper, we describe a number of extensions to this
rule-based tagger. First, we describe a method for expressing lexical relations
in tagging that are not captured by stochastic taggers. Next, we show a
rule-based approach to tagging unknown words. Finally, we show how the tagger
can be extended into a k-best tagger, where multiple tags can be assigned to
words in some cases of uncertainty.
</summary>
    <author>
      <name>Eric Brill</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIT</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages. Code available</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of AAAI94</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cmp-lg/9406010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cmp-lg/9406010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cmp-lg" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
